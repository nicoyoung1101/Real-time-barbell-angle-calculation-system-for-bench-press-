{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicoyoung1101/Real-time-barbell-angle-calculation-system-for-bench-press-/blob/main/colab_notebook/Colab_TrainNetwork_VideoAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK255E7YoEIt"
      },
      "source": [
        "# DeepLabCut Toolbox - Colab for standard (single animal) projects!\n",
        "https://github.com/DeepLabCut/DeepLabCut\n",
        "\n",
        "This notebook illustrates how to use the cloud to:\n",
        "- create a training set\n",
        "- train a network\n",
        "- evaluate a network\n",
        "- create simple quality check plots\n",
        "- analyze novel videos!\n",
        "\n",
        "### This notebook assumes you already have a project folder with labeled data!\n",
        "\n",
        "This notebook demonstrates the necessary steps to use DeepLabCut for your own project.\n",
        "\n",
        "This shows the most simple code to do so, but many of the functions have additional features, so please check out the overview & the protocol paper!\n",
        "\n",
        "Nath\\*, Mathis\\* et al.: Using DeepLabCut for markerless pose estimation during behavior across species. Nature Protocols, 2019.\n",
        "\n",
        "\n",
        "Paper: https://www.nature.com/articles/s41596-019-0176-0\n",
        "\n",
        "Pre-print: https://www.biorxiv.org/content/biorxiv/early/2018/11/24/476531.full.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txoddlM8hLKm"
      },
      "source": [
        "## First, go to \"Runtime\" ->\"change runtime type\"->select \"Python3\", and then select \"GPU\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q23BzhA6CXxu",
        "outputId": "2e8c0ec4-5106-40bf-e1c2-7a4e3956309b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [Connecting to security.ubuntu.com (185.125.1\u001b[0m\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "57 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cuda-11-8 is already the newest version (11.8.0-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 57 not upgraded.\n",
            "Requirement already satisfied: deeplabcut[tf] in /usr/local/lib/python3.10/dist-packages (2.3.10)\n",
            "Requirement already satisfied: dlclibrary>=0.0.6 in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (0.0.7)\n",
            "Requirement already satisfied: filterpy>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (1.4.5)\n",
            "Requirement already satisfied: ruamel.yaml>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (0.18.6)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (0.4.0)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (0.5.1)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (0.60.0)\n",
            "Requirement already satisfied: matplotlib!=3.7.0,!=3.7.1,<3.9,>=3.3 in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (3.8.0)\n",
            "Requirement already satisfied: networkx>=2.6 in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (1.26.4)\n",
            "Requirement already satisfied: pandas!=1.5.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (2.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.17 in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (0.24.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.11 in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (0.14.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (4.66.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (6.0.2)\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (11.0.0)\n",
            "Requirement already satisfied: tables==3.8.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (3.8.0)\n",
            "Requirement already satisfied: tensorflow<=2.10,>=2.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (2.10.0)\n",
            "Requirement already satisfied: tensorpack>=0.11 in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (0.11)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut[tf]) (1.1.0)\n",
            "Requirement already satisfied: cython>=0.29.21 in /usr/local/lib/python3.10/dist-packages (from tables==3.8.0->deeplabcut[tf]) (3.0.11)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from tables==3.8.0->deeplabcut[tf]) (2.10.2)\n",
            "Requirement already satisfied: blosc2~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from tables==3.8.0->deeplabcut[tf]) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tables==3.8.0->deeplabcut[tf]) (24.2)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from tables==3.8.0->deeplabcut[tf]) (9.0.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from dlclibrary>=0.0.6->deeplabcut[tf]) (0.26.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->deeplabcut[tf]) (1.16.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->deeplabcut[tf]) (4.10.0.84)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->deeplabcut[tf]) (2.36.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->deeplabcut[tf]) (2.0.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.7.0,!=3.7.1,<3.9,>=3.3->deeplabcut[tf]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.7.0,!=3.7.1,<3.9,>=3.3->deeplabcut[tf]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.7.0,!=3.7.1,<3.9,>=3.3->deeplabcut[tf]) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.7.0,!=3.7.1,<3.9,>=3.3->deeplabcut[tf]) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.7.0,!=3.7.1,<3.9,>=3.3->deeplabcut[tf]) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.7.0,!=3.7.1,<3.9,>=3.3->deeplabcut[tf]) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.54->deeplabcut[tf]) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.5.0,>=1.0.1->deeplabcut[tf]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.5.0,>=1.0.1->deeplabcut[tf]) (2024.2)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml>=0.15.0->deeplabcut[tf]) (0.2.12)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.17->deeplabcut[tf]) (2024.9.20)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.17->deeplabcut[tf]) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->deeplabcut[tf]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->deeplabcut[tf]) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.11->deeplabcut[tf]) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (1.68.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (3.12.1)\n",
            "Requirement already satisfied: keras<2.11,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (3.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (75.1.0)\n",
            "Requirement already satisfied: tensorboard<2.11,>=2.10 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (2.10.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (0.37.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10,>=2.0->deeplabcut[tf]) (1.17.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut[tf]) (0.9.0)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut[tf]) (1.1.0)\n",
            "Requirement already satisfied: msgpack-numpy>=0.4.4.2 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut[tf]) (0.4.8)\n",
            "Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut[tf]) (24.0.1)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut[tf]) (5.9.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deeplabcut[tf]) (3.16.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deeplabcut[tf]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deeplabcut[tf]) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->deeplabcut[tf]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->deeplabcut[tf]) (1.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<=2.10,>=2.0->deeplabcut[tf]) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<=2.10,>=2.0->deeplabcut[tf]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<=2.10,>=2.0->deeplabcut[tf]) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<=2.10,>=2.0->deeplabcut[tf]) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<=2.10,>=2.0->deeplabcut[tf]) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<=2.10,>=2.0->deeplabcut[tf]) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<=2.10,>=2.0->deeplabcut[tf]) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<=2.10,>=2.0->deeplabcut[tf]) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deeplabcut[tf]) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<=2.10,>=2.0->deeplabcut[tf]) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<=2.10,>=2.0->deeplabcut[tf]) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<=2.10,>=2.0->deeplabcut[tf]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<=2.10,>=2.0->deeplabcut[tf]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<=2.10,>=2.0->deeplabcut[tf]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<=2.10,>=2.0->deeplabcut[tf]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<=2.10,>=2.0->deeplabcut[tf]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<=2.10,>=2.0->deeplabcut[tf]) (2024.8.30)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<=2.10,>=2.0->deeplabcut[tf]) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<=2.10,>=2.0->deeplabcut[tf]) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "#(this will take a few minutes to install all the dependences!)\n",
        "!apt update && apt install cuda-11-8\n",
        "!pip install \"deeplabcut[tf]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25wSj6TlVclR"
      },
      "source": [
        "**(Be sure to click \"RESTART RUNTIME\" if it is displayed above before moving on !)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ-nlTkri4HZ"
      },
      "source": [
        "## Link your Google Drive (with your labeled data, or the demo data):\n",
        "\n",
        "### First, place your project folder into you google drive! \"i.e. move the folder named \"Project-YourName-TheDate\" into google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS4Q4UkR9rgG",
        "outputId": "6d33b18d-cb72-4104-8b6c-3a7e54a3f5cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Now, let's link to your GoogleDrive. Run this cell and follow the authorization instructions:\n",
        "#(We recommend putting a copy of the github repo in your google drive if you are using the demo \"examples\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frnj1RVDyEqs"
      },
      "source": [
        "YOU WILL NEED TO EDIT THE PROJECT PATH **in the config.yaml file** TO BE SET TO YOUR GOOGLE DRIVE LINK!\n",
        "\n",
        "Typically, this will be: /content/drive/My Drive/yourProjectFolderName\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhENAlQnFENJ",
        "outputId": "f009ca3f-9c16-42ed-bb1b-fbf54450b3cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Benchpress_PlateTracking-YANGKAI-2024-12-07/videos/']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#Setup your project variables:\n",
        "# PLEASE EDIT THESE:\n",
        "\n",
        "ProjectFolderName = 'Benchpress_PlateTracking-YANGKAI-2024-12-07'\n",
        "VideoType = 'mov'\n",
        "\n",
        "#don't edit these:\n",
        "videofile_path = ['/content/drive/My Drive/'+ProjectFolderName+'/videos/'] #Enter the list of videos or folder to analyze.\n",
        "videofile_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K9Ndy1beyfG",
        "outputId": "3ad99100-19b9-4007-daba-2cd170e30b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading DLC 2.3.10...\n",
            "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"
          ]
        }
      ],
      "source": [
        "import deeplabcut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o4orkg9QTHKK",
        "outputId": "ece9e18c-7873-456a-e8d7-d058ff95586e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.10'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "deeplabcut.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Z7ZlDr3wV4D1",
        "outputId": "06cb9ce8-4fb6-4129-829a-b0c42ea12281"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Benchpress_PlateTracking-YANGKAI-2024-12-07/config.yaml'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#This creates a path variable that links to your google drive copy\n",
        "#No need to edit this, as you set it up before:\n",
        "path_config_file = '/content/drive/My Drive/'+ProjectFolderName+'/config.yaml'\n",
        "path_config_file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.path.exists(path_config_file))  # 如果返回 True，则路径正确\n"
      ],
      "metadata": {
        "id": "bG2HefgPRsUR",
        "outputId": "68c3310e-43f2-42e2-f01a-35e212c60d0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNi9s1dboEJN"
      },
      "source": [
        "## Create a training dataset:\n",
        "### You must do this step inside of Colab:\n",
        "After running this script the training dataset is created and saved in the project directory under the subdirectory **'training-datasets'**\n",
        "\n",
        "This function also creates new subdirectories under **dlc-models** and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as **pose_cfg.yaml**.\n",
        "\n",
        "Now it is the time to start training the network!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMeUwgxPoEJP",
        "scrolled": true,
        "outputId": "693c72f0-c1f2-48e4-c3e4-020706e25de9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Benchpress_Fullbody-YANGKAI-2024-11-12/labeled-data/video22_full/CollectedData_YANGKAI.h5  not found (perhaps not annotated).\n",
            "Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz....\n",
            "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.95,\n",
              "  1,\n",
              "  (array([ 895,  605,  298,  356,  571,  834,   27,  231,  306,  708,  522,\n",
              "           891,  988,  239,  863,   55,  635,  175,   14,   77,   31,  483,\n",
              "           310,  311,  931,  790,   45,  103,  729,    1,  772,  712,  758,\n",
              "           451,  457,  738,  799,  367,  922,  267,  230, 1000,  530,  700,\n",
              "           251,  689,  821,  141,  316,  482,  331,  295,  262,  434,  864,\n",
              "           372,  272,  270,  503,  319,  557,  986,  142,  202,  796,  196,\n",
              "           264,  533,  252,  578,  395,  299,  889,  247,  389,  798,  878,\n",
              "           466,  603,  656,  261,  458,  876,  913,  807,  695,  236,  604,\n",
              "           362,  930,  516,  496,  803,  279,  640,  487,  912,  108,  494,\n",
              "           824,  787,  612,   65,  776,  317,  669,  752,  728,  757,  214,\n",
              "            97,  585,  601,  500,  255,  386,  741,  384,  992,  917,  811,\n",
              "           907,  122,  767,   60,  899,  624,  283,  943,  875,  887,  632,\n",
              "           402,  715,  456,  382,  312,    8,  531,  818,  101,  492,  580,\n",
              "           249,  278,  646,  385,  765,   54,   34,  271,   30,  380,  759,\n",
              "           740,  725,  505,  363,  862,  485,  582,  815,  880,  866,  435,\n",
              "           977,  320,  513,  352,  420,  949,  961,  145,  828,  366,  597,\n",
              "           240,  650,  967,  144,  769,  477,  788,  315,  993,  545,  894,\n",
              "           484,  786,  204,  836,  693,  952,    5,  946,  681,  724,  215,\n",
              "           655,  150,  342,  465,  399,  920,  819,  698,  412,  518,  520,\n",
              "           885,  113,  614,   37,  686,  898,  351,  158,  154,   40,  409,\n",
              "           538,  858,  773,  266,  408,  294,  665,  524,   18,  471,  529,\n",
              "           308,  983,  447,   62,   79,  682,  193,  775,  181,  997,  583,\n",
              "           200,  911,  995,  713,  406,    2,  674,  443,  253,  577,   85,\n",
              "           440,   75,  837,  897,  467,  924,   38,  948,  762,  918,  140,\n",
              "           820,  792,  608,  979,  493,  418, 1002,  869,   50,  162,  886,\n",
              "           596,  654,  526,  817,  172,  661,  330,  829,  859,  768,   39,\n",
              "           564,  378,  156,  222,  643,  210,  748,  964,  702,  167,  561,\n",
              "           474,  416,   10,  105,   68,  481,  954,  436,  258,  568,  587,\n",
              "           304,  813,  942,  928,  478,   20,  958,  771,  422,  285,   71,\n",
              "            49,  179,  403,  618,  116,  293,   76,   48,  358,  263,  810,\n",
              "           764,  670,  332,   64,  800,   52,  927,  124,  652,  627,  630,\n",
              "           956,  479,  118,   12,  157,  337,  127,   46,  235,  672,  241,\n",
              "           750,  996,  963,  965,  501,  325,  902,  784,  350,  364,   17,\n",
              "           186,  506,  338,  206,  626,  345,  313,  242,  432,  170,  187,\n",
              "           904,  133,  301,  733,  592,  318,   78,  523,  346,  198,  867,\n",
              "           491,  919,  590,  619,   21,  647,  720,  527,  250,  243,  638,\n",
              "            35,   81,  159,  333,  839,  718,  794,  109,  425,  379,  588,\n",
              "           664,   92,  453,  413,  753,  549,  390,  742,  989,  424,  211,\n",
              "           171,  165,  238,  188,  553,  691,  344,  926,  602,  648,  832,\n",
              "           155,  489,  877,  933,  125,   66,  613,  233,  224,  268,  161,\n",
              "           452,  511,  678,  178,  865,  415,  100,  743,  625,  229,  334,\n",
              "          1001,  704,  692,  462,  923,  220,  391,  972,  427,  309,  973,\n",
              "           925,   15,  495,  245,  785,    6,  615,  535,  397,  994,  736,\n",
              "           558,  104,  303,  687,  731,  208,  302,   51,   90,  354,  634,\n",
              "           460,   22,  374,  548,  439,  218,  857,  620,  780,  521,  546,\n",
              "           685,  205,  497,  903,  190,  361,  666,  375,  909,  185,  860,\n",
              "           404,  194,  783,  401,  576,  658,  441,  132,  173,  428,  569,\n",
              "           971,  760,  747,   96,  649,  499,   74,   89,  990,    0,  392,\n",
              "           476,  801,  519,  107,  710,  636,  223,  327,  102,  706,  475,\n",
              "           213,  135,  246,  974,  721,  805,  609,  365,  814,   26,  644,\n",
              "           417,  567,  854,    3,  134,  355,  879,  825,  838,  622,  793,\n",
              "           225,  822,  369,  683,  339,  419,  831,  955,  454,   63,  761,\n",
              "           360,  237,  112,  642,  532,  126,  259,  744,  711,  137,    7,\n",
              "           502,  833,  703,  892,  407,  962,  353,  153,  566,  542,  343,\n",
              "           890,  631,  827,   56,  400,  572,  473,  468,  653,  671,  504,\n",
              "           641,  651,  935,  300,  676,  667,  570,  163,  336,   59,  823,\n",
              "           937,  490,  789,  449,  978,  437,  563,  722,  893,  219,  680,\n",
              "           586,  282,  348,  688,    4,  808,  463,  968,  960,  944,  517,\n",
              "           573,  795,  914,  534,  906,  276,  284,  470,  426,  281,  120,\n",
              "           593,  985,   13, 1003,  455,  584,  959,  160,  981,  679,  717,\n",
              "           746,  195,  191,  628,  536,  883,  164,  106,   16,  575,  929,\n",
              "           950,  540,  329,  480,  498,  872,  405,  745,   93,  433,   83,\n",
              "           975,  781,  657,  414,  662,  982,  732,  737,  915,  751,  322,\n",
              "           556,  357,   69,  514,  509,  340,  221,  938,  146,  289,   29,\n",
              "           114,  508,  812,  976,  176,  168,  347,  376,  766,  873,  855,\n",
              "           905,  189,  136,  446,  841,  254,  884,  290,  945,  848,  232,\n",
              "           645,   33,   88,   44,  341,   61,  957,  199,  429,  809,  394,\n",
              "           730,  297,   73,  393,  547,  579,  541,  856,  999,  842,  663,\n",
              "           217,  539,  830,  623,  668,  826,  421,  694,  138,  212,  616,\n",
              "           844,  840,  936,  727,  969,  701,  726,  921,  716,  234,   67,\n",
              "            24,  381,  216,  735,  129,  349,  111,  166,  207,  438,  552,\n",
              "           274,  851,  591,  941,  991,  525,  287,  469,  326,  121,  998,\n",
              "           507,  228,  673,  595,  445,  117,  464,   25,  110,  149,  152,\n",
              "           528,  696,  621,  461,  598,  139,  749,  260,  852,  323,  900,\n",
              "           248,  450,  410,  607,   19,  328,  296,  269,  226,   94,  515,\n",
              "           843,  637,  280,  286,  589,  934,  444,  184,  371,  874,  980,\n",
              "           275,  806,  939,  182,   32,   80,  307,  770,   11,   43,   86,\n",
              "           778,   36,  984,   58,   41,  782,  411,  562,  209,  148,  739,\n",
              "           594,  123,  734,  574,   98,  377,  130,  699,   23,  871,  555,\n",
              "           370,  512,  383,  201,  368,  554,  610,  387,  861,  816,  292,\n",
              "           690,  881,  256,  606,  197,   95,  966,  947,  169,  581,  305,\n",
              "           560,  853,  373,  719,  791,  227,  660,  143,  846,  180,  868,\n",
              "           131,  953,   47,  774,  324,  203,   84,  633,  908,  565,  611,\n",
              "           398,  896,   91,   82,  430,  779,  119,  870,  291,   57,  321,\n",
              "           257,  888,  951,  442,   42,  617,  388,  335,  273,  756,  488,\n",
              "           550,  901,   53,  932,  940,  128,  802,   28,  183,  882,  459,\n",
              "           510,  675,  151,  244,  714,  543,  544]),\n",
              "   array([639, 697, 265, 288, 423, 910, 147, 659, 797, 970, 177,  99, 850,\n",
              "          448, 431, 847, 709, 755, 115, 916, 777,  72, 845, 537, 677, 849,\n",
              "          987, 174,  87, 551, 486, 705, 314, 396, 600, 472,  70, 599, 804,\n",
              "          754, 277, 723,   9, 359, 707, 763, 835, 192, 629, 559, 684])))]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Note: if you are using the demo data (i.e. examples/Reaching-Mackenzie-2018-08-30/), first delete the folder called dlc-models!\n",
        "#Then, run this cell. There are many more functions you can set here, including which netowkr to use!\n",
        "#check the docstring for full options you can do!\n",
        "deeplabcut.create_training_dataset(path_config_file, net_type='resnet_50', augmenter_type='imgaug')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4FczXGDoEJU"
      },
      "source": [
        "## Start training:\n",
        "This function trains the network for a specific shuffle of the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pOvDq_2oEJW",
        "outputId": "9b36e6a7-e8b1-48f6-d368-fcb8dabb615f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting single-animal trainer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1], [2], [3], [4], [5]],\n",
            " 'all_joints_names': ['left_wrist',\n",
            "                      'left_elbow',\n",
            "                      'left_shoulder',\n",
            "                      'right_wrist',\n",
            "                      'right_elbow',\n",
            "                      'right_shoulder'],\n",
            " 'alpha_r': 0.02,\n",
            " 'apply_prob': 0.5,\n",
            " 'batch_size': 1,\n",
            " 'contrast': {'clahe': True,\n",
            "              'claheratio': 0.1,\n",
            "              'histeq': True,\n",
            "              'histeqratio': 0.1},\n",
            " 'convolution': {'edge': False,\n",
            "                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},\n",
            "                 'embossratio': 0.1,\n",
            "                 'sharpen': False,\n",
            "                 'sharpenratio': 0.3},\n",
            " 'crop_pad': 0,\n",
            " 'cropratio': 0.4,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_Benchpress_FillbodyNov12/Benchpress_Fillbody_YANGKAI95shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'decay_steps': 30000,\n",
            " 'deterministic': False,\n",
            " 'display_iters': 1000,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.10/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 0.05,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'lr_init': 0.0005,\n",
            " 'max_input_size': 1500,\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_Benchpress_FillbodyNov12/Documentation_data-Benchpress_Fillbody_95shuffle1.pickle',\n",
            " 'min_input_size': 64,\n",
            " 'mirror': True,\n",
            " 'multi_stage': False,\n",
            " 'multi_step': [[0.005, 10000],\n",
            "                [0.02, 430000],\n",
            "                [0.002, 730000],\n",
            "                [0.001, 1030000]],\n",
            " 'net_type': 'resnet_50',\n",
            " 'num_joints': 6,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': False,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'pos_dist_thresh': 17,\n",
            " 'project_path': '/content/drive/My '\n",
            "                 'Drive/Benchpress_Fullbody-YANGKAI-2024-11-12',\n",
            " 'regularize': False,\n",
            " 'rotation': 25,\n",
            " 'rotratio': 0.4,\n",
            " 'save_iters': 50000,\n",
            " 'scale_jitter_lo': 0.5,\n",
            " 'scale_jitter_up': 1.25,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/My '\n",
            "                    'Drive/Benchpress_Fullbody-YANGKAI-2024-11-12/dlc-models/iteration-0/Benchpress_FillbodyNov12-trainset95shuffle1/train/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Size is 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ImageNet-pretrained resnet_50\n",
            "Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz....\n",
            "Max_iters overwritten as 25000\n",
            "Display_iters overwritten as 10\n",
            "Save_iters overwritten as 500\n",
            "Training parameter:\n",
            "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/My Drive/Benchpress_Fullbody-YANGKAI-2024-11-12/dlc-models/iteration-0/Benchpress_FillbodyNov12-trainset95shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': True, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4], [5]], 'all_joints_names': ['left_wrist', 'left_elbow', 'left_shoulder', 'right_wrist', 'right_elbow', 'right_shoulder'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'contrast': {'clahe': True, 'claheratio': 0.1, 'histeq': True, 'histeqratio': 0.1, 'gamma': False, 'sigmoid': False, 'log': False, 'linear': False}, 'convolution': {'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]}, 'embossratio': 0.1, 'sharpen': False, 'sharpenratio': 0.3}, 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_Benchpress_FillbodyNov12/Benchpress_Fillbody_YANGKAI95shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': '/usr/local/lib/python3.10/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_Benchpress_FillbodyNov12/Documentation_data-Benchpress_Fillbody_95shuffle1.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 6, 'pos_dist_thresh': 17, 'project_path': '/content/drive/My Drive/Benchpress_Fullbody-YANGKAI-2024-11-12', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
            "Starting training....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "iteration: 10 loss: 0.4061 lr: 0.005\n",
            "iteration: 20 loss: 0.0662 lr: 0.005\n",
            "iteration: 30 loss: 0.0337 lr: 0.005\n",
            "iteration: 40 loss: 0.0372 lr: 0.005\n",
            "iteration: 50 loss: 0.0326 lr: 0.005\n",
            "iteration: 60 loss: 0.0426 lr: 0.005\n",
            "iteration: 70 loss: 0.0291 lr: 0.005\n",
            "iteration: 80 loss: 0.0348 lr: 0.005\n",
            "iteration: 90 loss: 0.0472 lr: 0.005\n",
            "iteration: 100 loss: 0.0410 lr: 0.005\n",
            "iteration: 110 loss: 0.0451 lr: 0.005\n",
            "iteration: 120 loss: 0.0394 lr: 0.005\n",
            "iteration: 130 loss: 0.0453 lr: 0.005\n",
            "iteration: 140 loss: 0.0154 lr: 0.005\n",
            "iteration: 150 loss: 0.0536 lr: 0.005\n",
            "iteration: 160 loss: 0.0434 lr: 0.005\n",
            "iteration: 170 loss: 0.0427 lr: 0.005\n",
            "iteration: 180 loss: 0.0482 lr: 0.005\n",
            "iteration: 190 loss: 0.0223 lr: 0.005\n",
            "iteration: 200 loss: 0.0255 lr: 0.005\n",
            "iteration: 210 loss: 0.0398 lr: 0.005\n",
            "iteration: 220 loss: 0.0338 lr: 0.005\n",
            "iteration: 230 loss: 0.0431 lr: 0.005\n",
            "iteration: 240 loss: 0.0274 lr: 0.005\n",
            "iteration: 250 loss: 0.0357 lr: 0.005\n",
            "iteration: 260 loss: 0.0272 lr: 0.005\n",
            "iteration: 270 loss: 0.0431 lr: 0.005\n",
            "iteration: 280 loss: 0.0469 lr: 0.005\n",
            "iteration: 290 loss: 0.0287 lr: 0.005\n",
            "iteration: 300 loss: 0.0264 lr: 0.005\n",
            "iteration: 310 loss: 0.0341 lr: 0.005\n",
            "iteration: 320 loss: 0.0420 lr: 0.005\n",
            "iteration: 330 loss: 0.0338 lr: 0.005\n",
            "iteration: 340 loss: 0.0417 lr: 0.005\n",
            "iteration: 350 loss: 0.0356 lr: 0.005\n",
            "iteration: 360 loss: 0.0281 lr: 0.005\n",
            "iteration: 370 loss: 0.0192 lr: 0.005\n",
            "iteration: 380 loss: 0.0314 lr: 0.005\n",
            "iteration: 390 loss: 0.0346 lr: 0.005\n",
            "iteration: 400 loss: 0.0391 lr: 0.005\n",
            "iteration: 410 loss: 0.0324 lr: 0.005\n",
            "iteration: 420 loss: 0.0255 lr: 0.005\n",
            "iteration: 430 loss: 0.0248 lr: 0.005\n",
            "iteration: 440 loss: 0.0373 lr: 0.005\n",
            "iteration: 450 loss: 0.0335 lr: 0.005\n",
            "iteration: 460 loss: 0.0209 lr: 0.005\n",
            "iteration: 470 loss: 0.0335 lr: 0.005\n",
            "iteration: 480 loss: 0.0381 lr: 0.005\n",
            "iteration: 490 loss: 0.0269 lr: 0.005\n",
            "iteration: 500 loss: 0.0252 lr: 0.005\n",
            "iteration: 510 loss: 0.0290 lr: 0.005\n",
            "iteration: 520 loss: 0.0179 lr: 0.005\n",
            "iteration: 530 loss: 0.0224 lr: 0.005\n",
            "iteration: 540 loss: 0.0363 lr: 0.005\n",
            "iteration: 550 loss: 0.0247 lr: 0.005\n",
            "iteration: 560 loss: 0.0289 lr: 0.005\n",
            "iteration: 570 loss: 0.0160 lr: 0.005\n",
            "iteration: 580 loss: 0.0483 lr: 0.005\n",
            "iteration: 590 loss: 0.0129 lr: 0.005\n",
            "iteration: 600 loss: 0.0239 lr: 0.005\n",
            "iteration: 610 loss: 0.0479 lr: 0.005\n",
            "iteration: 620 loss: 0.0330 lr: 0.005\n",
            "iteration: 630 loss: 0.0210 lr: 0.005\n",
            "iteration: 640 loss: 0.0335 lr: 0.005\n",
            "iteration: 650 loss: 0.0140 lr: 0.005\n",
            "iteration: 660 loss: 0.0349 lr: 0.005\n",
            "iteration: 670 loss: 0.0257 lr: 0.005\n",
            "iteration: 680 loss: 0.0198 lr: 0.005\n",
            "iteration: 690 loss: 0.0167 lr: 0.005\n",
            "iteration: 700 loss: 0.0225 lr: 0.005\n",
            "iteration: 710 loss: 0.0277 lr: 0.005\n",
            "iteration: 720 loss: 0.0274 lr: 0.005\n",
            "iteration: 730 loss: 0.0283 lr: 0.005\n",
            "iteration: 740 loss: 0.0211 lr: 0.005\n",
            "iteration: 750 loss: 0.0253 lr: 0.005\n",
            "iteration: 760 loss: 0.0293 lr: 0.005\n",
            "iteration: 770 loss: 0.0354 lr: 0.005\n",
            "iteration: 780 loss: 0.0157 lr: 0.005\n",
            "iteration: 790 loss: 0.0319 lr: 0.005\n",
            "iteration: 800 loss: 0.0211 lr: 0.005\n",
            "iteration: 810 loss: 0.0308 lr: 0.005\n",
            "iteration: 820 loss: 0.0309 lr: 0.005\n",
            "iteration: 830 loss: 0.0163 lr: 0.005\n",
            "iteration: 840 loss: 0.0167 lr: 0.005\n",
            "iteration: 850 loss: 0.0251 lr: 0.005\n",
            "iteration: 860 loss: 0.0368 lr: 0.005\n",
            "iteration: 870 loss: 0.0247 lr: 0.005\n",
            "iteration: 880 loss: 0.0494 lr: 0.005\n",
            "iteration: 890 loss: 0.0253 lr: 0.005\n",
            "iteration: 900 loss: 0.0375 lr: 0.005\n",
            "iteration: 910 loss: 0.0267 lr: 0.005\n",
            "iteration: 920 loss: 0.0374 lr: 0.005\n",
            "iteration: 930 loss: 0.0176 lr: 0.005\n",
            "iteration: 940 loss: 0.0411 lr: 0.005\n",
            "iteration: 950 loss: 0.0557 lr: 0.005\n",
            "iteration: 960 loss: 0.0263 lr: 0.005\n",
            "iteration: 970 loss: 0.0224 lr: 0.005\n",
            "iteration: 980 loss: 0.0219 lr: 0.005\n",
            "iteration: 990 loss: 0.0452 lr: 0.005\n",
            "iteration: 1000 loss: 0.0287 lr: 0.005\n",
            "iteration: 1010 loss: 0.0390 lr: 0.005\n",
            "iteration: 1020 loss: 0.0286 lr: 0.005\n",
            "iteration: 1030 loss: 0.0362 lr: 0.005\n",
            "iteration: 1040 loss: 0.0275 lr: 0.005\n",
            "iteration: 1050 loss: 0.0250 lr: 0.005\n",
            "iteration: 1060 loss: 0.0322 lr: 0.005\n",
            "iteration: 1070 loss: 0.0320 lr: 0.005\n",
            "iteration: 1080 loss: 0.0264 lr: 0.005\n",
            "iteration: 1090 loss: 0.0282 lr: 0.005\n",
            "iteration: 1100 loss: 0.0406 lr: 0.005\n",
            "iteration: 1110 loss: 0.0341 lr: 0.005\n",
            "iteration: 1120 loss: 0.0280 lr: 0.005\n",
            "iteration: 1130 loss: 0.0584 lr: 0.005\n",
            "iteration: 1140 loss: 0.0211 lr: 0.005\n",
            "iteration: 1150 loss: 0.0146 lr: 0.005\n",
            "iteration: 1160 loss: 0.0182 lr: 0.005\n",
            "iteration: 1170 loss: 0.0385 lr: 0.005\n",
            "iteration: 1180 loss: 0.0452 lr: 0.005\n",
            "iteration: 1190 loss: 0.0275 lr: 0.005\n",
            "iteration: 1200 loss: 0.0142 lr: 0.005\n",
            "iteration: 1210 loss: 0.0344 lr: 0.005\n",
            "iteration: 1220 loss: 0.0195 lr: 0.005\n",
            "iteration: 1230 loss: 0.0318 lr: 0.005\n",
            "iteration: 1240 loss: 0.0413 lr: 0.005\n",
            "iteration: 1250 loss: 0.0363 lr: 0.005\n",
            "iteration: 1260 loss: 0.0253 lr: 0.005\n",
            "iteration: 1270 loss: 0.0392 lr: 0.005\n",
            "iteration: 1280 loss: 0.0306 lr: 0.005\n",
            "iteration: 1290 loss: 0.0478 lr: 0.005\n",
            "iteration: 1300 loss: 0.0331 lr: 0.005\n",
            "iteration: 1310 loss: 0.0327 lr: 0.005\n",
            "iteration: 1320 loss: 0.0310 lr: 0.005\n",
            "iteration: 1330 loss: 0.0213 lr: 0.005\n",
            "iteration: 1340 loss: 0.0168 lr: 0.005\n",
            "iteration: 1350 loss: 0.0074 lr: 0.005\n",
            "iteration: 1360 loss: 0.0311 lr: 0.005\n",
            "iteration: 1370 loss: 0.0254 lr: 0.005\n",
            "iteration: 1380 loss: 0.0258 lr: 0.005\n",
            "iteration: 1390 loss: 0.0159 lr: 0.005\n",
            "iteration: 1400 loss: 0.0319 lr: 0.005\n",
            "iteration: 1410 loss: 0.0255 lr: 0.005\n",
            "iteration: 1420 loss: 0.0327 lr: 0.005\n",
            "iteration: 1430 loss: 0.0408 lr: 0.005\n",
            "iteration: 1440 loss: 0.0279 lr: 0.005\n",
            "iteration: 1450 loss: 0.0336 lr: 0.005\n",
            "iteration: 1460 loss: 0.0306 lr: 0.005\n",
            "iteration: 1470 loss: 0.0302 lr: 0.005\n",
            "iteration: 1480 loss: 0.0380 lr: 0.005\n",
            "iteration: 1490 loss: 0.0317 lr: 0.005\n",
            "iteration: 1500 loss: 0.0263 lr: 0.005\n",
            "iteration: 1510 loss: 0.0292 lr: 0.005\n",
            "iteration: 1520 loss: 0.0255 lr: 0.005\n",
            "iteration: 1530 loss: 0.0203 lr: 0.005\n",
            "iteration: 1540 loss: 0.0142 lr: 0.005\n",
            "iteration: 1550 loss: 0.0236 lr: 0.005\n",
            "iteration: 1560 loss: 0.0085 lr: 0.005\n",
            "iteration: 1570 loss: 0.0230 lr: 0.005\n",
            "iteration: 1580 loss: 0.0294 lr: 0.005\n",
            "iteration: 1590 loss: 0.0192 lr: 0.005\n",
            "iteration: 1600 loss: 0.0230 lr: 0.005\n",
            "iteration: 1610 loss: 0.0225 lr: 0.005\n",
            "iteration: 1620 loss: 0.0291 lr: 0.005\n",
            "iteration: 1630 loss: 0.0316 lr: 0.005\n",
            "iteration: 1640 loss: 0.0237 lr: 0.005\n",
            "iteration: 1650 loss: 0.0315 lr: 0.005\n",
            "iteration: 1660 loss: 0.0238 lr: 0.005\n",
            "iteration: 1670 loss: 0.0341 lr: 0.005\n",
            "iteration: 1680 loss: 0.0278 lr: 0.005\n",
            "iteration: 1690 loss: 0.0297 lr: 0.005\n",
            "iteration: 1700 loss: 0.0212 lr: 0.005\n",
            "iteration: 1710 loss: 0.0277 lr: 0.005\n",
            "iteration: 1720 loss: 0.0261 lr: 0.005\n",
            "iteration: 1730 loss: 0.0426 lr: 0.005\n",
            "iteration: 1740 loss: 0.0156 lr: 0.005\n",
            "iteration: 1750 loss: 0.0245 lr: 0.005\n",
            "iteration: 1760 loss: 0.0151 lr: 0.005\n",
            "iteration: 1770 loss: 0.0176 lr: 0.005\n",
            "iteration: 1780 loss: 0.0294 lr: 0.005\n",
            "iteration: 1790 loss: 0.0129 lr: 0.005\n",
            "iteration: 1800 loss: 0.0245 lr: 0.005\n",
            "iteration: 1810 loss: 0.0272 lr: 0.005\n",
            "iteration: 1820 loss: 0.0354 lr: 0.005\n",
            "iteration: 1830 loss: 0.0456 lr: 0.005\n",
            "iteration: 1840 loss: 0.0133 lr: 0.005\n",
            "iteration: 1850 loss: 0.0270 lr: 0.005\n",
            "iteration: 1860 loss: 0.0297 lr: 0.005\n",
            "iteration: 1870 loss: 0.0169 lr: 0.005\n",
            "iteration: 1880 loss: 0.0215 lr: 0.005\n",
            "iteration: 1890 loss: 0.0469 lr: 0.005\n",
            "iteration: 1900 loss: 0.0351 lr: 0.005\n",
            "iteration: 1910 loss: 0.0256 lr: 0.005\n",
            "iteration: 1920 loss: 0.0207 lr: 0.005\n",
            "iteration: 1930 loss: 0.0315 lr: 0.005\n",
            "iteration: 1940 loss: 0.0332 lr: 0.005\n",
            "iteration: 1950 loss: 0.0264 lr: 0.005\n",
            "iteration: 1960 loss: 0.0174 lr: 0.005\n",
            "iteration: 1970 loss: 0.0289 lr: 0.005\n",
            "iteration: 1980 loss: 0.0215 lr: 0.005\n",
            "iteration: 1990 loss: 0.0246 lr: 0.005\n",
            "iteration: 2000 loss: 0.0148 lr: 0.005\n",
            "iteration: 2010 loss: 0.0241 lr: 0.005\n",
            "iteration: 2020 loss: 0.0320 lr: 0.005\n",
            "iteration: 2030 loss: 0.0437 lr: 0.005\n",
            "iteration: 2040 loss: 0.0174 lr: 0.005\n",
            "iteration: 2050 loss: 0.0294 lr: 0.005\n",
            "iteration: 2060 loss: 0.0178 lr: 0.005\n",
            "iteration: 2070 loss: 0.0325 lr: 0.005\n",
            "iteration: 2080 loss: 0.0317 lr: 0.005\n",
            "iteration: 2090 loss: 0.0499 lr: 0.005\n",
            "iteration: 2100 loss: 0.0308 lr: 0.005\n",
            "iteration: 2110 loss: 0.0212 lr: 0.005\n",
            "iteration: 2120 loss: 0.0408 lr: 0.005\n",
            "iteration: 2130 loss: 0.0278 lr: 0.005\n",
            "iteration: 2140 loss: 0.0314 lr: 0.005\n",
            "iteration: 2150 loss: 0.0257 lr: 0.005\n",
            "iteration: 2160 loss: 0.0271 lr: 0.005\n",
            "iteration: 2170 loss: 0.0210 lr: 0.005\n",
            "iteration: 2180 loss: 0.0308 lr: 0.005\n",
            "iteration: 2190 loss: 0.0374 lr: 0.005\n",
            "iteration: 2200 loss: 0.0336 lr: 0.005\n",
            "iteration: 2210 loss: 0.0159 lr: 0.005\n",
            "iteration: 2220 loss: 0.0128 lr: 0.005\n",
            "iteration: 2230 loss: 0.0189 lr: 0.005\n",
            "iteration: 2240 loss: 0.0378 lr: 0.005\n",
            "iteration: 2250 loss: 0.0328 lr: 0.005\n",
            "iteration: 2260 loss: 0.0267 lr: 0.005\n",
            "iteration: 2270 loss: 0.0234 lr: 0.005\n",
            "iteration: 2280 loss: 0.0099 lr: 0.005\n",
            "iteration: 2290 loss: 0.0203 lr: 0.005\n",
            "iteration: 2300 loss: 0.0194 lr: 0.005\n",
            "iteration: 2310 loss: 0.0091 lr: 0.005\n",
            "iteration: 2320 loss: 0.0271 lr: 0.005\n",
            "iteration: 2330 loss: 0.0146 lr: 0.005\n",
            "iteration: 2340 loss: 0.0178 lr: 0.005\n",
            "iteration: 2350 loss: 0.0291 lr: 0.005\n",
            "iteration: 2360 loss: 0.0392 lr: 0.005\n",
            "iteration: 2370 loss: 0.0360 lr: 0.005\n",
            "iteration: 2380 loss: 0.0472 lr: 0.005\n",
            "iteration: 2390 loss: 0.0341 lr: 0.005\n",
            "iteration: 2400 loss: 0.0401 lr: 0.005\n",
            "iteration: 2410 loss: 0.0298 lr: 0.005\n",
            "iteration: 2420 loss: 0.0212 lr: 0.005\n",
            "iteration: 2430 loss: 0.0255 lr: 0.005\n",
            "iteration: 2440 loss: 0.0293 lr: 0.005\n",
            "iteration: 2450 loss: 0.0235 lr: 0.005\n",
            "iteration: 2460 loss: 0.0250 lr: 0.005\n",
            "iteration: 2470 loss: 0.0215 lr: 0.005\n",
            "iteration: 2480 loss: 0.0282 lr: 0.005\n",
            "iteration: 2490 loss: 0.0380 lr: 0.005\n",
            "iteration: 2500 loss: 0.0280 lr: 0.005\n",
            "iteration: 2510 loss: 0.0187 lr: 0.005\n",
            "iteration: 2520 loss: 0.0412 lr: 0.005\n",
            "iteration: 2530 loss: 0.0260 lr: 0.005\n",
            "iteration: 2540 loss: 0.0455 lr: 0.005\n",
            "iteration: 2550 loss: 0.0294 lr: 0.005\n",
            "iteration: 2560 loss: 0.0121 lr: 0.005\n",
            "iteration: 2570 loss: 0.0307 lr: 0.005\n",
            "iteration: 2580 loss: 0.0251 lr: 0.005\n",
            "iteration: 2590 loss: 0.0337 lr: 0.005\n",
            "iteration: 2600 loss: 0.0410 lr: 0.005\n",
            "iteration: 2610 loss: 0.0170 lr: 0.005\n",
            "iteration: 2620 loss: 0.0345 lr: 0.005\n",
            "iteration: 2630 loss: 0.0428 lr: 0.005\n",
            "iteration: 2640 loss: 0.0326 lr: 0.005\n",
            "iteration: 2650 loss: 0.0493 lr: 0.005\n",
            "iteration: 2660 loss: 0.0272 lr: 0.005\n",
            "iteration: 2670 loss: 0.0296 lr: 0.005\n",
            "iteration: 2680 loss: 0.0199 lr: 0.005\n",
            "iteration: 2690 loss: 0.0247 lr: 0.005\n",
            "iteration: 2700 loss: 0.0256 lr: 0.005\n",
            "iteration: 2710 loss: 0.0219 lr: 0.005\n",
            "iteration: 2720 loss: 0.0492 lr: 0.005\n",
            "iteration: 2730 loss: 0.0156 lr: 0.005\n",
            "iteration: 2740 loss: 0.0299 lr: 0.005\n",
            "iteration: 2750 loss: 0.0168 lr: 0.005\n",
            "iteration: 2760 loss: 0.0105 lr: 0.005\n",
            "iteration: 2770 loss: 0.0326 lr: 0.005\n",
            "iteration: 2780 loss: 0.0248 lr: 0.005\n",
            "iteration: 2790 loss: 0.0300 lr: 0.005\n",
            "iteration: 2800 loss: 0.0144 lr: 0.005\n",
            "iteration: 2810 loss: 0.0383 lr: 0.005\n",
            "iteration: 2820 loss: 0.0276 lr: 0.005\n",
            "iteration: 2830 loss: 0.0301 lr: 0.005\n",
            "iteration: 2840 loss: 0.0381 lr: 0.005\n",
            "iteration: 2850 loss: 0.0324 lr: 0.005\n",
            "iteration: 2860 loss: 0.0293 lr: 0.005\n",
            "iteration: 2870 loss: 0.0246 lr: 0.005\n",
            "iteration: 2880 loss: 0.0386 lr: 0.005\n",
            "iteration: 2890 loss: 0.0175 lr: 0.005\n",
            "iteration: 2900 loss: 0.0245 lr: 0.005\n",
            "iteration: 2910 loss: 0.0415 lr: 0.005\n",
            "iteration: 2920 loss: 0.0263 lr: 0.005\n",
            "iteration: 2930 loss: 0.0134 lr: 0.005\n",
            "iteration: 2940 loss: 0.0453 lr: 0.005\n",
            "iteration: 2950 loss: 0.0280 lr: 0.005\n",
            "iteration: 2960 loss: 0.0183 lr: 0.005\n",
            "iteration: 2970 loss: 0.0313 lr: 0.005\n",
            "iteration: 2980 loss: 0.0259 lr: 0.005\n",
            "iteration: 2990 loss: 0.0191 lr: 0.005\n",
            "iteration: 3000 loss: 0.0120 lr: 0.005\n",
            "iteration: 3010 loss: 0.0237 lr: 0.005\n",
            "iteration: 3020 loss: 0.0141 lr: 0.005\n",
            "iteration: 3030 loss: 0.0241 lr: 0.005\n",
            "iteration: 3040 loss: 0.0178 lr: 0.005\n",
            "iteration: 3050 loss: 0.0273 lr: 0.005\n",
            "iteration: 3060 loss: 0.0201 lr: 0.005\n",
            "iteration: 3070 loss: 0.0250 lr: 0.005\n",
            "iteration: 3080 loss: 0.0229 lr: 0.005\n",
            "iteration: 3090 loss: 0.0116 lr: 0.005\n",
            "iteration: 3100 loss: 0.0150 lr: 0.005\n",
            "iteration: 3110 loss: 0.0310 lr: 0.005\n",
            "iteration: 3120 loss: 0.0184 lr: 0.005\n",
            "iteration: 3130 loss: 0.0316 lr: 0.005\n",
            "iteration: 3140 loss: 0.0168 lr: 0.005\n",
            "iteration: 3150 loss: 0.0376 lr: 0.005\n",
            "iteration: 3160 loss: 0.0234 lr: 0.005\n",
            "iteration: 3170 loss: 0.0224 lr: 0.005\n",
            "iteration: 3180 loss: 0.0154 lr: 0.005\n",
            "iteration: 3190 loss: 0.0290 lr: 0.005\n",
            "iteration: 3200 loss: 0.0309 lr: 0.005\n",
            "iteration: 3210 loss: 0.0293 lr: 0.005\n",
            "iteration: 3220 loss: 0.0185 lr: 0.005\n",
            "iteration: 3230 loss: 0.0381 lr: 0.005\n",
            "iteration: 3240 loss: 0.0164 lr: 0.005\n",
            "iteration: 3250 loss: 0.0386 lr: 0.005\n",
            "iteration: 3260 loss: 0.0211 lr: 0.005\n",
            "iteration: 3270 loss: 0.0311 lr: 0.005\n",
            "iteration: 3280 loss: 0.0223 lr: 0.005\n",
            "iteration: 3290 loss: 0.0228 lr: 0.005\n",
            "iteration: 3300 loss: 0.0162 lr: 0.005\n",
            "iteration: 3310 loss: 0.0267 lr: 0.005\n",
            "iteration: 3320 loss: 0.0265 lr: 0.005\n",
            "iteration: 3330 loss: 0.0347 lr: 0.005\n",
            "iteration: 3340 loss: 0.0241 lr: 0.005\n",
            "iteration: 3350 loss: 0.0190 lr: 0.005\n",
            "iteration: 3360 loss: 0.0111 lr: 0.005\n",
            "iteration: 3370 loss: 0.0112 lr: 0.005\n",
            "iteration: 3380 loss: 0.0126 lr: 0.005\n",
            "iteration: 3390 loss: 0.0111 lr: 0.005\n",
            "iteration: 3400 loss: 0.0384 lr: 0.005\n",
            "iteration: 3410 loss: 0.0146 lr: 0.005\n",
            "iteration: 3420 loss: 0.0265 lr: 0.005\n",
            "iteration: 3430 loss: 0.0204 lr: 0.005\n",
            "iteration: 3440 loss: 0.0289 lr: 0.005\n",
            "iteration: 3450 loss: 0.0404 lr: 0.005\n",
            "iteration: 3460 loss: 0.0258 lr: 0.005\n",
            "iteration: 3470 loss: 0.0222 lr: 0.005\n",
            "iteration: 3480 loss: 0.0345 lr: 0.005\n",
            "iteration: 3490 loss: 0.0278 lr: 0.005\n",
            "iteration: 3500 loss: 0.0136 lr: 0.005\n",
            "iteration: 3510 loss: 0.0316 lr: 0.005\n",
            "iteration: 3520 loss: 0.0287 lr: 0.005\n",
            "iteration: 3530 loss: 0.0321 lr: 0.005\n",
            "iteration: 3540 loss: 0.0281 lr: 0.005\n",
            "iteration: 3550 loss: 0.0279 lr: 0.005\n",
            "iteration: 3560 loss: 0.0197 lr: 0.005\n",
            "iteration: 3570 loss: 0.0273 lr: 0.005\n",
            "iteration: 3580 loss: 0.0188 lr: 0.005\n",
            "iteration: 3590 loss: 0.0292 lr: 0.005\n",
            "iteration: 3600 loss: 0.0190 lr: 0.005\n",
            "iteration: 3610 loss: 0.0470 lr: 0.005\n",
            "iteration: 3620 loss: 0.0216 lr: 0.005\n",
            "iteration: 3630 loss: 0.0287 lr: 0.005\n",
            "iteration: 3640 loss: 0.0275 lr: 0.005\n",
            "iteration: 3650 loss: 0.0168 lr: 0.005\n",
            "iteration: 3660 loss: 0.0286 lr: 0.005\n",
            "iteration: 3670 loss: 0.0398 lr: 0.005\n",
            "iteration: 3680 loss: 0.0109 lr: 0.005\n",
            "iteration: 3690 loss: 0.0166 lr: 0.005\n",
            "iteration: 3700 loss: 0.0315 lr: 0.005\n",
            "iteration: 3710 loss: 0.0285 lr: 0.005\n",
            "iteration: 3720 loss: 0.0263 lr: 0.005\n",
            "iteration: 3730 loss: 0.0408 lr: 0.005\n",
            "iteration: 3740 loss: 0.0123 lr: 0.005\n",
            "iteration: 3750 loss: 0.0214 lr: 0.005\n",
            "iteration: 3760 loss: 0.0220 lr: 0.005\n",
            "iteration: 3770 loss: 0.0268 lr: 0.005\n",
            "iteration: 3780 loss: 0.0154 lr: 0.005\n",
            "iteration: 3790 loss: 0.0344 lr: 0.005\n",
            "iteration: 3800 loss: 0.0210 lr: 0.005\n",
            "iteration: 3810 loss: 0.0148 lr: 0.005\n",
            "iteration: 3820 loss: 0.0148 lr: 0.005\n",
            "iteration: 3830 loss: 0.0154 lr: 0.005\n",
            "iteration: 3840 loss: 0.0287 lr: 0.005\n",
            "iteration: 3850 loss: 0.0164 lr: 0.005\n",
            "iteration: 3860 loss: 0.0306 lr: 0.005\n",
            "iteration: 3870 loss: 0.0202 lr: 0.005\n",
            "iteration: 3880 loss: 0.0284 lr: 0.005\n",
            "iteration: 3890 loss: 0.0205 lr: 0.005\n",
            "iteration: 3900 loss: 0.0272 lr: 0.005\n",
            "iteration: 3910 loss: 0.0531 lr: 0.005\n",
            "iteration: 3920 loss: 0.0073 lr: 0.005\n",
            "iteration: 3930 loss: 0.0134 lr: 0.005\n",
            "iteration: 3940 loss: 0.0115 lr: 0.005\n",
            "iteration: 3950 loss: 0.0152 lr: 0.005\n",
            "iteration: 3960 loss: 0.0295 lr: 0.005\n",
            "iteration: 3970 loss: 0.0199 lr: 0.005\n",
            "iteration: 3980 loss: 0.0253 lr: 0.005\n",
            "iteration: 3990 loss: 0.0234 lr: 0.005\n",
            "iteration: 4000 loss: 0.0334 lr: 0.005\n",
            "iteration: 4010 loss: 0.0105 lr: 0.005\n",
            "iteration: 4020 loss: 0.0191 lr: 0.005\n",
            "iteration: 4030 loss: 0.0212 lr: 0.005\n",
            "iteration: 4040 loss: 0.0214 lr: 0.005\n",
            "iteration: 4050 loss: 0.0231 lr: 0.005\n",
            "iteration: 4060 loss: 0.0349 lr: 0.005\n",
            "iteration: 4070 loss: 0.0293 lr: 0.005\n",
            "iteration: 4080 loss: 0.0336 lr: 0.005\n",
            "iteration: 4090 loss: 0.0277 lr: 0.005\n",
            "iteration: 4100 loss: 0.0286 lr: 0.005\n",
            "iteration: 4110 loss: 0.0313 lr: 0.005\n",
            "iteration: 4120 loss: 0.0284 lr: 0.005\n",
            "iteration: 4130 loss: 0.0250 lr: 0.005\n",
            "iteration: 4140 loss: 0.0205 lr: 0.005\n",
            "iteration: 4150 loss: 0.0240 lr: 0.005\n",
            "iteration: 4160 loss: 0.0453 lr: 0.005\n",
            "iteration: 4170 loss: 0.0241 lr: 0.005\n",
            "iteration: 4180 loss: 0.0251 lr: 0.005\n",
            "iteration: 4190 loss: 0.0164 lr: 0.005\n",
            "iteration: 4200 loss: 0.0177 lr: 0.005\n",
            "iteration: 4210 loss: 0.0418 lr: 0.005\n",
            "iteration: 4220 loss: 0.0355 lr: 0.005\n",
            "iteration: 4230 loss: 0.0161 lr: 0.005\n",
            "iteration: 4240 loss: 0.0369 lr: 0.005\n",
            "iteration: 4250 loss: 0.0417 lr: 0.005\n",
            "iteration: 4260 loss: 0.0232 lr: 0.005\n",
            "iteration: 4270 loss: 0.0241 lr: 0.005\n",
            "iteration: 4280 loss: 0.0239 lr: 0.005\n",
            "iteration: 4290 loss: 0.0278 lr: 0.005\n",
            "iteration: 4300 loss: 0.0126 lr: 0.005\n",
            "iteration: 4310 loss: 0.0356 lr: 0.005\n",
            "iteration: 4320 loss: 0.0309 lr: 0.005\n",
            "iteration: 4330 loss: 0.0221 lr: 0.005\n",
            "iteration: 4340 loss: 0.0226 lr: 0.005\n",
            "iteration: 4350 loss: 0.0375 lr: 0.005\n",
            "iteration: 4360 loss: 0.0429 lr: 0.005\n",
            "iteration: 4370 loss: 0.0190 lr: 0.005\n",
            "iteration: 4380 loss: 0.0300 lr: 0.005\n",
            "iteration: 4390 loss: 0.0097 lr: 0.005\n",
            "iteration: 4400 loss: 0.0297 lr: 0.005\n",
            "iteration: 4410 loss: 0.0328 lr: 0.005\n",
            "iteration: 4420 loss: 0.0174 lr: 0.005\n",
            "iteration: 4430 loss: 0.0278 lr: 0.005\n",
            "iteration: 4440 loss: 0.0154 lr: 0.005\n",
            "iteration: 4450 loss: 0.0181 lr: 0.005\n",
            "iteration: 4460 loss: 0.0240 lr: 0.005\n",
            "iteration: 4470 loss: 0.0167 lr: 0.005\n",
            "iteration: 4480 loss: 0.0300 lr: 0.005\n",
            "iteration: 4490 loss: 0.0302 lr: 0.005\n",
            "iteration: 4500 loss: 0.0211 lr: 0.005\n",
            "iteration: 4510 loss: 0.0137 lr: 0.005\n",
            "iteration: 4520 loss: 0.0358 lr: 0.005\n",
            "iteration: 4530 loss: 0.0206 lr: 0.005\n",
            "iteration: 4540 loss: 0.0145 lr: 0.005\n",
            "iteration: 4550 loss: 0.0090 lr: 0.005\n",
            "iteration: 4560 loss: 0.0165 lr: 0.005\n",
            "iteration: 4570 loss: 0.0279 lr: 0.005\n",
            "iteration: 4580 loss: 0.0257 lr: 0.005\n",
            "iteration: 4590 loss: 0.0342 lr: 0.005\n",
            "iteration: 4600 loss: 0.0205 lr: 0.005\n",
            "iteration: 4610 loss: 0.0169 lr: 0.005\n",
            "iteration: 4620 loss: 0.0257 lr: 0.005\n",
            "iteration: 4630 loss: 0.0259 lr: 0.005\n",
            "iteration: 4640 loss: 0.0365 lr: 0.005\n",
            "iteration: 4650 loss: 0.0176 lr: 0.005\n",
            "iteration: 4660 loss: 0.0188 lr: 0.005\n",
            "iteration: 4670 loss: 0.0391 lr: 0.005\n",
            "iteration: 4680 loss: 0.0350 lr: 0.005\n",
            "iteration: 4690 loss: 0.0456 lr: 0.005\n",
            "iteration: 4700 loss: 0.0200 lr: 0.005\n",
            "iteration: 4710 loss: 0.0166 lr: 0.005\n",
            "iteration: 4720 loss: 0.0326 lr: 0.005\n",
            "iteration: 4730 loss: 0.0309 lr: 0.005\n",
            "iteration: 4740 loss: 0.0217 lr: 0.005\n",
            "iteration: 4750 loss: 0.0308 lr: 0.005\n",
            "iteration: 4760 loss: 0.0386 lr: 0.005\n",
            "iteration: 4770 loss: 0.0224 lr: 0.005\n",
            "iteration: 4780 loss: 0.0408 lr: 0.005\n",
            "iteration: 4790 loss: 0.0215 lr: 0.005\n",
            "iteration: 4800 loss: 0.0197 lr: 0.005\n",
            "iteration: 4810 loss: 0.0326 lr: 0.005\n",
            "iteration: 4820 loss: 0.0256 lr: 0.005\n",
            "iteration: 4830 loss: 0.0211 lr: 0.005\n",
            "iteration: 4840 loss: 0.0259 lr: 0.005\n",
            "iteration: 4850 loss: 0.0069 lr: 0.005\n",
            "iteration: 4860 loss: 0.0459 lr: 0.005\n",
            "iteration: 4870 loss: 0.0235 lr: 0.005\n",
            "iteration: 4880 loss: 0.0248 lr: 0.005\n",
            "iteration: 4890 loss: 0.0310 lr: 0.005\n",
            "iteration: 4900 loss: 0.0272 lr: 0.005\n",
            "iteration: 4910 loss: 0.0336 lr: 0.005\n",
            "iteration: 4920 loss: 0.0166 lr: 0.005\n",
            "iteration: 4930 loss: 0.0110 lr: 0.005\n",
            "iteration: 4940 loss: 0.0148 lr: 0.005\n",
            "iteration: 4950 loss: 0.0213 lr: 0.005\n",
            "iteration: 4960 loss: 0.0282 lr: 0.005\n",
            "iteration: 4970 loss: 0.0198 lr: 0.005\n",
            "iteration: 4980 loss: 0.0344 lr: 0.005\n",
            "iteration: 4990 loss: 0.0228 lr: 0.005\n",
            "iteration: 5000 loss: 0.0266 lr: 0.005\n",
            "iteration: 5010 loss: 0.0190 lr: 0.005\n",
            "iteration: 5020 loss: 0.0249 lr: 0.005\n",
            "iteration: 5030 loss: 0.0274 lr: 0.005\n",
            "iteration: 5040 loss: 0.0183 lr: 0.005\n",
            "iteration: 5050 loss: 0.0144 lr: 0.005\n",
            "iteration: 5060 loss: 0.0228 lr: 0.005\n",
            "iteration: 5070 loss: 0.0373 lr: 0.005\n",
            "iteration: 5080 loss: 0.0202 lr: 0.005\n",
            "iteration: 5090 loss: 0.0222 lr: 0.005\n",
            "iteration: 5100 loss: 0.0378 lr: 0.005\n",
            "iteration: 5110 loss: 0.0203 lr: 0.005\n",
            "iteration: 5120 loss: 0.0367 lr: 0.005\n",
            "iteration: 5130 loss: 0.0299 lr: 0.005\n",
            "iteration: 5140 loss: 0.0254 lr: 0.005\n",
            "iteration: 5150 loss: 0.0240 lr: 0.005\n",
            "iteration: 5160 loss: 0.0266 lr: 0.005\n",
            "iteration: 5170 loss: 0.0394 lr: 0.005\n",
            "iteration: 5180 loss: 0.0271 lr: 0.005\n",
            "iteration: 5190 loss: 0.0214 lr: 0.005\n",
            "iteration: 5200 loss: 0.0146 lr: 0.005\n",
            "iteration: 5210 loss: 0.0181 lr: 0.005\n",
            "iteration: 5220 loss: 0.0241 lr: 0.005\n",
            "iteration: 5230 loss: 0.0372 lr: 0.005\n",
            "iteration: 5240 loss: 0.0270 lr: 0.005\n",
            "iteration: 5250 loss: 0.0350 lr: 0.005\n",
            "iteration: 5260 loss: 0.0088 lr: 0.005\n",
            "iteration: 5270 loss: 0.0098 lr: 0.005\n",
            "iteration: 5280 loss: 0.0256 lr: 0.005\n",
            "iteration: 5290 loss: 0.0490 lr: 0.005\n",
            "iteration: 5300 loss: 0.0085 lr: 0.005\n",
            "iteration: 5310 loss: 0.0262 lr: 0.005\n",
            "iteration: 5320 loss: 0.0219 lr: 0.005\n",
            "iteration: 5330 loss: 0.0337 lr: 0.005\n",
            "iteration: 5340 loss: 0.0177 lr: 0.005\n",
            "iteration: 5350 loss: 0.0282 lr: 0.005\n",
            "iteration: 5360 loss: 0.0358 lr: 0.005\n",
            "iteration: 5370 loss: 0.0231 lr: 0.005\n",
            "iteration: 5380 loss: 0.0158 lr: 0.005\n",
            "iteration: 5390 loss: 0.0169 lr: 0.005\n",
            "iteration: 5400 loss: 0.0131 lr: 0.005\n",
            "iteration: 5410 loss: 0.0281 lr: 0.005\n",
            "iteration: 5420 loss: 0.0226 lr: 0.005\n",
            "iteration: 5430 loss: 0.0424 lr: 0.005\n",
            "iteration: 5440 loss: 0.0227 lr: 0.005\n",
            "iteration: 5450 loss: 0.0319 lr: 0.005\n",
            "iteration: 5460 loss: 0.0182 lr: 0.005\n",
            "iteration: 5470 loss: 0.0183 lr: 0.005\n",
            "iteration: 5480 loss: 0.0124 lr: 0.005\n",
            "iteration: 5490 loss: 0.0223 lr: 0.005\n",
            "iteration: 5500 loss: 0.0357 lr: 0.005\n",
            "iteration: 5510 loss: 0.0380 lr: 0.005\n",
            "iteration: 5520 loss: 0.0082 lr: 0.005\n",
            "iteration: 5530 loss: 0.0395 lr: 0.005\n",
            "iteration: 5540 loss: 0.0276 lr: 0.005\n",
            "iteration: 5550 loss: 0.0240 lr: 0.005\n",
            "iteration: 5560 loss: 0.0286 lr: 0.005\n",
            "iteration: 5570 loss: 0.0421 lr: 0.005\n",
            "iteration: 5580 loss: 0.0246 lr: 0.005\n",
            "iteration: 5590 loss: 0.0172 lr: 0.005\n",
            "iteration: 5600 loss: 0.0112 lr: 0.005\n",
            "iteration: 5610 loss: 0.0131 lr: 0.005\n",
            "iteration: 5620 loss: 0.0568 lr: 0.005\n",
            "iteration: 5630 loss: 0.0402 lr: 0.005\n",
            "iteration: 5640 loss: 0.0276 lr: 0.005\n",
            "iteration: 5650 loss: 0.0274 lr: 0.005\n",
            "iteration: 5660 loss: 0.0257 lr: 0.005\n",
            "iteration: 5670 loss: 0.0442 lr: 0.005\n",
            "iteration: 5680 loss: 0.0208 lr: 0.005\n",
            "iteration: 5690 loss: 0.0193 lr: 0.005\n",
            "iteration: 5700 loss: 0.0439 lr: 0.005\n",
            "iteration: 5710 loss: 0.0186 lr: 0.005\n",
            "iteration: 5720 loss: 0.0260 lr: 0.005\n",
            "iteration: 5730 loss: 0.0215 lr: 0.005\n",
            "iteration: 5740 loss: 0.0184 lr: 0.005\n",
            "iteration: 5750 loss: 0.0315 lr: 0.005\n",
            "iteration: 5760 loss: 0.0258 lr: 0.005\n",
            "iteration: 5770 loss: 0.0177 lr: 0.005\n",
            "iteration: 5780 loss: 0.0251 lr: 0.005\n",
            "iteration: 5790 loss: 0.0195 lr: 0.005\n",
            "iteration: 5800 loss: 0.0142 lr: 0.005\n",
            "iteration: 5810 loss: 0.0181 lr: 0.005\n",
            "iteration: 5820 loss: 0.0366 lr: 0.005\n",
            "iteration: 5830 loss: 0.0271 lr: 0.005\n",
            "iteration: 5840 loss: 0.0265 lr: 0.005\n",
            "iteration: 5850 loss: 0.0217 lr: 0.005\n",
            "iteration: 5860 loss: 0.0114 lr: 0.005\n",
            "iteration: 5870 loss: 0.0173 lr: 0.005\n",
            "iteration: 5880 loss: 0.0183 lr: 0.005\n",
            "iteration: 5890 loss: 0.0154 lr: 0.005\n",
            "iteration: 5900 loss: 0.0131 lr: 0.005\n",
            "iteration: 5910 loss: 0.0099 lr: 0.005\n",
            "iteration: 5920 loss: 0.0224 lr: 0.005\n",
            "iteration: 5930 loss: 0.0244 lr: 0.005\n",
            "iteration: 5940 loss: 0.0195 lr: 0.005\n",
            "iteration: 5950 loss: 0.0208 lr: 0.005\n",
            "iteration: 5960 loss: 0.0245 lr: 0.005\n",
            "iteration: 5970 loss: 0.0287 lr: 0.005\n",
            "iteration: 5980 loss: 0.0209 lr: 0.005\n",
            "iteration: 5990 loss: 0.0244 lr: 0.005\n",
            "iteration: 6000 loss: 0.0085 lr: 0.005\n",
            "iteration: 6010 loss: 0.0278 lr: 0.005\n",
            "iteration: 6020 loss: 0.0149 lr: 0.005\n",
            "iteration: 6030 loss: 0.0383 lr: 0.005\n",
            "iteration: 6040 loss: 0.0293 lr: 0.005\n",
            "iteration: 6050 loss: 0.0178 lr: 0.005\n",
            "iteration: 6060 loss: 0.0193 lr: 0.005\n",
            "iteration: 6070 loss: 0.0204 lr: 0.005\n",
            "iteration: 6080 loss: 0.0222 lr: 0.005\n",
            "iteration: 6090 loss: 0.0331 lr: 0.005\n",
            "iteration: 6100 loss: 0.0140 lr: 0.005\n",
            "iteration: 6110 loss: 0.0193 lr: 0.005\n",
            "iteration: 6120 loss: 0.0234 lr: 0.005\n",
            "iteration: 6130 loss: 0.0234 lr: 0.005\n",
            "iteration: 6140 loss: 0.0385 lr: 0.005\n",
            "iteration: 6150 loss: 0.0244 lr: 0.005\n",
            "iteration: 6160 loss: 0.0336 lr: 0.005\n",
            "iteration: 6170 loss: 0.0177 lr: 0.005\n",
            "iteration: 6180 loss: 0.0330 lr: 0.005\n",
            "iteration: 6190 loss: 0.0341 lr: 0.005\n",
            "iteration: 6200 loss: 0.0224 lr: 0.005\n",
            "iteration: 6210 loss: 0.0378 lr: 0.005\n",
            "iteration: 6220 loss: 0.0111 lr: 0.005\n",
            "iteration: 6230 loss: 0.0361 lr: 0.005\n",
            "iteration: 6240 loss: 0.0216 lr: 0.005\n",
            "iteration: 6250 loss: 0.0255 lr: 0.005\n",
            "iteration: 6260 loss: 0.0087 lr: 0.005\n",
            "iteration: 6270 loss: 0.0313 lr: 0.005\n",
            "iteration: 6280 loss: 0.0196 lr: 0.005\n",
            "iteration: 6290 loss: 0.0154 lr: 0.005\n",
            "iteration: 6300 loss: 0.0186 lr: 0.005\n",
            "iteration: 6310 loss: 0.0253 lr: 0.005\n",
            "iteration: 6320 loss: 0.0134 lr: 0.005\n",
            "iteration: 6330 loss: 0.0289 lr: 0.005\n",
            "iteration: 6340 loss: 0.0171 lr: 0.005\n",
            "iteration: 6350 loss: 0.0386 lr: 0.005\n",
            "iteration: 6360 loss: 0.0182 lr: 0.005\n",
            "iteration: 6370 loss: 0.0246 lr: 0.005\n",
            "iteration: 6380 loss: 0.0363 lr: 0.005\n",
            "iteration: 6390 loss: 0.0161 lr: 0.005\n",
            "iteration: 6400 loss: 0.0147 lr: 0.005\n",
            "iteration: 6410 loss: 0.0404 lr: 0.005\n",
            "iteration: 6420 loss: 0.0150 lr: 0.005\n",
            "iteration: 6430 loss: 0.0187 lr: 0.005\n",
            "iteration: 6440 loss: 0.0324 lr: 0.005\n",
            "iteration: 6450 loss: 0.0381 lr: 0.005\n",
            "iteration: 6460 loss: 0.0195 lr: 0.005\n",
            "iteration: 6470 loss: 0.0217 lr: 0.005\n",
            "iteration: 6480 loss: 0.0182 lr: 0.005\n",
            "iteration: 6490 loss: 0.0334 lr: 0.005\n",
            "iteration: 6500 loss: 0.0259 lr: 0.005\n",
            "iteration: 6510 loss: 0.0274 lr: 0.005\n",
            "iteration: 6520 loss: 0.0184 lr: 0.005\n",
            "iteration: 6530 loss: 0.0425 lr: 0.005\n",
            "iteration: 6540 loss: 0.0145 lr: 0.005\n",
            "iteration: 6550 loss: 0.0129 lr: 0.005\n",
            "iteration: 6560 loss: 0.0476 lr: 0.005\n",
            "iteration: 6570 loss: 0.0150 lr: 0.005\n",
            "iteration: 6580 loss: 0.0112 lr: 0.005\n",
            "iteration: 6590 loss: 0.0335 lr: 0.005\n",
            "iteration: 6600 loss: 0.0302 lr: 0.005\n",
            "iteration: 6610 loss: 0.0200 lr: 0.005\n",
            "iteration: 6620 loss: 0.0434 lr: 0.005\n",
            "iteration: 6630 loss: 0.0126 lr: 0.005\n",
            "iteration: 6640 loss: 0.0435 lr: 0.005\n",
            "iteration: 6650 loss: 0.0161 lr: 0.005\n",
            "iteration: 6660 loss: 0.0209 lr: 0.005\n",
            "iteration: 6670 loss: 0.0184 lr: 0.005\n",
            "iteration: 6680 loss: 0.0235 lr: 0.005\n",
            "iteration: 6690 loss: 0.0413 lr: 0.005\n",
            "iteration: 6700 loss: 0.0199 lr: 0.005\n",
            "iteration: 6710 loss: 0.0210 lr: 0.005\n",
            "iteration: 6720 loss: 0.0203 lr: 0.005\n",
            "iteration: 6730 loss: 0.0219 lr: 0.005\n",
            "iteration: 6740 loss: 0.0351 lr: 0.005\n",
            "iteration: 6750 loss: 0.0251 lr: 0.005\n",
            "iteration: 6760 loss: 0.0298 lr: 0.005\n",
            "iteration: 6770 loss: 0.0119 lr: 0.005\n",
            "iteration: 6780 loss: 0.0155 lr: 0.005\n",
            "iteration: 6790 loss: 0.0441 lr: 0.005\n",
            "iteration: 6800 loss: 0.0322 lr: 0.005\n",
            "iteration: 6810 loss: 0.0186 lr: 0.005\n",
            "iteration: 6820 loss: 0.0120 lr: 0.005\n",
            "iteration: 6830 loss: 0.0194 lr: 0.005\n",
            "iteration: 6840 loss: 0.0169 lr: 0.005\n",
            "iteration: 6850 loss: 0.0248 lr: 0.005\n",
            "iteration: 6860 loss: 0.0324 lr: 0.005\n",
            "iteration: 6870 loss: 0.0209 lr: 0.005\n",
            "iteration: 6880 loss: 0.0210 lr: 0.005\n",
            "iteration: 6890 loss: 0.0379 lr: 0.005\n",
            "iteration: 6900 loss: 0.0107 lr: 0.005\n",
            "iteration: 6910 loss: 0.0217 lr: 0.005\n",
            "iteration: 6920 loss: 0.0263 lr: 0.005\n",
            "iteration: 6930 loss: 0.0078 lr: 0.005\n",
            "iteration: 6940 loss: 0.0185 lr: 0.005\n",
            "iteration: 6950 loss: 0.0232 lr: 0.005\n",
            "iteration: 6960 loss: 0.0373 lr: 0.005\n",
            "iteration: 6970 loss: 0.0148 lr: 0.005\n",
            "iteration: 6980 loss: 0.0291 lr: 0.005\n",
            "iteration: 6990 loss: 0.0165 lr: 0.005\n",
            "iteration: 7000 loss: 0.0387 lr: 0.005\n",
            "iteration: 7010 loss: 0.0078 lr: 0.005\n",
            "iteration: 7020 loss: 0.0242 lr: 0.005\n",
            "iteration: 7030 loss: 0.0149 lr: 0.005\n",
            "iteration: 7040 loss: 0.0164 lr: 0.005\n",
            "iteration: 7050 loss: 0.0267 lr: 0.005\n",
            "iteration: 7060 loss: 0.0273 lr: 0.005\n",
            "iteration: 7070 loss: 0.0161 lr: 0.005\n",
            "iteration: 7080 loss: 0.0134 lr: 0.005\n",
            "iteration: 7090 loss: 0.0207 lr: 0.005\n",
            "iteration: 7100 loss: 0.0209 lr: 0.005\n",
            "iteration: 7110 loss: 0.0361 lr: 0.005\n",
            "iteration: 7120 loss: 0.0329 lr: 0.005\n",
            "iteration: 7130 loss: 0.0272 lr: 0.005\n",
            "iteration: 7140 loss: 0.0282 lr: 0.005\n",
            "iteration: 7150 loss: 0.0373 lr: 0.005\n",
            "iteration: 7160 loss: 0.0091 lr: 0.005\n",
            "iteration: 7170 loss: 0.0124 lr: 0.005\n",
            "iteration: 7180 loss: 0.0359 lr: 0.005\n",
            "iteration: 7190 loss: 0.0299 lr: 0.005\n",
            "iteration: 7200 loss: 0.0144 lr: 0.005\n",
            "iteration: 7210 loss: 0.0285 lr: 0.005\n",
            "iteration: 7220 loss: 0.0200 lr: 0.005\n",
            "iteration: 7230 loss: 0.0405 lr: 0.005\n",
            "iteration: 7240 loss: 0.0274 lr: 0.005\n",
            "iteration: 7250 loss: 0.0135 lr: 0.005\n",
            "iteration: 7260 loss: 0.0186 lr: 0.005\n",
            "iteration: 7270 loss: 0.0337 lr: 0.005\n",
            "iteration: 7280 loss: 0.0244 lr: 0.005\n",
            "iteration: 7290 loss: 0.0182 lr: 0.005\n",
            "iteration: 7300 loss: 0.0222 lr: 0.005\n",
            "iteration: 7310 loss: 0.0204 lr: 0.005\n",
            "iteration: 7320 loss: 0.0273 lr: 0.005\n",
            "iteration: 7330 loss: 0.0309 lr: 0.005\n",
            "iteration: 7340 loss: 0.0352 lr: 0.005\n",
            "iteration: 7350 loss: 0.0309 lr: 0.005\n",
            "iteration: 7360 loss: 0.0275 lr: 0.005\n",
            "iteration: 7370 loss: 0.0153 lr: 0.005\n",
            "iteration: 7380 loss: 0.0364 lr: 0.005\n",
            "iteration: 7390 loss: 0.0274 lr: 0.005\n",
            "iteration: 7400 loss: 0.0274 lr: 0.005\n",
            "iteration: 7410 loss: 0.0095 lr: 0.005\n",
            "iteration: 7420 loss: 0.0159 lr: 0.005\n",
            "iteration: 7430 loss: 0.0131 lr: 0.005\n",
            "iteration: 7440 loss: 0.0267 lr: 0.005\n",
            "iteration: 7450 loss: 0.0197 lr: 0.005\n",
            "iteration: 7460 loss: 0.0301 lr: 0.005\n",
            "iteration: 7470 loss: 0.0324 lr: 0.005\n",
            "iteration: 7480 loss: 0.0366 lr: 0.005\n",
            "iteration: 7490 loss: 0.0152 lr: 0.005\n",
            "iteration: 7500 loss: 0.0203 lr: 0.005\n",
            "iteration: 7510 loss: 0.0280 lr: 0.005\n",
            "iteration: 7520 loss: 0.0216 lr: 0.005\n",
            "iteration: 7530 loss: 0.0350 lr: 0.005\n",
            "iteration: 7540 loss: 0.0187 lr: 0.005\n",
            "iteration: 7550 loss: 0.0220 lr: 0.005\n",
            "iteration: 7560 loss: 0.0098 lr: 0.005\n",
            "iteration: 7570 loss: 0.0167 lr: 0.005\n",
            "iteration: 7580 loss: 0.0204 lr: 0.005\n",
            "iteration: 7590 loss: 0.0349 lr: 0.005\n",
            "iteration: 7600 loss: 0.0101 lr: 0.005\n",
            "iteration: 7610 loss: 0.0281 lr: 0.005\n",
            "iteration: 7620 loss: 0.0222 lr: 0.005\n",
            "iteration: 7630 loss: 0.0145 lr: 0.005\n",
            "iteration: 7640 loss: 0.0207 lr: 0.005\n",
            "iteration: 7650 loss: 0.0253 lr: 0.005\n",
            "iteration: 7660 loss: 0.0207 lr: 0.005\n",
            "iteration: 7670 loss: 0.0263 lr: 0.005\n",
            "iteration: 7680 loss: 0.0149 lr: 0.005\n",
            "iteration: 7690 loss: 0.0320 lr: 0.005\n",
            "iteration: 7700 loss: 0.0308 lr: 0.005\n",
            "iteration: 7710 loss: 0.0235 lr: 0.005\n",
            "iteration: 7720 loss: 0.0175 lr: 0.005\n",
            "iteration: 7730 loss: 0.0267 lr: 0.005\n",
            "iteration: 7740 loss: 0.0525 lr: 0.005\n",
            "iteration: 7750 loss: 0.0316 lr: 0.005\n",
            "iteration: 7760 loss: 0.0137 lr: 0.005\n",
            "iteration: 7770 loss: 0.0273 lr: 0.005\n",
            "iteration: 7780 loss: 0.0145 lr: 0.005\n",
            "iteration: 7790 loss: 0.0310 lr: 0.005\n",
            "iteration: 7800 loss: 0.0247 lr: 0.005\n",
            "iteration: 7810 loss: 0.0240 lr: 0.005\n",
            "iteration: 7820 loss: 0.0329 lr: 0.005\n",
            "iteration: 7830 loss: 0.0383 lr: 0.005\n",
            "iteration: 7840 loss: 0.0322 lr: 0.005\n",
            "iteration: 7850 loss: 0.0198 lr: 0.005\n",
            "iteration: 7860 loss: 0.0287 lr: 0.005\n",
            "iteration: 7870 loss: 0.0335 lr: 0.005\n",
            "iteration: 7880 loss: 0.0201 lr: 0.005\n",
            "iteration: 7890 loss: 0.0310 lr: 0.005\n",
            "iteration: 7900 loss: 0.0178 lr: 0.005\n",
            "iteration: 7910 loss: 0.0160 lr: 0.005\n",
            "iteration: 7920 loss: 0.0156 lr: 0.005\n",
            "iteration: 7930 loss: 0.0248 lr: 0.005\n",
            "iteration: 7940 loss: 0.0410 lr: 0.005\n",
            "iteration: 7950 loss: 0.0109 lr: 0.005\n",
            "iteration: 7960 loss: 0.0204 lr: 0.005\n",
            "iteration: 7970 loss: 0.0328 lr: 0.005\n",
            "iteration: 7980 loss: 0.0397 lr: 0.005\n",
            "iteration: 7990 loss: 0.0176 lr: 0.005\n",
            "iteration: 8000 loss: 0.0241 lr: 0.005\n",
            "iteration: 8010 loss: 0.0238 lr: 0.005\n",
            "iteration: 8020 loss: 0.0243 lr: 0.005\n",
            "iteration: 8030 loss: 0.0212 lr: 0.005\n",
            "iteration: 8040 loss: 0.0149 lr: 0.005\n",
            "iteration: 8050 loss: 0.0222 lr: 0.005\n",
            "iteration: 8060 loss: 0.0215 lr: 0.005\n",
            "iteration: 8070 loss: 0.0125 lr: 0.005\n",
            "iteration: 8080 loss: 0.0243 lr: 0.005\n",
            "iteration: 8090 loss: 0.0177 lr: 0.005\n",
            "iteration: 8100 loss: 0.0162 lr: 0.005\n",
            "iteration: 8110 loss: 0.0279 lr: 0.005\n",
            "iteration: 8120 loss: 0.0303 lr: 0.005\n",
            "iteration: 8130 loss: 0.0128 lr: 0.005\n",
            "iteration: 8140 loss: 0.0134 lr: 0.005\n",
            "iteration: 8150 loss: 0.0130 lr: 0.005\n",
            "iteration: 8160 loss: 0.0225 lr: 0.005\n",
            "iteration: 8170 loss: 0.0305 lr: 0.005\n",
            "iteration: 8180 loss: 0.0230 lr: 0.005\n",
            "iteration: 8190 loss: 0.0162 lr: 0.005\n",
            "iteration: 8200 loss: 0.0245 lr: 0.005\n",
            "iteration: 8210 loss: 0.0235 lr: 0.005\n",
            "iteration: 8220 loss: 0.0177 lr: 0.005\n",
            "iteration: 8230 loss: 0.0233 lr: 0.005\n",
            "iteration: 8240 loss: 0.0282 lr: 0.005\n",
            "iteration: 8250 loss: 0.0240 lr: 0.005\n",
            "iteration: 8260 loss: 0.0221 lr: 0.005\n",
            "iteration: 8270 loss: 0.0249 lr: 0.005\n",
            "iteration: 8280 loss: 0.0277 lr: 0.005\n",
            "iteration: 8290 loss: 0.0204 lr: 0.005\n",
            "iteration: 8300 loss: 0.0218 lr: 0.005\n",
            "iteration: 8310 loss: 0.0348 lr: 0.005\n",
            "iteration: 8320 loss: 0.0121 lr: 0.005\n",
            "iteration: 8330 loss: 0.0400 lr: 0.005\n",
            "iteration: 8340 loss: 0.0105 lr: 0.005\n",
            "iteration: 8350 loss: 0.0142 lr: 0.005\n",
            "iteration: 8360 loss: 0.0142 lr: 0.005\n",
            "iteration: 8370 loss: 0.0322 lr: 0.005\n",
            "iteration: 8380 loss: 0.0141 lr: 0.005\n",
            "iteration: 8390 loss: 0.0046 lr: 0.005\n",
            "iteration: 8400 loss: 0.0241 lr: 0.005\n",
            "iteration: 8410 loss: 0.0115 lr: 0.005\n",
            "iteration: 8420 loss: 0.0142 lr: 0.005\n",
            "iteration: 8430 loss: 0.0195 lr: 0.005\n",
            "iteration: 8440 loss: 0.0259 lr: 0.005\n",
            "iteration: 8450 loss: 0.0302 lr: 0.005\n",
            "iteration: 8460 loss: 0.0089 lr: 0.005\n",
            "iteration: 8470 loss: 0.0205 lr: 0.005\n",
            "iteration: 8480 loss: 0.0112 lr: 0.005\n",
            "iteration: 8490 loss: 0.0307 lr: 0.005\n",
            "iteration: 8500 loss: 0.0165 lr: 0.005\n",
            "iteration: 8510 loss: 0.0385 lr: 0.005\n",
            "iteration: 8520 loss: 0.0348 lr: 0.005\n",
            "iteration: 8530 loss: 0.0219 lr: 0.005\n",
            "iteration: 8540 loss: 0.0270 lr: 0.005\n",
            "iteration: 8550 loss: 0.0175 lr: 0.005\n",
            "iteration: 8560 loss: 0.0176 lr: 0.005\n",
            "iteration: 8570 loss: 0.0295 lr: 0.005\n",
            "iteration: 8580 loss: 0.0336 lr: 0.005\n",
            "iteration: 8590 loss: 0.0419 lr: 0.005\n",
            "iteration: 8600 loss: 0.0213 lr: 0.005\n",
            "iteration: 8610 loss: 0.0319 lr: 0.005\n",
            "iteration: 8620 loss: 0.0220 lr: 0.005\n",
            "iteration: 8630 loss: 0.0244 lr: 0.005\n",
            "iteration: 8640 loss: 0.0295 lr: 0.005\n",
            "iteration: 8650 loss: 0.0102 lr: 0.005\n",
            "iteration: 8660 loss: 0.0271 lr: 0.005\n",
            "iteration: 8670 loss: 0.0250 lr: 0.005\n",
            "iteration: 8680 loss: 0.0313 lr: 0.005\n",
            "iteration: 8690 loss: 0.0260 lr: 0.005\n",
            "iteration: 8700 loss: 0.0282 lr: 0.005\n",
            "iteration: 8710 loss: 0.0389 lr: 0.005\n",
            "iteration: 8720 loss: 0.0269 lr: 0.005\n",
            "iteration: 8730 loss: 0.0371 lr: 0.005\n",
            "iteration: 8740 loss: 0.0394 lr: 0.005\n",
            "iteration: 8750 loss: 0.0132 lr: 0.005\n",
            "iteration: 8760 loss: 0.0314 lr: 0.005\n",
            "iteration: 8770 loss: 0.0214 lr: 0.005\n",
            "iteration: 8780 loss: 0.0367 lr: 0.005\n",
            "iteration: 8790 loss: 0.0285 lr: 0.005\n",
            "iteration: 8800 loss: 0.0297 lr: 0.005\n",
            "iteration: 8810 loss: 0.0153 lr: 0.005\n",
            "iteration: 8820 loss: 0.0177 lr: 0.005\n",
            "iteration: 8830 loss: 0.0254 lr: 0.005\n",
            "iteration: 8840 loss: 0.0231 lr: 0.005\n",
            "iteration: 8850 loss: 0.0193 lr: 0.005\n",
            "iteration: 8860 loss: 0.0266 lr: 0.005\n",
            "iteration: 8870 loss: 0.0326 lr: 0.005\n",
            "iteration: 8880 loss: 0.0093 lr: 0.005\n",
            "iteration: 8890 loss: 0.0269 lr: 0.005\n",
            "iteration: 8900 loss: 0.0523 lr: 0.005\n",
            "iteration: 8910 loss: 0.0312 lr: 0.005\n",
            "iteration: 8920 loss: 0.0267 lr: 0.005\n",
            "iteration: 8930 loss: 0.0256 lr: 0.005\n",
            "iteration: 8940 loss: 0.0267 lr: 0.005\n",
            "iteration: 8950 loss: 0.0370 lr: 0.005\n",
            "iteration: 8960 loss: 0.0319 lr: 0.005\n",
            "iteration: 8970 loss: 0.0153 lr: 0.005\n",
            "iteration: 8980 loss: 0.0203 lr: 0.005\n",
            "iteration: 8990 loss: 0.0119 lr: 0.005\n",
            "iteration: 9000 loss: 0.0144 lr: 0.005\n",
            "iteration: 9010 loss: 0.0364 lr: 0.005\n",
            "iteration: 9020 loss: 0.0199 lr: 0.005\n",
            "iteration: 9030 loss: 0.0204 lr: 0.005\n",
            "iteration: 9040 loss: 0.0226 lr: 0.005\n",
            "iteration: 9050 loss: 0.0255 lr: 0.005\n",
            "iteration: 9060 loss: 0.0301 lr: 0.005\n",
            "iteration: 9070 loss: 0.0160 lr: 0.005\n",
            "iteration: 9080 loss: 0.0309 lr: 0.005\n",
            "iteration: 9090 loss: 0.0192 lr: 0.005\n",
            "iteration: 9100 loss: 0.0267 lr: 0.005\n",
            "iteration: 9110 loss: 0.0074 lr: 0.005\n",
            "iteration: 9120 loss: 0.0421 lr: 0.005\n",
            "iteration: 9130 loss: 0.0217 lr: 0.005\n",
            "iteration: 9140 loss: 0.0255 lr: 0.005\n",
            "iteration: 9150 loss: 0.0337 lr: 0.005\n",
            "iteration: 9160 loss: 0.0183 lr: 0.005\n",
            "iteration: 9170 loss: 0.0341 lr: 0.005\n",
            "iteration: 9180 loss: 0.0396 lr: 0.005\n",
            "iteration: 9190 loss: 0.0167 lr: 0.005\n",
            "iteration: 9200 loss: 0.0167 lr: 0.005\n",
            "iteration: 9210 loss: 0.0244 lr: 0.005\n",
            "iteration: 9220 loss: 0.0228 lr: 0.005\n",
            "iteration: 9230 loss: 0.0503 lr: 0.005\n",
            "iteration: 9240 loss: 0.0213 lr: 0.005\n",
            "iteration: 9250 loss: 0.0300 lr: 0.005\n",
            "iteration: 9260 loss: 0.0275 lr: 0.005\n",
            "iteration: 9270 loss: 0.0253 lr: 0.005\n",
            "iteration: 9280 loss: 0.0399 lr: 0.005\n",
            "iteration: 9290 loss: 0.0312 lr: 0.005\n",
            "iteration: 9300 loss: 0.0323 lr: 0.005\n",
            "iteration: 9310 loss: 0.0280 lr: 0.005\n",
            "iteration: 9320 loss: 0.0164 lr: 0.005\n",
            "iteration: 9330 loss: 0.0153 lr: 0.005\n",
            "iteration: 9340 loss: 0.0255 lr: 0.005\n",
            "iteration: 9350 loss: 0.0333 lr: 0.005\n",
            "iteration: 9360 loss: 0.0218 lr: 0.005\n",
            "iteration: 9370 loss: 0.0183 lr: 0.005\n",
            "iteration: 9380 loss: 0.0299 lr: 0.005\n",
            "iteration: 9390 loss: 0.0289 lr: 0.005\n",
            "iteration: 9400 loss: 0.0294 lr: 0.005\n",
            "iteration: 9410 loss: 0.0167 lr: 0.005\n",
            "iteration: 9420 loss: 0.0310 lr: 0.005\n",
            "iteration: 9430 loss: 0.0257 lr: 0.005\n",
            "iteration: 9440 loss: 0.0341 lr: 0.005\n",
            "iteration: 9450 loss: 0.0243 lr: 0.005\n",
            "iteration: 9460 loss: 0.0193 lr: 0.005\n",
            "iteration: 9470 loss: 0.0378 lr: 0.005\n",
            "iteration: 9480 loss: 0.0124 lr: 0.005\n",
            "iteration: 9490 loss: 0.0212 lr: 0.005\n",
            "iteration: 9500 loss: 0.0339 lr: 0.005\n",
            "iteration: 9510 loss: 0.0258 lr: 0.005\n",
            "iteration: 9520 loss: 0.0264 lr: 0.005\n",
            "iteration: 9530 loss: 0.0463 lr: 0.005\n",
            "iteration: 9540 loss: 0.0213 lr: 0.005\n",
            "iteration: 9550 loss: 0.0439 lr: 0.005\n",
            "iteration: 9560 loss: 0.0323 lr: 0.005\n",
            "iteration: 9570 loss: 0.0309 lr: 0.005\n",
            "iteration: 9580 loss: 0.0122 lr: 0.005\n",
            "iteration: 9590 loss: 0.0260 lr: 0.005\n",
            "iteration: 9600 loss: 0.0213 lr: 0.005\n",
            "iteration: 9610 loss: 0.0358 lr: 0.005\n",
            "iteration: 9620 loss: 0.0229 lr: 0.005\n",
            "iteration: 9630 loss: 0.0377 lr: 0.005\n",
            "iteration: 9640 loss: 0.0276 lr: 0.005\n",
            "iteration: 9650 loss: 0.0359 lr: 0.005\n",
            "iteration: 9660 loss: 0.0149 lr: 0.005\n",
            "iteration: 9670 loss: 0.0222 lr: 0.005\n",
            "iteration: 9680 loss: 0.0093 lr: 0.005\n",
            "iteration: 9690 loss: 0.0243 lr: 0.005\n",
            "iteration: 9700 loss: 0.0191 lr: 0.005\n",
            "iteration: 9710 loss: 0.0340 lr: 0.005\n",
            "iteration: 9720 loss: 0.0380 lr: 0.005\n",
            "iteration: 9730 loss: 0.0256 lr: 0.005\n",
            "iteration: 9740 loss: 0.0277 lr: 0.005\n",
            "iteration: 9750 loss: 0.0108 lr: 0.005\n",
            "iteration: 9760 loss: 0.0310 lr: 0.005\n",
            "iteration: 9770 loss: 0.0370 lr: 0.005\n",
            "iteration: 9780 loss: 0.0217 lr: 0.005\n",
            "iteration: 9790 loss: 0.0121 lr: 0.005\n",
            "iteration: 9800 loss: 0.0430 lr: 0.005\n",
            "iteration: 9810 loss: 0.0141 lr: 0.005\n",
            "iteration: 9820 loss: 0.0228 lr: 0.005\n",
            "iteration: 9830 loss: 0.0196 lr: 0.005\n",
            "iteration: 9840 loss: 0.0244 lr: 0.005\n",
            "iteration: 9850 loss: 0.0258 lr: 0.005\n",
            "iteration: 9860 loss: 0.0273 lr: 0.005\n",
            "iteration: 9870 loss: 0.0349 lr: 0.005\n",
            "iteration: 9880 loss: 0.0237 lr: 0.005\n",
            "iteration: 9890 loss: 0.0198 lr: 0.005\n",
            "iteration: 9900 loss: 0.0328 lr: 0.005\n",
            "iteration: 9910 loss: 0.0328 lr: 0.005\n",
            "iteration: 9920 loss: 0.0110 lr: 0.005\n",
            "iteration: 9930 loss: 0.0316 lr: 0.005\n",
            "iteration: 9940 loss: 0.0116 lr: 0.005\n",
            "iteration: 9950 loss: 0.0237 lr: 0.005\n",
            "iteration: 9960 loss: 0.0326 lr: 0.005\n",
            "iteration: 9970 loss: 0.0320 lr: 0.005\n",
            "iteration: 9980 loss: 0.0226 lr: 0.005\n",
            "iteration: 9990 loss: 0.0334 lr: 0.005\n",
            "iteration: 10000 loss: 0.0312 lr: 0.005\n",
            "iteration: 10010 loss: 0.0276 lr: 0.02\n",
            "iteration: 10020 loss: 0.0173 lr: 0.02\n",
            "iteration: 10030 loss: 0.0190 lr: 0.02\n",
            "iteration: 10040 loss: 0.0426 lr: 0.02\n",
            "iteration: 10050 loss: 0.0487 lr: 0.02\n",
            "iteration: 10060 loss: 0.0338 lr: 0.02\n",
            "iteration: 10070 loss: 0.0342 lr: 0.02\n",
            "iteration: 10080 loss: 0.0247 lr: 0.02\n",
            "iteration: 10090 loss: 0.0263 lr: 0.02\n",
            "iteration: 10100 loss: 0.0247 lr: 0.02\n",
            "iteration: 10110 loss: 0.0369 lr: 0.02\n",
            "iteration: 10120 loss: 0.0361 lr: 0.02\n",
            "iteration: 10130 loss: 0.0276 lr: 0.02\n",
            "iteration: 10140 loss: 0.0195 lr: 0.02\n",
            "iteration: 10150 loss: 0.0354 lr: 0.02\n",
            "iteration: 10160 loss: 0.0236 lr: 0.02\n",
            "iteration: 10170 loss: 0.0243 lr: 0.02\n",
            "iteration: 10180 loss: 0.0442 lr: 0.02\n",
            "iteration: 10190 loss: 0.0327 lr: 0.02\n",
            "iteration: 10200 loss: 0.0272 lr: 0.02\n",
            "iteration: 10210 loss: 0.0238 lr: 0.02\n",
            "iteration: 10220 loss: 0.0344 lr: 0.02\n",
            "iteration: 10230 loss: 0.0377 lr: 0.02\n",
            "iteration: 10240 loss: 0.0271 lr: 0.02\n",
            "iteration: 10250 loss: 0.0262 lr: 0.02\n",
            "iteration: 10260 loss: 0.0319 lr: 0.02\n",
            "iteration: 10270 loss: 0.0429 lr: 0.02\n",
            "iteration: 10280 loss: 0.0495 lr: 0.02\n",
            "iteration: 10290 loss: 0.0267 lr: 0.02\n",
            "iteration: 10300 loss: 0.0313 lr: 0.02\n",
            "iteration: 10310 loss: 0.0165 lr: 0.02\n",
            "iteration: 10320 loss: 0.0319 lr: 0.02\n",
            "iteration: 10330 loss: 0.0259 lr: 0.02\n",
            "iteration: 10340 loss: 0.0362 lr: 0.02\n",
            "iteration: 10350 loss: 0.0238 lr: 0.02\n",
            "iteration: 10360 loss: 0.0229 lr: 0.02\n",
            "iteration: 10370 loss: 0.0266 lr: 0.02\n",
            "iteration: 10380 loss: 0.0324 lr: 0.02\n",
            "iteration: 10390 loss: 0.0122 lr: 0.02\n",
            "iteration: 10400 loss: 0.0351 lr: 0.02\n",
            "iteration: 10410 loss: 0.0307 lr: 0.02\n",
            "iteration: 10420 loss: 0.0190 lr: 0.02\n",
            "iteration: 10430 loss: 0.0277 lr: 0.02\n",
            "iteration: 10440 loss: 0.0361 lr: 0.02\n",
            "iteration: 10450 loss: 0.0190 lr: 0.02\n",
            "iteration: 10460 loss: 0.0301 lr: 0.02\n",
            "iteration: 10470 loss: 0.0171 lr: 0.02\n",
            "iteration: 10480 loss: 0.0223 lr: 0.02\n",
            "iteration: 10490 loss: 0.0167 lr: 0.02\n",
            "iteration: 10500 loss: 0.0296 lr: 0.02\n",
            "iteration: 10510 loss: 0.0275 lr: 0.02\n",
            "iteration: 10520 loss: 0.0147 lr: 0.02\n",
            "iteration: 10530 loss: 0.0227 lr: 0.02\n",
            "iteration: 10540 loss: 0.0336 lr: 0.02\n",
            "iteration: 10550 loss: 0.0233 lr: 0.02\n",
            "iteration: 10560 loss: 0.0243 lr: 0.02\n",
            "iteration: 10570 loss: 0.0193 lr: 0.02\n",
            "iteration: 10580 loss: 0.0204 lr: 0.02\n",
            "iteration: 10590 loss: 0.0230 lr: 0.02\n",
            "iteration: 10600 loss: 0.0235 lr: 0.02\n",
            "iteration: 10610 loss: 0.0362 lr: 0.02\n",
            "iteration: 10620 loss: 0.0348 lr: 0.02\n",
            "iteration: 10630 loss: 0.0207 lr: 0.02\n",
            "iteration: 10640 loss: 0.0239 lr: 0.02\n",
            "iteration: 10650 loss: 0.0201 lr: 0.02\n",
            "iteration: 10660 loss: 0.0489 lr: 0.02\n",
            "iteration: 10670 loss: 0.0230 lr: 0.02\n",
            "iteration: 10680 loss: 0.0180 lr: 0.02\n",
            "iteration: 10690 loss: 0.0139 lr: 0.02\n",
            "iteration: 10700 loss: 0.0339 lr: 0.02\n",
            "iteration: 10710 loss: 0.0385 lr: 0.02\n",
            "iteration: 10720 loss: 0.0204 lr: 0.02\n",
            "iteration: 10730 loss: 0.0189 lr: 0.02\n",
            "iteration: 10740 loss: 0.0150 lr: 0.02\n",
            "iteration: 10750 loss: 0.0294 lr: 0.02\n",
            "iteration: 10760 loss: 0.0310 lr: 0.02\n",
            "iteration: 10770 loss: 0.0144 lr: 0.02\n",
            "iteration: 10780 loss: 0.0407 lr: 0.02\n",
            "iteration: 10790 loss: 0.0365 lr: 0.02\n",
            "iteration: 10800 loss: 0.0283 lr: 0.02\n",
            "iteration: 10810 loss: 0.0103 lr: 0.02\n",
            "iteration: 10820 loss: 0.0291 lr: 0.02\n",
            "iteration: 10830 loss: 0.0417 lr: 0.02\n",
            "iteration: 10840 loss: 0.0405 lr: 0.02\n",
            "iteration: 10850 loss: 0.0357 lr: 0.02\n",
            "iteration: 10860 loss: 0.0417 lr: 0.02\n",
            "iteration: 10870 loss: 0.0222 lr: 0.02\n",
            "iteration: 10880 loss: 0.0200 lr: 0.02\n",
            "iteration: 10890 loss: 0.0350 lr: 0.02\n",
            "iteration: 10900 loss: 0.0411 lr: 0.02\n",
            "iteration: 10910 loss: 0.0254 lr: 0.02\n",
            "iteration: 10920 loss: 0.0236 lr: 0.02\n",
            "iteration: 10930 loss: 0.0190 lr: 0.02\n",
            "iteration: 10940 loss: 0.0189 lr: 0.02\n",
            "iteration: 10950 loss: 0.0138 lr: 0.02\n",
            "iteration: 10960 loss: 0.0322 lr: 0.02\n",
            "iteration: 10970 loss: 0.0319 lr: 0.02\n",
            "iteration: 10980 loss: 0.0246 lr: 0.02\n",
            "iteration: 10990 loss: 0.0170 lr: 0.02\n",
            "iteration: 11000 loss: 0.0213 lr: 0.02\n",
            "iteration: 11010 loss: 0.0494 lr: 0.02\n",
            "iteration: 11020 loss: 0.0309 lr: 0.02\n",
            "iteration: 11030 loss: 0.0197 lr: 0.02\n",
            "iteration: 11040 loss: 0.0332 lr: 0.02\n",
            "iteration: 11050 loss: 0.0203 lr: 0.02\n",
            "iteration: 11060 loss: 0.0289 lr: 0.02\n",
            "iteration: 11070 loss: 0.0388 lr: 0.02\n",
            "iteration: 11080 loss: 0.0220 lr: 0.02\n",
            "iteration: 11090 loss: 0.0294 lr: 0.02\n",
            "iteration: 11100 loss: 0.0247 lr: 0.02\n",
            "iteration: 11110 loss: 0.0128 lr: 0.02\n",
            "iteration: 11120 loss: 0.0160 lr: 0.02\n",
            "iteration: 11130 loss: 0.0214 lr: 0.02\n",
            "iteration: 11140 loss: 0.0401 lr: 0.02\n",
            "iteration: 11150 loss: 0.0314 lr: 0.02\n",
            "iteration: 11160 loss: 0.0380 lr: 0.02\n",
            "iteration: 11170 loss: 0.0088 lr: 0.02\n",
            "iteration: 11180 loss: 0.0334 lr: 0.02\n",
            "iteration: 11190 loss: 0.0250 lr: 0.02\n",
            "iteration: 11200 loss: 0.0380 lr: 0.02\n",
            "iteration: 11210 loss: 0.0208 lr: 0.02\n",
            "iteration: 11220 loss: 0.0279 lr: 0.02\n",
            "iteration: 11230 loss: 0.0174 lr: 0.02\n",
            "iteration: 11240 loss: 0.0218 lr: 0.02\n",
            "iteration: 11250 loss: 0.0145 lr: 0.02\n",
            "iteration: 11260 loss: 0.0259 lr: 0.02\n",
            "iteration: 11270 loss: 0.0235 lr: 0.02\n",
            "iteration: 11280 loss: 0.0406 lr: 0.02\n",
            "iteration: 11290 loss: 0.0175 lr: 0.02\n",
            "iteration: 11300 loss: 0.0406 lr: 0.02\n",
            "iteration: 11310 loss: 0.0366 lr: 0.02\n",
            "iteration: 11320 loss: 0.0290 lr: 0.02\n",
            "iteration: 11330 loss: 0.0259 lr: 0.02\n",
            "iteration: 11340 loss: 0.0103 lr: 0.02\n",
            "iteration: 11350 loss: 0.0277 lr: 0.02\n",
            "iteration: 11360 loss: 0.0360 lr: 0.02\n",
            "iteration: 11370 loss: 0.0313 lr: 0.02\n",
            "iteration: 11380 loss: 0.0500 lr: 0.02\n",
            "iteration: 11390 loss: 0.0365 lr: 0.02\n",
            "iteration: 11400 loss: 0.0244 lr: 0.02\n",
            "iteration: 11410 loss: 0.0281 lr: 0.02\n",
            "iteration: 11420 loss: 0.0249 lr: 0.02\n",
            "iteration: 11430 loss: 0.0197 lr: 0.02\n",
            "iteration: 11440 loss: 0.0101 lr: 0.02\n",
            "iteration: 11450 loss: 0.0287 lr: 0.02\n",
            "iteration: 11460 loss: 0.0145 lr: 0.02\n",
            "iteration: 11470 loss: 0.0485 lr: 0.02\n",
            "iteration: 11480 loss: 0.0236 lr: 0.02\n",
            "iteration: 11490 loss: 0.0268 lr: 0.02\n",
            "iteration: 11500 loss: 0.0300 lr: 0.02\n",
            "iteration: 11510 loss: 0.0280 lr: 0.02\n",
            "iteration: 11520 loss: 0.0163 lr: 0.02\n",
            "iteration: 11530 loss: 0.0222 lr: 0.02\n",
            "iteration: 11540 loss: 0.0294 lr: 0.02\n",
            "iteration: 11550 loss: 0.0213 lr: 0.02\n",
            "iteration: 11560 loss: 0.0227 lr: 0.02\n",
            "iteration: 11570 loss: 0.0200 lr: 0.02\n",
            "iteration: 11580 loss: 0.0322 lr: 0.02\n",
            "iteration: 11590 loss: 0.0367 lr: 0.02\n",
            "iteration: 11600 loss: 0.0232 lr: 0.02\n",
            "iteration: 11610 loss: 0.0313 lr: 0.02\n",
            "iteration: 11620 loss: 0.0328 lr: 0.02\n",
            "iteration: 11630 loss: 0.0160 lr: 0.02\n",
            "iteration: 11640 loss: 0.0305 lr: 0.02\n",
            "iteration: 11650 loss: 0.0311 lr: 0.02\n",
            "iteration: 11660 loss: 0.0347 lr: 0.02\n",
            "iteration: 11670 loss: 0.0219 lr: 0.02\n",
            "iteration: 11680 loss: 0.0172 lr: 0.02\n",
            "iteration: 11690 loss: 0.0243 lr: 0.02\n",
            "iteration: 11700 loss: 0.0199 lr: 0.02\n",
            "iteration: 11710 loss: 0.0345 lr: 0.02\n",
            "iteration: 11720 loss: 0.0254 lr: 0.02\n",
            "iteration: 11730 loss: 0.0185 lr: 0.02\n",
            "iteration: 11740 loss: 0.0231 lr: 0.02\n",
            "iteration: 11750 loss: 0.0290 lr: 0.02\n",
            "iteration: 11760 loss: 0.0198 lr: 0.02\n",
            "iteration: 11770 loss: 0.0350 lr: 0.02\n",
            "iteration: 11780 loss: 0.0221 lr: 0.02\n",
            "iteration: 11790 loss: 0.0147 lr: 0.02\n",
            "iteration: 11800 loss: 0.0302 lr: 0.02\n",
            "iteration: 11810 loss: 0.0227 lr: 0.02\n",
            "iteration: 11820 loss: 0.0195 lr: 0.02\n",
            "iteration: 11830 loss: 0.0289 lr: 0.02\n",
            "iteration: 11840 loss: 0.0240 lr: 0.02\n",
            "iteration: 11850 loss: 0.0418 lr: 0.02\n",
            "iteration: 11860 loss: 0.0227 lr: 0.02\n",
            "iteration: 11870 loss: 0.0360 lr: 0.02\n",
            "iteration: 11880 loss: 0.0297 lr: 0.02\n",
            "iteration: 11890 loss: 0.0180 lr: 0.02\n",
            "iteration: 11900 loss: 0.0380 lr: 0.02\n",
            "iteration: 11910 loss: 0.0370 lr: 0.02\n",
            "iteration: 11920 loss: 0.0229 lr: 0.02\n",
            "iteration: 11930 loss: 0.0260 lr: 0.02\n",
            "iteration: 11940 loss: 0.0350 lr: 0.02\n",
            "iteration: 11950 loss: 0.0194 lr: 0.02\n",
            "iteration: 11960 loss: 0.0197 lr: 0.02\n",
            "iteration: 11970 loss: 0.0125 lr: 0.02\n",
            "iteration: 11980 loss: 0.0217 lr: 0.02\n",
            "iteration: 11990 loss: 0.0270 lr: 0.02\n",
            "iteration: 12000 loss: 0.0275 lr: 0.02\n",
            "iteration: 12010 loss: 0.0364 lr: 0.02\n",
            "iteration: 12020 loss: 0.0261 lr: 0.02\n",
            "iteration: 12030 loss: 0.0255 lr: 0.02\n",
            "iteration: 12040 loss: 0.0261 lr: 0.02\n",
            "iteration: 12050 loss: 0.0303 lr: 0.02\n",
            "iteration: 12060 loss: 0.0192 lr: 0.02\n",
            "iteration: 12070 loss: 0.0208 lr: 0.02\n",
            "iteration: 12080 loss: 0.0496 lr: 0.02\n",
            "iteration: 12090 loss: 0.0252 lr: 0.02\n",
            "iteration: 12100 loss: 0.0316 lr: 0.02\n",
            "iteration: 12110 loss: 0.0141 lr: 0.02\n",
            "iteration: 12120 loss: 0.0366 lr: 0.02\n",
            "iteration: 12130 loss: 0.0312 lr: 0.02\n",
            "iteration: 12140 loss: 0.0082 lr: 0.02\n",
            "iteration: 12150 loss: 0.0247 lr: 0.02\n",
            "iteration: 12160 loss: 0.0467 lr: 0.02\n",
            "iteration: 12170 loss: 0.0327 lr: 0.02\n",
            "iteration: 12180 loss: 0.0291 lr: 0.02\n",
            "iteration: 12190 loss: 0.0312 lr: 0.02\n",
            "iteration: 12200 loss: 0.0191 lr: 0.02\n",
            "iteration: 12210 loss: 0.0250 lr: 0.02\n",
            "iteration: 12220 loss: 0.0289 lr: 0.02\n",
            "iteration: 12230 loss: 0.0223 lr: 0.02\n",
            "iteration: 12240 loss: 0.0160 lr: 0.02\n",
            "iteration: 12250 loss: 0.0201 lr: 0.02\n",
            "iteration: 12260 loss: 0.0389 lr: 0.02\n",
            "iteration: 12270 loss: 0.0204 lr: 0.02\n",
            "iteration: 12280 loss: 0.0213 lr: 0.02\n",
            "iteration: 12290 loss: 0.0300 lr: 0.02\n",
            "iteration: 12300 loss: 0.0164 lr: 0.02\n",
            "iteration: 12310 loss: 0.0359 lr: 0.02\n",
            "iteration: 12320 loss: 0.0455 lr: 0.02\n",
            "iteration: 12330 loss: 0.0246 lr: 0.02\n",
            "iteration: 12340 loss: 0.0213 lr: 0.02\n",
            "iteration: 12350 loss: 0.0208 lr: 0.02\n",
            "iteration: 12360 loss: 0.0242 lr: 0.02\n",
            "iteration: 12370 loss: 0.0204 lr: 0.02\n",
            "iteration: 12380 loss: 0.0283 lr: 0.02\n",
            "iteration: 12390 loss: 0.0477 lr: 0.02\n",
            "iteration: 12400 loss: 0.0120 lr: 0.02\n",
            "iteration: 12410 loss: 0.0221 lr: 0.02\n",
            "iteration: 12420 loss: 0.0236 lr: 0.02\n",
            "iteration: 12430 loss: 0.0346 lr: 0.02\n",
            "iteration: 12440 loss: 0.0316 lr: 0.02\n",
            "iteration: 12450 loss: 0.0286 lr: 0.02\n",
            "iteration: 12460 loss: 0.0273 lr: 0.02\n",
            "iteration: 12470 loss: 0.0208 lr: 0.02\n",
            "iteration: 12480 loss: 0.0176 lr: 0.02\n",
            "iteration: 12490 loss: 0.0400 lr: 0.02\n",
            "iteration: 12500 loss: 0.0256 lr: 0.02\n",
            "iteration: 12510 loss: 0.0220 lr: 0.02\n",
            "iteration: 12520 loss: 0.0182 lr: 0.02\n",
            "iteration: 12530 loss: 0.0270 lr: 0.02\n",
            "iteration: 12540 loss: 0.0229 lr: 0.02\n",
            "iteration: 12550 loss: 0.0135 lr: 0.02\n",
            "iteration: 12560 loss: 0.0336 lr: 0.02\n",
            "iteration: 12570 loss: 0.0242 lr: 0.02\n",
            "iteration: 12580 loss: 0.0190 lr: 0.02\n",
            "iteration: 12590 loss: 0.0387 lr: 0.02\n",
            "iteration: 12600 loss: 0.0235 lr: 0.02\n",
            "iteration: 12610 loss: 0.0196 lr: 0.02\n",
            "iteration: 12620 loss: 0.0449 lr: 0.02\n",
            "iteration: 12630 loss: 0.0324 lr: 0.02\n",
            "iteration: 12640 loss: 0.0197 lr: 0.02\n",
            "iteration: 12650 loss: 0.0272 lr: 0.02\n",
            "iteration: 12660 loss: 0.0108 lr: 0.02\n",
            "iteration: 12670 loss: 0.0179 lr: 0.02\n",
            "iteration: 12680 loss: 0.0254 lr: 0.02\n",
            "iteration: 12690 loss: 0.0167 lr: 0.02\n",
            "iteration: 12700 loss: 0.0246 lr: 0.02\n",
            "iteration: 12710 loss: 0.0136 lr: 0.02\n",
            "iteration: 12720 loss: 0.0142 lr: 0.02\n",
            "iteration: 12730 loss: 0.0279 lr: 0.02\n",
            "iteration: 12740 loss: 0.0214 lr: 0.02\n",
            "iteration: 12750 loss: 0.0203 lr: 0.02\n",
            "iteration: 12760 loss: 0.0373 lr: 0.02\n",
            "iteration: 12770 loss: 0.0395 lr: 0.02\n",
            "iteration: 12780 loss: 0.0350 lr: 0.02\n",
            "iteration: 12790 loss: 0.0131 lr: 0.02\n",
            "iteration: 12800 loss: 0.0424 lr: 0.02\n",
            "iteration: 12810 loss: 0.0413 lr: 0.02\n",
            "iteration: 12820 loss: 0.0474 lr: 0.02\n",
            "iteration: 12830 loss: 0.0307 lr: 0.02\n",
            "iteration: 12840 loss: 0.0076 lr: 0.02\n",
            "iteration: 12850 loss: 0.0269 lr: 0.02\n",
            "iteration: 12860 loss: 0.0138 lr: 0.02\n",
            "iteration: 12870 loss: 0.0120 lr: 0.02\n",
            "iteration: 12880 loss: 0.0228 lr: 0.02\n",
            "iteration: 12890 loss: 0.0176 lr: 0.02\n",
            "iteration: 12900 loss: 0.0315 lr: 0.02\n",
            "iteration: 12910 loss: 0.0154 lr: 0.02\n",
            "iteration: 12920 loss: 0.0106 lr: 0.02\n",
            "iteration: 12930 loss: 0.0342 lr: 0.02\n",
            "iteration: 12940 loss: 0.0265 lr: 0.02\n",
            "iteration: 12950 loss: 0.0265 lr: 0.02\n",
            "iteration: 12960 loss: 0.0347 lr: 0.02\n",
            "iteration: 12970 loss: 0.0337 lr: 0.02\n",
            "iteration: 12980 loss: 0.0218 lr: 0.02\n",
            "iteration: 12990 loss: 0.0376 lr: 0.02\n",
            "iteration: 13000 loss: 0.0449 lr: 0.02\n",
            "iteration: 13010 loss: 0.0164 lr: 0.02\n",
            "iteration: 13020 loss: 0.0245 lr: 0.02\n",
            "iteration: 13030 loss: 0.0137 lr: 0.02\n",
            "iteration: 13040 loss: 0.0353 lr: 0.02\n",
            "iteration: 13050 loss: 0.0370 lr: 0.02\n",
            "iteration: 13060 loss: 0.0413 lr: 0.02\n",
            "iteration: 13070 loss: 0.0086 lr: 0.02\n",
            "iteration: 13080 loss: 0.0236 lr: 0.02\n",
            "iteration: 13090 loss: 0.0180 lr: 0.02\n",
            "iteration: 13100 loss: 0.0308 lr: 0.02\n",
            "iteration: 13110 loss: 0.0194 lr: 0.02\n",
            "iteration: 13120 loss: 0.0126 lr: 0.02\n",
            "iteration: 13130 loss: 0.0156 lr: 0.02\n",
            "iteration: 13140 loss: 0.0208 lr: 0.02\n",
            "iteration: 13150 loss: 0.0224 lr: 0.02\n",
            "iteration: 13160 loss: 0.0255 lr: 0.02\n",
            "iteration: 13170 loss: 0.0158 lr: 0.02\n",
            "iteration: 13180 loss: 0.0272 lr: 0.02\n",
            "iteration: 13190 loss: 0.0369 lr: 0.02\n",
            "iteration: 13200 loss: 0.0294 lr: 0.02\n",
            "iteration: 13210 loss: 0.0258 lr: 0.02\n",
            "iteration: 13220 loss: 0.0253 lr: 0.02\n",
            "iteration: 13230 loss: 0.0219 lr: 0.02\n",
            "iteration: 13240 loss: 0.0167 lr: 0.02\n",
            "iteration: 13250 loss: 0.0242 lr: 0.02\n",
            "iteration: 13260 loss: 0.0175 lr: 0.02\n",
            "iteration: 13270 loss: 0.0161 lr: 0.02\n",
            "iteration: 13280 loss: 0.0399 lr: 0.02\n",
            "iteration: 13290 loss: 0.0235 lr: 0.02\n",
            "iteration: 13300 loss: 0.0327 lr: 0.02\n",
            "iteration: 13310 loss: 0.0372 lr: 0.02\n",
            "iteration: 13320 loss: 0.0330 lr: 0.02\n",
            "iteration: 13330 loss: 0.0139 lr: 0.02\n",
            "iteration: 13340 loss: 0.0306 lr: 0.02\n",
            "iteration: 13350 loss: 0.0217 lr: 0.02\n",
            "iteration: 13360 loss: 0.0164 lr: 0.02\n",
            "iteration: 13370 loss: 0.0236 lr: 0.02\n",
            "iteration: 13380 loss: 0.0131 lr: 0.02\n",
            "iteration: 13390 loss: 0.0279 lr: 0.02\n",
            "iteration: 13400 loss: 0.0176 lr: 0.02\n",
            "iteration: 13410 loss: 0.0229 lr: 0.02\n",
            "iteration: 13420 loss: 0.0315 lr: 0.02\n",
            "iteration: 13430 loss: 0.0357 lr: 0.02\n",
            "iteration: 13440 loss: 0.0312 lr: 0.02\n",
            "iteration: 13450 loss: 0.0191 lr: 0.02\n",
            "iteration: 13460 loss: 0.0176 lr: 0.02\n",
            "iteration: 13470 loss: 0.0306 lr: 0.02\n",
            "iteration: 13480 loss: 0.0217 lr: 0.02\n",
            "iteration: 13490 loss: 0.0280 lr: 0.02\n",
            "iteration: 13500 loss: 0.0179 lr: 0.02\n",
            "iteration: 13510 loss: 0.0168 lr: 0.02\n",
            "iteration: 13520 loss: 0.0385 lr: 0.02\n",
            "iteration: 13530 loss: 0.0384 lr: 0.02\n",
            "iteration: 13540 loss: 0.0293 lr: 0.02\n",
            "iteration: 13550 loss: 0.0236 lr: 0.02\n",
            "iteration: 13560 loss: 0.0257 lr: 0.02\n",
            "iteration: 13570 loss: 0.0291 lr: 0.02\n",
            "iteration: 13580 loss: 0.0271 lr: 0.02\n",
            "iteration: 13590 loss: 0.0252 lr: 0.02\n",
            "iteration: 13600 loss: 0.0282 lr: 0.02\n",
            "iteration: 13610 loss: 0.0203 lr: 0.02\n",
            "iteration: 13620 loss: 0.0269 lr: 0.02\n",
            "iteration: 13630 loss: 0.0098 lr: 0.02\n",
            "iteration: 13640 loss: 0.0297 lr: 0.02\n",
            "iteration: 13650 loss: 0.0428 lr: 0.02\n",
            "iteration: 13660 loss: 0.0108 lr: 0.02\n",
            "iteration: 13670 loss: 0.0075 lr: 0.02\n",
            "iteration: 13680 loss: 0.0315 lr: 0.02\n",
            "iteration: 13690 loss: 0.0209 lr: 0.02\n",
            "iteration: 13700 loss: 0.0183 lr: 0.02\n",
            "iteration: 13710 loss: 0.0334 lr: 0.02\n",
            "iteration: 13720 loss: 0.0099 lr: 0.02\n",
            "iteration: 13730 loss: 0.0163 lr: 0.02\n",
            "iteration: 13740 loss: 0.0127 lr: 0.02\n",
            "iteration: 13750 loss: 0.0274 lr: 0.02\n",
            "iteration: 13760 loss: 0.0270 lr: 0.02\n",
            "iteration: 13770 loss: 0.0202 lr: 0.02\n",
            "iteration: 13780 loss: 0.0486 lr: 0.02\n",
            "iteration: 13790 loss: 0.0359 lr: 0.02\n",
            "iteration: 13800 loss: 0.0295 lr: 0.02\n",
            "iteration: 13810 loss: 0.0278 lr: 0.02\n",
            "iteration: 13820 loss: 0.0349 lr: 0.02\n",
            "iteration: 13830 loss: 0.0134 lr: 0.02\n",
            "iteration: 13840 loss: 0.0343 lr: 0.02\n",
            "iteration: 13850 loss: 0.0116 lr: 0.02\n",
            "iteration: 13860 loss: 0.0288 lr: 0.02\n",
            "iteration: 13870 loss: 0.0282 lr: 0.02\n",
            "iteration: 13880 loss: 0.0218 lr: 0.02\n",
            "iteration: 13890 loss: 0.0199 lr: 0.02\n",
            "iteration: 13900 loss: 0.0152 lr: 0.02\n",
            "iteration: 13910 loss: 0.0273 lr: 0.02\n",
            "iteration: 13920 loss: 0.0189 lr: 0.02\n",
            "iteration: 13930 loss: 0.0266 lr: 0.02\n",
            "iteration: 13950 loss: 0.0052 lr: 0.02\n",
            "iteration: 13960 loss: 0.0182 lr: 0.02\n",
            "iteration: 13970 loss: 0.0133 lr: 0.02\n",
            "iteration: 13980 loss: 0.0243 lr: 0.02\n",
            "iteration: 13990 loss: 0.0177 lr: 0.02\n",
            "iteration: 14000 loss: 0.0246 lr: 0.02\n",
            "iteration: 14010 loss: 0.0243 lr: 0.02\n",
            "iteration: 14020 loss: 0.0264 lr: 0.02\n",
            "iteration: 14030 loss: 0.0207 lr: 0.02\n",
            "iteration: 14040 loss: 0.0124 lr: 0.02\n",
            "iteration: 14050 loss: 0.0253 lr: 0.02\n",
            "iteration: 14060 loss: 0.0284 lr: 0.02\n",
            "iteration: 14070 loss: 0.0292 lr: 0.02\n",
            "iteration: 14080 loss: 0.0207 lr: 0.02\n",
            "iteration: 14090 loss: 0.0199 lr: 0.02\n",
            "iteration: 14100 loss: 0.0076 lr: 0.02\n",
            "iteration: 14110 loss: 0.0233 lr: 0.02\n",
            "iteration: 14120 loss: 0.0424 lr: 0.02\n",
            "iteration: 14130 loss: 0.0172 lr: 0.02\n",
            "iteration: 14140 loss: 0.0249 lr: 0.02\n",
            "iteration: 14150 loss: 0.0215 lr: 0.02\n",
            "iteration: 14160 loss: 0.0310 lr: 0.02\n",
            "iteration: 14170 loss: 0.0118 lr: 0.02\n",
            "iteration: 14180 loss: 0.0132 lr: 0.02\n",
            "iteration: 14190 loss: 0.0382 lr: 0.02\n",
            "iteration: 14200 loss: 0.0384 lr: 0.02\n",
            "iteration: 14210 loss: 0.0321 lr: 0.02\n",
            "iteration: 14220 loss: 0.0371 lr: 0.02\n",
            "iteration: 14230 loss: 0.0231 lr: 0.02\n",
            "iteration: 14240 loss: 0.0290 lr: 0.02\n",
            "iteration: 14250 loss: 0.0238 lr: 0.02\n",
            "iteration: 14260 loss: 0.0169 lr: 0.02\n",
            "iteration: 14270 loss: 0.0241 lr: 0.02\n",
            "iteration: 14280 loss: 0.0323 lr: 0.02\n",
            "iteration: 14290 loss: 0.0229 lr: 0.02\n",
            "iteration: 14300 loss: 0.0097 lr: 0.02\n",
            "iteration: 14310 loss: 0.0354 lr: 0.02\n",
            "iteration: 14320 loss: 0.0252 lr: 0.02\n",
            "iteration: 14330 loss: 0.0353 lr: 0.02\n",
            "iteration: 14340 loss: 0.0164 lr: 0.02\n",
            "iteration: 14350 loss: 0.0438 lr: 0.02\n",
            "iteration: 14360 loss: 0.0223 lr: 0.02\n",
            "iteration: 14370 loss: 0.0245 lr: 0.02\n",
            "iteration: 14380 loss: 0.0314 lr: 0.02\n",
            "iteration: 14390 loss: 0.0144 lr: 0.02\n",
            "iteration: 14400 loss: 0.0278 lr: 0.02\n",
            "iteration: 14410 loss: 0.0326 lr: 0.02\n",
            "iteration: 14420 loss: 0.0308 lr: 0.02\n",
            "iteration: 14430 loss: 0.0217 lr: 0.02\n",
            "iteration: 14440 loss: 0.0177 lr: 0.02\n",
            "iteration: 14450 loss: 0.0268 lr: 0.02\n",
            "iteration: 14460 loss: 0.0310 lr: 0.02\n",
            "iteration: 14470 loss: 0.0415 lr: 0.02\n",
            "iteration: 14480 loss: 0.0322 lr: 0.02\n",
            "iteration: 14490 loss: 0.0229 lr: 0.02\n",
            "iteration: 14500 loss: 0.0176 lr: 0.02\n",
            "iteration: 14510 loss: 0.0181 lr: 0.02\n",
            "iteration: 14520 loss: 0.0310 lr: 0.02\n",
            "iteration: 14530 loss: 0.0186 lr: 0.02\n",
            "iteration: 14540 loss: 0.0359 lr: 0.02\n",
            "iteration: 14550 loss: 0.0187 lr: 0.02\n",
            "iteration: 14560 loss: 0.0323 lr: 0.02\n",
            "iteration: 14570 loss: 0.0077 lr: 0.02\n",
            "iteration: 14580 loss: 0.0108 lr: 0.02\n",
            "iteration: 14590 loss: 0.0296 lr: 0.02\n",
            "iteration: 14600 loss: 0.0324 lr: 0.02\n",
            "iteration: 14610 loss: 0.0091 lr: 0.02\n",
            "iteration: 14620 loss: 0.0122 lr: 0.02\n",
            "iteration: 14630 loss: 0.0312 lr: 0.02\n",
            "iteration: 14640 loss: 0.0348 lr: 0.02\n",
            "iteration: 14650 loss: 0.0331 lr: 0.02\n",
            "iteration: 14660 loss: 0.0280 lr: 0.02\n",
            "iteration: 14670 loss: 0.0201 lr: 0.02\n",
            "iteration: 14680 loss: 0.0216 lr: 0.02\n",
            "iteration: 14690 loss: 0.0229 lr: 0.02\n",
            "iteration: 14700 loss: 0.0194 lr: 0.02\n",
            "iteration: 14710 loss: 0.0259 lr: 0.02\n",
            "iteration: 14720 loss: 0.0253 lr: 0.02\n",
            "iteration: 14730 loss: 0.0162 lr: 0.02\n",
            "iteration: 14740 loss: 0.0182 lr: 0.02\n",
            "iteration: 14750 loss: 0.0281 lr: 0.02\n",
            "iteration: 14760 loss: 0.0116 lr: 0.02\n",
            "iteration: 14770 loss: 0.0238 lr: 0.02\n",
            "iteration: 14780 loss: 0.0296 lr: 0.02\n",
            "iteration: 14790 loss: 0.0100 lr: 0.02\n",
            "iteration: 14800 loss: 0.0411 lr: 0.02\n",
            "iteration: 14810 loss: 0.0307 lr: 0.02\n",
            "iteration: 14820 loss: 0.0300 lr: 0.02\n",
            "iteration: 14830 loss: 0.0284 lr: 0.02\n",
            "iteration: 14840 loss: 0.0324 lr: 0.02\n",
            "iteration: 14850 loss: 0.0199 lr: 0.02\n",
            "iteration: 14860 loss: 0.0278 lr: 0.02\n",
            "iteration: 14870 loss: 0.0197 lr: 0.02\n",
            "iteration: 14880 loss: 0.0185 lr: 0.02\n",
            "iteration: 14890 loss: 0.0254 lr: 0.02\n",
            "iteration: 14900 loss: 0.0237 lr: 0.02\n",
            "iteration: 14910 loss: 0.0205 lr: 0.02\n",
            "iteration: 14920 loss: 0.0304 lr: 0.02\n",
            "iteration: 14930 loss: 0.0395 lr: 0.02\n",
            "iteration: 14940 loss: 0.0233 lr: 0.02\n",
            "iteration: 14950 loss: 0.0297 lr: 0.02\n",
            "iteration: 14960 loss: 0.0122 lr: 0.02\n",
            "iteration: 14970 loss: 0.0122 lr: 0.02\n",
            "iteration: 14980 loss: 0.0300 lr: 0.02\n",
            "iteration: 14990 loss: 0.0307 lr: 0.02\n",
            "iteration: 15000 loss: 0.0287 lr: 0.02\n",
            "iteration: 15010 loss: 0.0174 lr: 0.02\n",
            "iteration: 15020 loss: 0.0228 lr: 0.02\n",
            "iteration: 15030 loss: 0.0245 lr: 0.02\n",
            "iteration: 15040 loss: 0.0430 lr: 0.02\n",
            "iteration: 15050 loss: 0.0471 lr: 0.02\n",
            "iteration: 15060 loss: 0.0294 lr: 0.02\n",
            "iteration: 15070 loss: 0.0202 lr: 0.02\n",
            "iteration: 15080 loss: 0.0189 lr: 0.02\n",
            "iteration: 15090 loss: 0.0156 lr: 0.02\n",
            "iteration: 15100 loss: 0.0204 lr: 0.02\n",
            "iteration: 15110 loss: 0.0380 lr: 0.02\n",
            "iteration: 15120 loss: 0.0368 lr: 0.02\n",
            "iteration: 15130 loss: 0.0262 lr: 0.02\n",
            "iteration: 15140 loss: 0.0246 lr: 0.02\n",
            "iteration: 15150 loss: 0.0312 lr: 0.02\n",
            "iteration: 15160 loss: 0.0163 lr: 0.02\n",
            "iteration: 15170 loss: 0.0217 lr: 0.02\n",
            "iteration: 15180 loss: 0.0228 lr: 0.02\n",
            "iteration: 15190 loss: 0.0327 lr: 0.02\n",
            "iteration: 15200 loss: 0.0132 lr: 0.02\n",
            "iteration: 15210 loss: 0.0335 lr: 0.02\n",
            "iteration: 15220 loss: 0.0390 lr: 0.02\n",
            "iteration: 15230 loss: 0.0356 lr: 0.02\n",
            "iteration: 15240 loss: 0.0154 lr: 0.02\n",
            "iteration: 15250 loss: 0.0214 lr: 0.02\n",
            "iteration: 15260 loss: 0.0340 lr: 0.02\n",
            "iteration: 15270 loss: 0.0138 lr: 0.02\n",
            "iteration: 15280 loss: 0.0124 lr: 0.02\n",
            "iteration: 15290 loss: 0.0224 lr: 0.02\n",
            "iteration: 15300 loss: 0.0388 lr: 0.02\n",
            "iteration: 15310 loss: 0.0338 lr: 0.02\n",
            "iteration: 15320 loss: 0.0277 lr: 0.02\n",
            "iteration: 15330 loss: 0.0278 lr: 0.02\n",
            "iteration: 15340 loss: 0.0233 lr: 0.02\n",
            "iteration: 15350 loss: 0.0380 lr: 0.02\n",
            "iteration: 15360 loss: 0.0169 lr: 0.02\n",
            "iteration: 15370 loss: 0.0265 lr: 0.02\n",
            "iteration: 15380 loss: 0.0216 lr: 0.02\n",
            "iteration: 15390 loss: 0.0178 lr: 0.02\n",
            "iteration: 15400 loss: 0.0181 lr: 0.02\n",
            "iteration: 15410 loss: 0.0287 lr: 0.02\n",
            "iteration: 15420 loss: 0.0256 lr: 0.02\n",
            "iteration: 15430 loss: 0.0144 lr: 0.02\n",
            "iteration: 15440 loss: 0.0193 lr: 0.02\n",
            "iteration: 15450 loss: 0.0467 lr: 0.02\n",
            "iteration: 15460 loss: 0.0084 lr: 0.02\n",
            "iteration: 15470 loss: 0.0067 lr: 0.02\n",
            "iteration: 15480 loss: 0.0270 lr: 0.02\n",
            "iteration: 15490 loss: 0.0291 lr: 0.02\n",
            "iteration: 15500 loss: 0.0203 lr: 0.02\n",
            "iteration: 15510 loss: 0.0376 lr: 0.02\n",
            "iteration: 15520 loss: 0.0278 lr: 0.02\n",
            "iteration: 15530 loss: 0.0155 lr: 0.02\n",
            "iteration: 15540 loss: 0.0320 lr: 0.02\n",
            "iteration: 15550 loss: 0.0255 lr: 0.02\n",
            "iteration: 15560 loss: 0.0198 lr: 0.02\n",
            "iteration: 15570 loss: 0.0134 lr: 0.02\n",
            "iteration: 15580 loss: 0.0367 lr: 0.02\n",
            "iteration: 15590 loss: 0.0244 lr: 0.02\n",
            "iteration: 15600 loss: 0.0075 lr: 0.02\n",
            "iteration: 15610 loss: 0.0120 lr: 0.02\n",
            "iteration: 15620 loss: 0.0282 lr: 0.02\n",
            "iteration: 15630 loss: 0.0192 lr: 0.02\n",
            "iteration: 15640 loss: 0.0277 lr: 0.02\n",
            "iteration: 15650 loss: 0.0091 lr: 0.02\n",
            "iteration: 15660 loss: 0.0247 lr: 0.02\n",
            "iteration: 15670 loss: 0.0245 lr: 0.02\n",
            "iteration: 15680 loss: 0.0189 lr: 0.02\n",
            "iteration: 15690 loss: 0.0258 lr: 0.02\n",
            "iteration: 15700 loss: 0.0169 lr: 0.02\n",
            "iteration: 15710 loss: 0.0319 lr: 0.02\n",
            "iteration: 15720 loss: 0.0209 lr: 0.02\n",
            "iteration: 15730 loss: 0.0306 lr: 0.02\n",
            "iteration: 15740 loss: 0.0451 lr: 0.02\n",
            "iteration: 15750 loss: 0.0195 lr: 0.02\n",
            "iteration: 15760 loss: 0.0171 lr: 0.02\n",
            "iteration: 15770 loss: 0.0215 lr: 0.02\n",
            "iteration: 15780 loss: 0.0287 lr: 0.02\n",
            "iteration: 15790 loss: 0.0268 lr: 0.02\n",
            "iteration: 15800 loss: 0.0163 lr: 0.02\n",
            "iteration: 15810 loss: 0.0402 lr: 0.02\n",
            "iteration: 15820 loss: 0.0189 lr: 0.02\n",
            "iteration: 15830 loss: 0.0245 lr: 0.02\n",
            "iteration: 15840 loss: 0.0141 lr: 0.02\n",
            "iteration: 15850 loss: 0.0250 lr: 0.02\n",
            "iteration: 15860 loss: 0.0290 lr: 0.02\n",
            "iteration: 15870 loss: 0.0166 lr: 0.02\n",
            "iteration: 15880 loss: 0.0343 lr: 0.02\n",
            "iteration: 15890 loss: 0.0183 lr: 0.02\n",
            "iteration: 15900 loss: 0.0184 lr: 0.02\n",
            "iteration: 15910 loss: 0.0261 lr: 0.02\n",
            "iteration: 15920 loss: 0.0245 lr: 0.02\n",
            "iteration: 15930 loss: 0.0169 lr: 0.02\n",
            "iteration: 15940 loss: 0.0275 lr: 0.02\n",
            "iteration: 15950 loss: 0.0208 lr: 0.02\n",
            "iteration: 15960 loss: 0.0349 lr: 0.02\n",
            "iteration: 15970 loss: 0.0166 lr: 0.02\n",
            "iteration: 15980 loss: 0.0127 lr: 0.02\n",
            "iteration: 15990 loss: 0.0118 lr: 0.02\n",
            "iteration: 16000 loss: 0.0392 lr: 0.02\n",
            "iteration: 16010 loss: 0.0238 lr: 0.02\n",
            "iteration: 16020 loss: 0.0188 lr: 0.02\n",
            "iteration: 16030 loss: 0.0084 lr: 0.02\n",
            "iteration: 16040 loss: 0.0363 lr: 0.02\n",
            "iteration: 16050 loss: 0.0134 lr: 0.02\n",
            "iteration: 16060 loss: 0.0212 lr: 0.02\n",
            "iteration: 16070 loss: 0.0257 lr: 0.02\n",
            "iteration: 16080 loss: 0.0369 lr: 0.02\n",
            "iteration: 16090 loss: 0.0104 lr: 0.02\n",
            "iteration: 16100 loss: 0.0252 lr: 0.02\n",
            "iteration: 16110 loss: 0.0183 lr: 0.02\n",
            "iteration: 16120 loss: 0.0219 lr: 0.02\n",
            "iteration: 16130 loss: 0.0477 lr: 0.02\n",
            "iteration: 16140 loss: 0.0200 lr: 0.02\n",
            "iteration: 16150 loss: 0.0110 lr: 0.02\n",
            "iteration: 16160 loss: 0.0106 lr: 0.02\n",
            "iteration: 16170 loss: 0.0332 lr: 0.02\n",
            "iteration: 16180 loss: 0.0306 lr: 0.02\n",
            "iteration: 16190 loss: 0.0264 lr: 0.02\n",
            "iteration: 16200 loss: 0.0231 lr: 0.02\n",
            "iteration: 16210 loss: 0.0111 lr: 0.02\n",
            "iteration: 16220 loss: 0.0202 lr: 0.02\n",
            "iteration: 16230 loss: 0.0172 lr: 0.02\n",
            "iteration: 16240 loss: 0.0314 lr: 0.02\n",
            "iteration: 16250 loss: 0.0250 lr: 0.02\n",
            "iteration: 16260 loss: 0.0251 lr: 0.02\n",
            "iteration: 16270 loss: 0.0393 lr: 0.02\n",
            "iteration: 16280 loss: 0.0272 lr: 0.02\n",
            "iteration: 16290 loss: 0.0134 lr: 0.02\n",
            "iteration: 16300 loss: 0.0192 lr: 0.02\n",
            "iteration: 16310 loss: 0.0318 lr: 0.02\n",
            "iteration: 16320 loss: 0.0254 lr: 0.02\n",
            "iteration: 16330 loss: 0.0207 lr: 0.02\n",
            "iteration: 16340 loss: 0.0337 lr: 0.02\n",
            "iteration: 16350 loss: 0.0255 lr: 0.02\n",
            "iteration: 16360 loss: 0.0267 lr: 0.02\n",
            "iteration: 16370 loss: 0.0390 lr: 0.02\n",
            "iteration: 16380 loss: 0.0191 lr: 0.02\n",
            "iteration: 16390 loss: 0.0076 lr: 0.02\n",
            "iteration: 16400 loss: 0.0199 lr: 0.02\n",
            "iteration: 16410 loss: 0.0213 lr: 0.02\n",
            "iteration: 16420 loss: 0.0115 lr: 0.02\n",
            "iteration: 16430 loss: 0.0140 lr: 0.02\n",
            "iteration: 16440 loss: 0.0214 lr: 0.02\n",
            "iteration: 16450 loss: 0.0115 lr: 0.02\n",
            "iteration: 16460 loss: 0.0275 lr: 0.02\n",
            "iteration: 16470 loss: 0.0217 lr: 0.02\n",
            "iteration: 16480 loss: 0.0259 lr: 0.02\n",
            "iteration: 16490 loss: 0.0263 lr: 0.02\n",
            "iteration: 16500 loss: 0.0292 lr: 0.02\n",
            "iteration: 16510 loss: 0.0336 lr: 0.02\n",
            "iteration: 16520 loss: 0.0279 lr: 0.02\n",
            "iteration: 16530 loss: 0.0317 lr: 0.02\n",
            "iteration: 16540 loss: 0.0337 lr: 0.02\n",
            "iteration: 16550 loss: 0.0171 lr: 0.02\n",
            "iteration: 16560 loss: 0.0184 lr: 0.02\n",
            "iteration: 16570 loss: 0.0158 lr: 0.02\n",
            "iteration: 16580 loss: 0.0276 lr: 0.02\n",
            "iteration: 16590 loss: 0.0084 lr: 0.02\n",
            "iteration: 16600 loss: 0.0250 lr: 0.02\n",
            "iteration: 16610 loss: 0.0065 lr: 0.02\n",
            "iteration: 16620 loss: 0.0208 lr: 0.02\n",
            "iteration: 16630 loss: 0.0086 lr: 0.02\n",
            "iteration: 16640 loss: 0.0293 lr: 0.02\n",
            "iteration: 16650 loss: 0.0329 lr: 0.02\n",
            "iteration: 16660 loss: 0.0233 lr: 0.02\n",
            "iteration: 16670 loss: 0.0068 lr: 0.02\n",
            "iteration: 16680 loss: 0.0361 lr: 0.02\n",
            "iteration: 16690 loss: 0.0198 lr: 0.02\n",
            "iteration: 16700 loss: 0.0243 lr: 0.02\n",
            "iteration: 16710 loss: 0.0270 lr: 0.02\n",
            "iteration: 16720 loss: 0.0255 lr: 0.02\n",
            "iteration: 16730 loss: 0.0078 lr: 0.02\n",
            "iteration: 16740 loss: 0.0136 lr: 0.02\n",
            "iteration: 16750 loss: 0.0130 lr: 0.02\n",
            "iteration: 16760 loss: 0.0276 lr: 0.02\n",
            "iteration: 16770 loss: 0.0208 lr: 0.02\n",
            "iteration: 16780 loss: 0.0235 lr: 0.02\n",
            "iteration: 16790 loss: 0.0205 lr: 0.02\n",
            "iteration: 16800 loss: 0.0267 lr: 0.02\n",
            "iteration: 16810 loss: 0.0233 lr: 0.02\n",
            "iteration: 16820 loss: 0.0229 lr: 0.02\n",
            "iteration: 16830 loss: 0.0308 lr: 0.02\n",
            "iteration: 16840 loss: 0.0514 lr: 0.02\n",
            "iteration: 16850 loss: 0.0193 lr: 0.02\n",
            "iteration: 16860 loss: 0.0099 lr: 0.02\n",
            "iteration: 16870 loss: 0.0331 lr: 0.02\n",
            "iteration: 16880 loss: 0.0278 lr: 0.02\n",
            "iteration: 16890 loss: 0.0192 lr: 0.02\n",
            "iteration: 16900 loss: 0.0254 lr: 0.02\n",
            "iteration: 16910 loss: 0.0305 lr: 0.02\n",
            "iteration: 16920 loss: 0.0194 lr: 0.02\n",
            "iteration: 16930 loss: 0.0381 lr: 0.02\n",
            "iteration: 16940 loss: 0.0232 lr: 0.02\n",
            "iteration: 16950 loss: 0.0207 lr: 0.02\n",
            "iteration: 16960 loss: 0.0211 lr: 0.02\n",
            "iteration: 16970 loss: 0.0139 lr: 0.02\n",
            "iteration: 16980 loss: 0.0257 lr: 0.02\n",
            "iteration: 16990 loss: 0.0166 lr: 0.02\n",
            "iteration: 17000 loss: 0.0231 lr: 0.02\n",
            "iteration: 17010 loss: 0.0211 lr: 0.02\n",
            "iteration: 17020 loss: 0.0146 lr: 0.02\n",
            "iteration: 17030 loss: 0.0161 lr: 0.02\n",
            "iteration: 17040 loss: 0.0272 lr: 0.02\n",
            "iteration: 17050 loss: 0.0264 lr: 0.02\n",
            "iteration: 17060 loss: 0.0240 lr: 0.02\n",
            "iteration: 17070 loss: 0.0123 lr: 0.02\n",
            "iteration: 17080 loss: 0.0253 lr: 0.02\n",
            "iteration: 17090 loss: 0.0138 lr: 0.02\n",
            "iteration: 17100 loss: 0.0095 lr: 0.02\n",
            "iteration: 17110 loss: 0.0270 lr: 0.02\n",
            "iteration: 17120 loss: 0.0223 lr: 0.02\n",
            "iteration: 17130 loss: 0.0352 lr: 0.02\n",
            "iteration: 17140 loss: 0.0331 lr: 0.02\n",
            "iteration: 17150 loss: 0.0248 lr: 0.02\n",
            "iteration: 17160 loss: 0.0148 lr: 0.02\n",
            "iteration: 17170 loss: 0.0345 lr: 0.02\n",
            "iteration: 17180 loss: 0.0155 lr: 0.02\n",
            "iteration: 17190 loss: 0.0166 lr: 0.02\n",
            "iteration: 17200 loss: 0.0279 lr: 0.02\n",
            "iteration: 17210 loss: 0.0240 lr: 0.02\n",
            "iteration: 17220 loss: 0.0259 lr: 0.02\n",
            "iteration: 17230 loss: 0.0195 lr: 0.02\n",
            "iteration: 17240 loss: 0.0294 lr: 0.02\n",
            "iteration: 17250 loss: 0.0446 lr: 0.02\n",
            "iteration: 17260 loss: 0.0364 lr: 0.02\n",
            "iteration: 17270 loss: 0.0457 lr: 0.02\n",
            "iteration: 17280 loss: 0.0287 lr: 0.02\n",
            "iteration: 17290 loss: 0.0267 lr: 0.02\n",
            "iteration: 17300 loss: 0.0191 lr: 0.02\n",
            "iteration: 17310 loss: 0.0282 lr: 0.02\n",
            "iteration: 17320 loss: 0.0222 lr: 0.02\n",
            "iteration: 17330 loss: 0.0097 lr: 0.02\n",
            "iteration: 17340 loss: 0.0195 lr: 0.02\n",
            "iteration: 17350 loss: 0.0252 lr: 0.02\n",
            "iteration: 17360 loss: 0.0383 lr: 0.02\n",
            "iteration: 17370 loss: 0.0118 lr: 0.02\n",
            "iteration: 17380 loss: 0.0098 lr: 0.02\n",
            "iteration: 17390 loss: 0.0311 lr: 0.02\n",
            "iteration: 17400 loss: 0.0190 lr: 0.02\n",
            "iteration: 17410 loss: 0.0524 lr: 0.02\n",
            "iteration: 17420 loss: 0.0284 lr: 0.02\n",
            "iteration: 17430 loss: 0.0241 lr: 0.02\n",
            "iteration: 17440 loss: 0.0301 lr: 0.02\n",
            "iteration: 17450 loss: 0.0183 lr: 0.02\n",
            "iteration: 17460 loss: 0.0250 lr: 0.02\n",
            "iteration: 17470 loss: 0.0092 lr: 0.02\n",
            "iteration: 17480 loss: 0.0186 lr: 0.02\n",
            "iteration: 17490 loss: 0.0335 lr: 0.02\n",
            "iteration: 17500 loss: 0.0475 lr: 0.02\n",
            "iteration: 17510 loss: 0.0082 lr: 0.02\n",
            "iteration: 17520 loss: 0.0435 lr: 0.02\n",
            "iteration: 17530 loss: 0.0071 lr: 0.02\n",
            "iteration: 17540 loss: 0.0208 lr: 0.02\n",
            "iteration: 17550 loss: 0.0075 lr: 0.02\n",
            "iteration: 17560 loss: 0.0187 lr: 0.02\n",
            "iteration: 17570 loss: 0.0304 lr: 0.02\n",
            "iteration: 17580 loss: 0.0191 lr: 0.02\n",
            "iteration: 17590 loss: 0.0163 lr: 0.02\n",
            "iteration: 17600 loss: 0.0333 lr: 0.02\n",
            "iteration: 17610 loss: 0.0216 lr: 0.02\n",
            "iteration: 17620 loss: 0.0083 lr: 0.02\n",
            "iteration: 17630 loss: 0.0202 lr: 0.02\n",
            "iteration: 17640 loss: 0.0141 lr: 0.02\n",
            "iteration: 17650 loss: 0.0327 lr: 0.02\n",
            "iteration: 17660 loss: 0.0280 lr: 0.02\n",
            "iteration: 17670 loss: 0.0156 lr: 0.02\n",
            "iteration: 17680 loss: 0.0292 lr: 0.02\n",
            "iteration: 17690 loss: 0.0275 lr: 0.02\n",
            "iteration: 17700 loss: 0.0167 lr: 0.02\n",
            "iteration: 17710 loss: 0.0179 lr: 0.02\n",
            "iteration: 17720 loss: 0.0159 lr: 0.02\n",
            "iteration: 17730 loss: 0.0153 lr: 0.02\n",
            "iteration: 17740 loss: 0.0141 lr: 0.02\n",
            "iteration: 17750 loss: 0.0364 lr: 0.02\n",
            "iteration: 17760 loss: 0.0207 lr: 0.02\n",
            "iteration: 17770 loss: 0.0252 lr: 0.02\n",
            "iteration: 17780 loss: 0.0106 lr: 0.02\n",
            "iteration: 17790 loss: 0.0215 lr: 0.02\n",
            "iteration: 17800 loss: 0.0345 lr: 0.02\n",
            "iteration: 17810 loss: 0.0109 lr: 0.02\n",
            "iteration: 17820 loss: 0.0346 lr: 0.02\n",
            "iteration: 17830 loss: 0.0112 lr: 0.02\n",
            "iteration: 17840 loss: 0.0332 lr: 0.02\n",
            "iteration: 17850 loss: 0.0264 lr: 0.02\n",
            "iteration: 17860 loss: 0.0311 lr: 0.02\n",
            "iteration: 17870 loss: 0.0423 lr: 0.02\n",
            "iteration: 17880 loss: 0.0162 lr: 0.02\n",
            "iteration: 17890 loss: 0.0277 lr: 0.02\n",
            "iteration: 17900 loss: 0.0287 lr: 0.02\n",
            "iteration: 17910 loss: 0.0164 lr: 0.02\n",
            "iteration: 17920 loss: 0.0404 lr: 0.02\n",
            "iteration: 17930 loss: 0.0284 lr: 0.02\n",
            "iteration: 17940 loss: 0.0235 lr: 0.02\n",
            "iteration: 17950 loss: 0.0308 lr: 0.02\n",
            "iteration: 17960 loss: 0.0242 lr: 0.02\n",
            "iteration: 17970 loss: 0.0412 lr: 0.02\n",
            "iteration: 17980 loss: 0.0259 lr: 0.02\n",
            "iteration: 17990 loss: 0.0233 lr: 0.02\n",
            "iteration: 18000 loss: 0.0150 lr: 0.02\n",
            "iteration: 18010 loss: 0.0207 lr: 0.02\n",
            "iteration: 18020 loss: 0.0248 lr: 0.02\n",
            "iteration: 18030 loss: 0.0312 lr: 0.02\n",
            "iteration: 18040 loss: 0.0259 lr: 0.02\n",
            "iteration: 18050 loss: 0.0283 lr: 0.02\n",
            "iteration: 18060 loss: 0.0187 lr: 0.02\n",
            "iteration: 18070 loss: 0.0253 lr: 0.02\n",
            "iteration: 18080 loss: 0.0211 lr: 0.02\n",
            "iteration: 18090 loss: 0.0155 lr: 0.02\n",
            "iteration: 18100 loss: 0.0228 lr: 0.02\n",
            "iteration: 18110 loss: 0.0182 lr: 0.02\n",
            "iteration: 18120 loss: 0.0254 lr: 0.02\n",
            "iteration: 18130 loss: 0.0278 lr: 0.02\n",
            "iteration: 18140 loss: 0.0059 lr: 0.02\n",
            "iteration: 18150 loss: 0.0360 lr: 0.02\n",
            "iteration: 18160 loss: 0.0193 lr: 0.02\n",
            "iteration: 18170 loss: 0.0215 lr: 0.02\n",
            "iteration: 18180 loss: 0.0112 lr: 0.02\n",
            "iteration: 18190 loss: 0.0362 lr: 0.02\n",
            "iteration: 18200 loss: 0.0040 lr: 0.02\n",
            "iteration: 18210 loss: 0.0132 lr: 0.02\n",
            "iteration: 18220 loss: 0.0249 lr: 0.02\n",
            "iteration: 18230 loss: 0.0163 lr: 0.02\n",
            "iteration: 18240 loss: 0.0361 lr: 0.02\n",
            "iteration: 18250 loss: 0.0126 lr: 0.02\n",
            "iteration: 18260 loss: 0.0323 lr: 0.02\n",
            "iteration: 18270 loss: 0.0235 lr: 0.02\n",
            "iteration: 18280 loss: 0.0096 lr: 0.02\n",
            "iteration: 18290 loss: 0.0279 lr: 0.02\n",
            "iteration: 18300 loss: 0.0205 lr: 0.02\n",
            "iteration: 18310 loss: 0.0320 lr: 0.02\n",
            "iteration: 18320 loss: 0.0153 lr: 0.02\n",
            "iteration: 18330 loss: 0.0121 lr: 0.02\n",
            "iteration: 18340 loss: 0.0236 lr: 0.02\n",
            "iteration: 18350 loss: 0.0242 lr: 0.02\n",
            "iteration: 18360 loss: 0.0372 lr: 0.02\n",
            "iteration: 18370 loss: 0.0316 lr: 0.02\n",
            "iteration: 18380 loss: 0.0325 lr: 0.02\n",
            "iteration: 18390 loss: 0.0248 lr: 0.02\n",
            "iteration: 18400 loss: 0.0294 lr: 0.02\n",
            "iteration: 18410 loss: 0.0275 lr: 0.02\n",
            "iteration: 18420 loss: 0.0297 lr: 0.02\n",
            "iteration: 18430 loss: 0.0216 lr: 0.02\n",
            "iteration: 18440 loss: 0.0302 lr: 0.02\n",
            "iteration: 18450 loss: 0.0287 lr: 0.02\n",
            "iteration: 18460 loss: 0.0321 lr: 0.02\n",
            "iteration: 18470 loss: 0.0109 lr: 0.02\n",
            "iteration: 18480 loss: 0.0154 lr: 0.02\n",
            "iteration: 18490 loss: 0.0278 lr: 0.02\n",
            "iteration: 18500 loss: 0.0187 lr: 0.02\n",
            "iteration: 18510 loss: 0.0235 lr: 0.02\n",
            "iteration: 18520 loss: 0.0133 lr: 0.02\n",
            "iteration: 18530 loss: 0.0265 lr: 0.02\n",
            "iteration: 18540 loss: 0.0262 lr: 0.02\n",
            "iteration: 18550 loss: 0.0092 lr: 0.02\n",
            "iteration: 18560 loss: 0.0250 lr: 0.02\n",
            "iteration: 18570 loss: 0.0209 lr: 0.02\n",
            "iteration: 18580 loss: 0.0159 lr: 0.02\n",
            "iteration: 18590 loss: 0.0149 lr: 0.02\n",
            "iteration: 18600 loss: 0.0306 lr: 0.02\n",
            "iteration: 18610 loss: 0.0081 lr: 0.02\n",
            "iteration: 18620 loss: 0.0259 lr: 0.02\n",
            "iteration: 18630 loss: 0.0381 lr: 0.02\n",
            "iteration: 18640 loss: 0.0341 lr: 0.02\n",
            "iteration: 18650 loss: 0.0229 lr: 0.02\n",
            "iteration: 18660 loss: 0.0223 lr: 0.02\n",
            "iteration: 18670 loss: 0.0105 lr: 0.02\n",
            "iteration: 18680 loss: 0.0403 lr: 0.02\n",
            "iteration: 18690 loss: 0.0185 lr: 0.02\n",
            "iteration: 18700 loss: 0.0155 lr: 0.02\n",
            "iteration: 18710 loss: 0.0278 lr: 0.02\n",
            "iteration: 18720 loss: 0.0294 lr: 0.02\n",
            "iteration: 18730 loss: 0.0263 lr: 0.02\n",
            "iteration: 18740 loss: 0.0201 lr: 0.02\n",
            "iteration: 18750 loss: 0.0146 lr: 0.02\n",
            "iteration: 18760 loss: 0.0121 lr: 0.02\n",
            "iteration: 18770 loss: 0.0246 lr: 0.02\n",
            "iteration: 18780 loss: 0.0258 lr: 0.02\n",
            "iteration: 18790 loss: 0.0183 lr: 0.02\n",
            "iteration: 18800 loss: 0.0343 lr: 0.02\n",
            "iteration: 18810 loss: 0.0226 lr: 0.02\n",
            "iteration: 18820 loss: 0.0352 lr: 0.02\n",
            "iteration: 18830 loss: 0.0230 lr: 0.02\n",
            "iteration: 18840 loss: 0.0284 lr: 0.02\n",
            "iteration: 18850 loss: 0.0210 lr: 0.02\n",
            "iteration: 18860 loss: 0.0321 lr: 0.02\n",
            "iteration: 18870 loss: 0.0315 lr: 0.02\n",
            "iteration: 18880 loss: 0.0131 lr: 0.02\n",
            "iteration: 18890 loss: 0.0213 lr: 0.02\n",
            "iteration: 18900 loss: 0.0106 lr: 0.02\n",
            "iteration: 18910 loss: 0.0233 lr: 0.02\n",
            "iteration: 18920 loss: 0.0239 lr: 0.02\n",
            "iteration: 18930 loss: 0.0086 lr: 0.02\n",
            "iteration: 18940 loss: 0.0263 lr: 0.02\n",
            "iteration: 18950 loss: 0.0207 lr: 0.02\n",
            "iteration: 18960 loss: 0.0261 lr: 0.02\n",
            "iteration: 18970 loss: 0.0148 lr: 0.02\n",
            "iteration: 18980 loss: 0.0136 lr: 0.02\n",
            "iteration: 18990 loss: 0.0220 lr: 0.02\n",
            "iteration: 19000 loss: 0.0343 lr: 0.02\n",
            "iteration: 19010 loss: 0.0153 lr: 0.02\n",
            "iteration: 19020 loss: 0.0200 lr: 0.02\n",
            "iteration: 19030 loss: 0.0295 lr: 0.02\n",
            "iteration: 19040 loss: 0.0299 lr: 0.02\n",
            "iteration: 19050 loss: 0.0316 lr: 0.02\n",
            "iteration: 19060 loss: 0.0124 lr: 0.02\n",
            "iteration: 19070 loss: 0.0098 lr: 0.02\n",
            "iteration: 19080 loss: 0.0225 lr: 0.02\n",
            "iteration: 19090 loss: 0.0351 lr: 0.02\n",
            "iteration: 19100 loss: 0.0126 lr: 0.02\n",
            "iteration: 19110 loss: 0.0195 lr: 0.02\n",
            "iteration: 19120 loss: 0.0481 lr: 0.02\n",
            "iteration: 19130 loss: 0.0303 lr: 0.02\n",
            "iteration: 19140 loss: 0.0147 lr: 0.02\n",
            "iteration: 19150 loss: 0.0218 lr: 0.02\n",
            "iteration: 19160 loss: 0.0251 lr: 0.02\n",
            "iteration: 19170 loss: 0.0194 lr: 0.02\n",
            "iteration: 19180 loss: 0.0270 lr: 0.02\n",
            "iteration: 19190 loss: 0.0184 lr: 0.02\n",
            "iteration: 19200 loss: 0.0329 lr: 0.02\n",
            "iteration: 19210 loss: 0.0186 lr: 0.02\n",
            "iteration: 19220 loss: 0.0099 lr: 0.02\n",
            "iteration: 19230 loss: 0.0092 lr: 0.02\n",
            "iteration: 19240 loss: 0.0387 lr: 0.02\n",
            "iteration: 19250 loss: 0.0088 lr: 0.02\n",
            "iteration: 19260 loss: 0.0388 lr: 0.02\n",
            "iteration: 19270 loss: 0.0262 lr: 0.02\n",
            "iteration: 19280 loss: 0.0121 lr: 0.02\n",
            "iteration: 19290 loss: 0.0371 lr: 0.02\n",
            "iteration: 19300 loss: 0.0117 lr: 0.02\n",
            "iteration: 19310 loss: 0.0325 lr: 0.02\n",
            "iteration: 19320 loss: 0.0206 lr: 0.02\n",
            "iteration: 19330 loss: 0.0163 lr: 0.02\n",
            "iteration: 19340 loss: 0.0270 lr: 0.02\n",
            "iteration: 19350 loss: 0.0070 lr: 0.02\n",
            "iteration: 19360 loss: 0.0293 lr: 0.02\n",
            "iteration: 19370 loss: 0.0139 lr: 0.02\n",
            "iteration: 19380 loss: 0.0226 lr: 0.02\n",
            "iteration: 19390 loss: 0.0156 lr: 0.02\n",
            "iteration: 19400 loss: 0.0360 lr: 0.02\n",
            "iteration: 19410 loss: 0.0232 lr: 0.02\n",
            "iteration: 19420 loss: 0.0157 lr: 0.02\n",
            "iteration: 19430 loss: 0.0326 lr: 0.02\n",
            "iteration: 19440 loss: 0.0292 lr: 0.02\n",
            "iteration: 19450 loss: 0.0366 lr: 0.02\n",
            "iteration: 19460 loss: 0.0257 lr: 0.02\n",
            "iteration: 19470 loss: 0.0230 lr: 0.02\n",
            "iteration: 19480 loss: 0.0268 lr: 0.02\n",
            "iteration: 19490 loss: 0.0203 lr: 0.02\n",
            "iteration: 19500 loss: 0.0216 lr: 0.02\n",
            "iteration: 19510 loss: 0.0195 lr: 0.02\n",
            "iteration: 19520 loss: 0.0109 lr: 0.02\n",
            "iteration: 19530 loss: 0.0255 lr: 0.02\n",
            "iteration: 19540 loss: 0.0262 lr: 0.02\n",
            "iteration: 19550 loss: 0.0265 lr: 0.02\n",
            "iteration: 19560 loss: 0.0271 lr: 0.02\n",
            "iteration: 19570 loss: 0.0082 lr: 0.02\n",
            "iteration: 19580 loss: 0.0272 lr: 0.02\n",
            "iteration: 19590 loss: 0.0215 lr: 0.02\n",
            "iteration: 19600 loss: 0.0266 lr: 0.02\n",
            "iteration: 19610 loss: 0.0415 lr: 0.02\n",
            "iteration: 19620 loss: 0.0285 lr: 0.02\n",
            "iteration: 19630 loss: 0.0346 lr: 0.02\n",
            "iteration: 19640 loss: 0.0192 lr: 0.02\n",
            "iteration: 19650 loss: 0.0222 lr: 0.02\n",
            "iteration: 19660 loss: 0.0311 lr: 0.02\n",
            "iteration: 19670 loss: 0.0247 lr: 0.02\n",
            "iteration: 19680 loss: 0.0127 lr: 0.02\n",
            "iteration: 19690 loss: 0.0211 lr: 0.02\n",
            "iteration: 19700 loss: 0.0242 lr: 0.02\n",
            "iteration: 19710 loss: 0.0162 lr: 0.02\n",
            "iteration: 19720 loss: 0.0322 lr: 0.02\n",
            "iteration: 19730 loss: 0.0373 lr: 0.02\n",
            "iteration: 19740 loss: 0.0310 lr: 0.02\n",
            "iteration: 19750 loss: 0.0270 lr: 0.02\n",
            "iteration: 19760 loss: 0.0276 lr: 0.02\n",
            "iteration: 19770 loss: 0.0292 lr: 0.02\n",
            "iteration: 19780 loss: 0.0360 lr: 0.02\n",
            "iteration: 19790 loss: 0.0125 lr: 0.02\n",
            "iteration: 19800 loss: 0.0222 lr: 0.02\n",
            "iteration: 19810 loss: 0.0209 lr: 0.02\n",
            "iteration: 19820 loss: 0.0138 lr: 0.02\n",
            "iteration: 19830 loss: 0.0357 lr: 0.02\n",
            "iteration: 19840 loss: 0.0339 lr: 0.02\n",
            "iteration: 19850 loss: 0.0141 lr: 0.02\n",
            "iteration: 19860 loss: 0.0167 lr: 0.02\n",
            "iteration: 19870 loss: 0.0214 lr: 0.02\n",
            "iteration: 19880 loss: 0.0274 lr: 0.02\n",
            "iteration: 19890 loss: 0.0378 lr: 0.02\n",
            "iteration: 19900 loss: 0.0520 lr: 0.02\n",
            "iteration: 19910 loss: 0.0226 lr: 0.02\n",
            "iteration: 19920 loss: 0.0218 lr: 0.02\n",
            "iteration: 19930 loss: 0.0173 lr: 0.02\n",
            "iteration: 19940 loss: 0.0210 lr: 0.02\n",
            "iteration: 19950 loss: 0.0488 lr: 0.02\n",
            "iteration: 19960 loss: 0.0308 lr: 0.02\n",
            "iteration: 19970 loss: 0.0135 lr: 0.02\n",
            "iteration: 19980 loss: 0.0229 lr: 0.02\n",
            "iteration: 19990 loss: 0.0309 lr: 0.02\n",
            "iteration: 20000 loss: 0.0277 lr: 0.02\n",
            "iteration: 20010 loss: 0.0187 lr: 0.02\n",
            "iteration: 20020 loss: 0.0433 lr: 0.02\n",
            "iteration: 20030 loss: 0.0179 lr: 0.02\n",
            "iteration: 20040 loss: 0.0332 lr: 0.02\n",
            "iteration: 20050 loss: 0.0290 lr: 0.02\n",
            "iteration: 20060 loss: 0.0202 lr: 0.02\n",
            "iteration: 20070 loss: 0.0437 lr: 0.02\n",
            "iteration: 20080 loss: 0.0140 lr: 0.02\n",
            "iteration: 20090 loss: 0.0079 lr: 0.02\n",
            "iteration: 20100 loss: 0.0300 lr: 0.02\n",
            "iteration: 20110 loss: 0.0087 lr: 0.02\n",
            "iteration: 20120 loss: 0.0291 lr: 0.02\n",
            "iteration: 20130 loss: 0.0376 lr: 0.02\n",
            "iteration: 20140 loss: 0.0309 lr: 0.02\n",
            "iteration: 20150 loss: 0.0081 lr: 0.02\n",
            "iteration: 20160 loss: 0.0200 lr: 0.02\n",
            "iteration: 20170 loss: 0.0313 lr: 0.02\n",
            "iteration: 20180 loss: 0.0119 lr: 0.02\n",
            "iteration: 20190 loss: 0.0212 lr: 0.02\n",
            "iteration: 20200 loss: 0.0383 lr: 0.02\n",
            "iteration: 20210 loss: 0.0192 lr: 0.02\n",
            "iteration: 20220 loss: 0.0168 lr: 0.02\n",
            "iteration: 20230 loss: 0.0179 lr: 0.02\n",
            "iteration: 20240 loss: 0.0223 lr: 0.02\n",
            "iteration: 20250 loss: 0.0293 lr: 0.02\n",
            "iteration: 20260 loss: 0.0226 lr: 0.02\n",
            "iteration: 20270 loss: 0.0216 lr: 0.02\n",
            "iteration: 20280 loss: 0.0135 lr: 0.02\n",
            "iteration: 20290 loss: 0.0141 lr: 0.02\n",
            "iteration: 20300 loss: 0.0165 lr: 0.02\n",
            "iteration: 20310 loss: 0.0209 lr: 0.02\n",
            "iteration: 20320 loss: 0.0197 lr: 0.02\n",
            "iteration: 20330 loss: 0.0234 lr: 0.02\n",
            "iteration: 20340 loss: 0.0168 lr: 0.02\n",
            "iteration: 20350 loss: 0.0226 lr: 0.02\n",
            "iteration: 20360 loss: 0.0198 lr: 0.02\n",
            "iteration: 20370 loss: 0.0310 lr: 0.02\n",
            "iteration: 20380 loss: 0.0230 lr: 0.02\n",
            "iteration: 20390 loss: 0.0190 lr: 0.02\n",
            "iteration: 20400 loss: 0.0217 lr: 0.02\n",
            "iteration: 20410 loss: 0.0136 lr: 0.02\n",
            "iteration: 20420 loss: 0.0259 lr: 0.02\n",
            "iteration: 20430 loss: 0.0370 lr: 0.02\n",
            "iteration: 20440 loss: 0.0166 lr: 0.02\n",
            "iteration: 20450 loss: 0.0262 lr: 0.02\n",
            "iteration: 20460 loss: 0.0289 lr: 0.02\n",
            "iteration: 20470 loss: 0.0223 lr: 0.02\n",
            "iteration: 20480 loss: 0.0309 lr: 0.02\n",
            "iteration: 20490 loss: 0.0298 lr: 0.02\n",
            "iteration: 20500 loss: 0.0243 lr: 0.02\n",
            "iteration: 20510 loss: 0.0400 lr: 0.02\n",
            "iteration: 20520 loss: 0.0178 lr: 0.02\n",
            "iteration: 20530 loss: 0.0191 lr: 0.02\n",
            "iteration: 20540 loss: 0.0304 lr: 0.02\n",
            "iteration: 20550 loss: 0.0218 lr: 0.02\n",
            "iteration: 20560 loss: 0.0168 lr: 0.02\n",
            "iteration: 20570 loss: 0.0199 lr: 0.02\n",
            "iteration: 20580 loss: 0.0260 lr: 0.02\n",
            "iteration: 20590 loss: 0.0242 lr: 0.02\n",
            "iteration: 20600 loss: 0.0241 lr: 0.02\n",
            "iteration: 20610 loss: 0.0243 lr: 0.02\n",
            "iteration: 20620 loss: 0.0227 lr: 0.02\n",
            "iteration: 20630 loss: 0.0163 lr: 0.02\n",
            "iteration: 20640 loss: 0.0152 lr: 0.02\n",
            "iteration: 20650 loss: 0.0336 lr: 0.02\n",
            "iteration: 20660 loss: 0.0156 lr: 0.02\n",
            "iteration: 20670 loss: 0.0102 lr: 0.02\n",
            "iteration: 20680 loss: 0.0348 lr: 0.02\n",
            "iteration: 20690 loss: 0.0199 lr: 0.02\n",
            "iteration: 20700 loss: 0.0147 lr: 0.02\n",
            "iteration: 20710 loss: 0.0167 lr: 0.02\n",
            "iteration: 20720 loss: 0.0193 lr: 0.02\n",
            "iteration: 20730 loss: 0.0123 lr: 0.02\n",
            "iteration: 20740 loss: 0.0079 lr: 0.02\n",
            "iteration: 20750 loss: 0.0295 lr: 0.02\n",
            "iteration: 20760 loss: 0.0139 lr: 0.02\n",
            "iteration: 20770 loss: 0.0192 lr: 0.02\n",
            "iteration: 20780 loss: 0.0401 lr: 0.02\n",
            "iteration: 20790 loss: 0.0162 lr: 0.02\n",
            "iteration: 20800 loss: 0.0417 lr: 0.02\n",
            "iteration: 20810 loss: 0.0076 lr: 0.02\n",
            "iteration: 20820 loss: 0.0268 lr: 0.02\n",
            "iteration: 20830 loss: 0.0224 lr: 0.02\n",
            "iteration: 20840 loss: 0.0107 lr: 0.02\n",
            "iteration: 20850 loss: 0.0173 lr: 0.02\n",
            "iteration: 20860 loss: 0.0222 lr: 0.02\n",
            "iteration: 20870 loss: 0.0250 lr: 0.02\n",
            "iteration: 20880 loss: 0.0234 lr: 0.02\n",
            "iteration: 20890 loss: 0.0234 lr: 0.02\n",
            "iteration: 20900 loss: 0.0174 lr: 0.02\n",
            "iteration: 20910 loss: 0.0276 lr: 0.02\n",
            "iteration: 20920 loss: 0.0089 lr: 0.02\n",
            "iteration: 20930 loss: 0.0355 lr: 0.02\n",
            "iteration: 20940 loss: 0.0410 lr: 0.02\n",
            "iteration: 20950 loss: 0.0272 lr: 0.02\n",
            "iteration: 20960 loss: 0.0222 lr: 0.02\n",
            "iteration: 20970 loss: 0.0308 lr: 0.02\n",
            "iteration: 20980 loss: 0.0213 lr: 0.02\n",
            "iteration: 20990 loss: 0.0220 lr: 0.02\n",
            "iteration: 21000 loss: 0.0099 lr: 0.02\n",
            "iteration: 21010 loss: 0.0183 lr: 0.02\n",
            "iteration: 21020 loss: 0.0090 lr: 0.02\n",
            "iteration: 21030 loss: 0.0205 lr: 0.02\n",
            "iteration: 21040 loss: 0.0291 lr: 0.02\n",
            "iteration: 21050 loss: 0.0235 lr: 0.02\n",
            "iteration: 21060 loss: 0.0325 lr: 0.02\n",
            "iteration: 21070 loss: 0.0320 lr: 0.02\n",
            "iteration: 21080 loss: 0.0286 lr: 0.02\n",
            "iteration: 21090 loss: 0.0145 lr: 0.02\n",
            "iteration: 21100 loss: 0.0245 lr: 0.02\n",
            "iteration: 21110 loss: 0.0150 lr: 0.02\n",
            "iteration: 21120 loss: 0.0130 lr: 0.02\n",
            "iteration: 21130 loss: 0.0279 lr: 0.02\n",
            "iteration: 21140 loss: 0.0147 lr: 0.02\n",
            "iteration: 21150 loss: 0.0183 lr: 0.02\n",
            "iteration: 21160 loss: 0.0339 lr: 0.02\n",
            "iteration: 21170 loss: 0.0337 lr: 0.02\n",
            "iteration: 21180 loss: 0.0408 lr: 0.02\n",
            "iteration: 21190 loss: 0.0317 lr: 0.02\n",
            "iteration: 21200 loss: 0.0150 lr: 0.02\n",
            "iteration: 21210 loss: 0.0097 lr: 0.02\n",
            "iteration: 21220 loss: 0.0240 lr: 0.02\n",
            "iteration: 21230 loss: 0.0229 lr: 0.02\n",
            "iteration: 21240 loss: 0.0217 lr: 0.02\n",
            "iteration: 21250 loss: 0.0428 lr: 0.02\n",
            "iteration: 21260 loss: 0.0065 lr: 0.02\n",
            "iteration: 21270 loss: 0.0212 lr: 0.02\n",
            "iteration: 21280 loss: 0.0411 lr: 0.02\n",
            "iteration: 21290 loss: 0.0231 lr: 0.02\n",
            "iteration: 21300 loss: 0.0362 lr: 0.02\n",
            "iteration: 21310 loss: 0.0324 lr: 0.02\n",
            "iteration: 21320 loss: 0.0283 lr: 0.02\n",
            "iteration: 21330 loss: 0.0330 lr: 0.02\n",
            "iteration: 21340 loss: 0.0226 lr: 0.02\n",
            "iteration: 21350 loss: 0.0250 lr: 0.02\n",
            "iteration: 21360 loss: 0.0095 lr: 0.02\n",
            "iteration: 21370 loss: 0.0184 lr: 0.02\n",
            "iteration: 21380 loss: 0.0236 lr: 0.02\n",
            "iteration: 21390 loss: 0.0398 lr: 0.02\n",
            "iteration: 21400 loss: 0.0427 lr: 0.02\n",
            "iteration: 21410 loss: 0.0235 lr: 0.02\n",
            "iteration: 21420 loss: 0.0221 lr: 0.02\n",
            "iteration: 21430 loss: 0.0251 lr: 0.02\n",
            "iteration: 21440 loss: 0.0444 lr: 0.02\n",
            "iteration: 21450 loss: 0.0350 lr: 0.02\n",
            "iteration: 21460 loss: 0.0262 lr: 0.02\n",
            "iteration: 21470 loss: 0.0233 lr: 0.02\n",
            "iteration: 21480 loss: 0.0118 lr: 0.02\n",
            "iteration: 21490 loss: 0.0106 lr: 0.02\n",
            "iteration: 21500 loss: 0.0267 lr: 0.02\n",
            "iteration: 21510 loss: 0.0260 lr: 0.02\n",
            "iteration: 21520 loss: 0.0401 lr: 0.02\n",
            "iteration: 21530 loss: 0.0281 lr: 0.02\n",
            "iteration: 21540 loss: 0.0222 lr: 0.02\n",
            "iteration: 21550 loss: 0.0196 lr: 0.02\n",
            "iteration: 21560 loss: 0.0185 lr: 0.02\n",
            "iteration: 21570 loss: 0.0241 lr: 0.02\n",
            "iteration: 21580 loss: 0.0091 lr: 0.02\n",
            "iteration: 21590 loss: 0.0283 lr: 0.02\n",
            "iteration: 21600 loss: 0.0115 lr: 0.02\n",
            "iteration: 21610 loss: 0.0296 lr: 0.02\n",
            "iteration: 21620 loss: 0.0179 lr: 0.02\n",
            "iteration: 21630 loss: 0.0353 lr: 0.02\n",
            "iteration: 21640 loss: 0.0242 lr: 0.02\n",
            "iteration: 21650 loss: 0.0164 lr: 0.02\n",
            "iteration: 21660 loss: 0.0245 lr: 0.02\n",
            "iteration: 21670 loss: 0.0112 lr: 0.02\n",
            "iteration: 21680 loss: 0.0299 lr: 0.02\n",
            "iteration: 21690 loss: 0.0271 lr: 0.02\n",
            "iteration: 21700 loss: 0.0350 lr: 0.02\n",
            "iteration: 21710 loss: 0.0190 lr: 0.02\n",
            "iteration: 21720 loss: 0.0281 lr: 0.02\n",
            "iteration: 21730 loss: 0.0373 lr: 0.02\n",
            "iteration: 21740 loss: 0.0202 lr: 0.02\n",
            "iteration: 21750 loss: 0.0405 lr: 0.02\n",
            "iteration: 21760 loss: 0.0076 lr: 0.02\n",
            "iteration: 21770 loss: 0.0254 lr: 0.02\n",
            "iteration: 21780 loss: 0.0300 lr: 0.02\n",
            "iteration: 21790 loss: 0.0187 lr: 0.02\n",
            "iteration: 21800 loss: 0.0111 lr: 0.02\n",
            "iteration: 21810 loss: 0.0122 lr: 0.02\n",
            "iteration: 21820 loss: 0.0268 lr: 0.02\n",
            "iteration: 21830 loss: 0.0330 lr: 0.02\n",
            "iteration: 21840 loss: 0.0125 lr: 0.02\n",
            "iteration: 21850 loss: 0.0433 lr: 0.02\n",
            "iteration: 21860 loss: 0.0121 lr: 0.02\n",
            "iteration: 21870 loss: 0.0129 lr: 0.02\n",
            "iteration: 21880 loss: 0.0233 lr: 0.02\n",
            "iteration: 21890 loss: 0.0160 lr: 0.02\n",
            "iteration: 21900 loss: 0.0223 lr: 0.02\n",
            "iteration: 21910 loss: 0.0325 lr: 0.02\n",
            "iteration: 21920 loss: 0.0248 lr: 0.02\n",
            "iteration: 21930 loss: 0.0234 lr: 0.02\n",
            "iteration: 21940 loss: 0.0391 lr: 0.02\n",
            "iteration: 21950 loss: 0.0180 lr: 0.02\n",
            "iteration: 21960 loss: 0.0188 lr: 0.02\n",
            "iteration: 21970 loss: 0.0359 lr: 0.02\n",
            "iteration: 21980 loss: 0.0248 lr: 0.02\n",
            "iteration: 21990 loss: 0.0186 lr: 0.02\n",
            "iteration: 22000 loss: 0.0283 lr: 0.02\n",
            "iteration: 22010 loss: 0.0326 lr: 0.02\n",
            "iteration: 22020 loss: 0.0082 lr: 0.02\n",
            "iteration: 22030 loss: 0.0224 lr: 0.02\n",
            "iteration: 22040 loss: 0.0372 lr: 0.02\n",
            "iteration: 22050 loss: 0.0191 lr: 0.02\n",
            "iteration: 22060 loss: 0.0089 lr: 0.02\n",
            "iteration: 22070 loss: 0.0152 lr: 0.02\n",
            "iteration: 22080 loss: 0.0200 lr: 0.02\n",
            "iteration: 22090 loss: 0.0327 lr: 0.02\n",
            "iteration: 22100 loss: 0.0145 lr: 0.02\n",
            "iteration: 22110 loss: 0.0179 lr: 0.02\n",
            "iteration: 22120 loss: 0.0357 lr: 0.02\n",
            "iteration: 22130 loss: 0.0304 lr: 0.02\n",
            "iteration: 22140 loss: 0.0195 lr: 0.02\n",
            "iteration: 22150 loss: 0.0431 lr: 0.02\n",
            "iteration: 22160 loss: 0.0331 lr: 0.02\n",
            "iteration: 22170 loss: 0.0107 lr: 0.02\n",
            "iteration: 22180 loss: 0.0164 lr: 0.02\n",
            "iteration: 22190 loss: 0.0205 lr: 0.02\n",
            "iteration: 22200 loss: 0.0286 lr: 0.02\n",
            "iteration: 22210 loss: 0.0144 lr: 0.02\n",
            "iteration: 22220 loss: 0.0119 lr: 0.02\n",
            "iteration: 22230 loss: 0.0099 lr: 0.02\n",
            "iteration: 22240 loss: 0.0407 lr: 0.02\n",
            "iteration: 22250 loss: 0.0134 lr: 0.02\n",
            "iteration: 22260 loss: 0.0247 lr: 0.02\n",
            "iteration: 22270 loss: 0.0237 lr: 0.02\n",
            "iteration: 22280 loss: 0.0093 lr: 0.02\n",
            "iteration: 22290 loss: 0.0243 lr: 0.02\n",
            "iteration: 22300 loss: 0.0207 lr: 0.02\n",
            "iteration: 22310 loss: 0.0327 lr: 0.02\n",
            "iteration: 22320 loss: 0.0326 lr: 0.02\n",
            "iteration: 22330 loss: 0.0209 lr: 0.02\n",
            "iteration: 22340 loss: 0.0177 lr: 0.02\n",
            "iteration: 22350 loss: 0.0262 lr: 0.02\n",
            "iteration: 22360 loss: 0.0189 lr: 0.02\n",
            "iteration: 22370 loss: 0.0144 lr: 0.02\n",
            "iteration: 22380 loss: 0.0283 lr: 0.02\n",
            "iteration: 22390 loss: 0.0106 lr: 0.02\n",
            "iteration: 22400 loss: 0.0310 lr: 0.02\n",
            "iteration: 22410 loss: 0.0160 lr: 0.02\n",
            "iteration: 22420 loss: 0.0248 lr: 0.02\n",
            "iteration: 22430 loss: 0.0332 lr: 0.02\n",
            "iteration: 22440 loss: 0.0083 lr: 0.02\n",
            "iteration: 22450 loss: 0.0252 lr: 0.02\n",
            "iteration: 22460 loss: 0.0209 lr: 0.02\n",
            "iteration: 22470 loss: 0.0194 lr: 0.02\n",
            "iteration: 22480 loss: 0.0136 lr: 0.02\n",
            "iteration: 22490 loss: 0.0249 lr: 0.02\n",
            "iteration: 22500 loss: 0.0309 lr: 0.02\n",
            "iteration: 22510 loss: 0.0200 lr: 0.02\n",
            "iteration: 22520 loss: 0.0234 lr: 0.02\n",
            "iteration: 22530 loss: 0.0140 lr: 0.02\n",
            "iteration: 22540 loss: 0.0227 lr: 0.02\n",
            "iteration: 22550 loss: 0.0437 lr: 0.02\n",
            "iteration: 22560 loss: 0.0339 lr: 0.02\n",
            "iteration: 22570 loss: 0.0340 lr: 0.02\n",
            "iteration: 22580 loss: 0.0252 lr: 0.02\n",
            "iteration: 22590 loss: 0.0181 lr: 0.02\n",
            "iteration: 22600 loss: 0.0249 lr: 0.02\n",
            "iteration: 22610 loss: 0.0187 lr: 0.02\n",
            "iteration: 22620 loss: 0.0357 lr: 0.02\n",
            "iteration: 22630 loss: 0.0151 lr: 0.02\n",
            "iteration: 22640 loss: 0.0309 lr: 0.02\n",
            "iteration: 22650 loss: 0.0286 lr: 0.02\n",
            "iteration: 22660 loss: 0.0371 lr: 0.02\n",
            "iteration: 22670 loss: 0.0402 lr: 0.02\n",
            "iteration: 22680 loss: 0.0267 lr: 0.02\n",
            "iteration: 22690 loss: 0.0267 lr: 0.02\n",
            "iteration: 22700 loss: 0.0332 lr: 0.02\n",
            "iteration: 22710 loss: 0.0210 lr: 0.02\n",
            "iteration: 22720 loss: 0.0240 lr: 0.02\n",
            "iteration: 22730 loss: 0.0097 lr: 0.02\n",
            "iteration: 22740 loss: 0.0365 lr: 0.02\n",
            "iteration: 22750 loss: 0.0276 lr: 0.02\n",
            "iteration: 22760 loss: 0.0287 lr: 0.02\n",
            "iteration: 22770 loss: 0.0134 lr: 0.02\n",
            "iteration: 22780 loss: 0.0235 lr: 0.02\n",
            "iteration: 22790 loss: 0.0142 lr: 0.02\n",
            "iteration: 22800 loss: 0.0204 lr: 0.02\n",
            "iteration: 22810 loss: 0.0178 lr: 0.02\n",
            "iteration: 22820 loss: 0.0283 lr: 0.02\n",
            "iteration: 22830 loss: 0.0110 lr: 0.02\n",
            "iteration: 22840 loss: 0.0197 lr: 0.02\n",
            "iteration: 22850 loss: 0.0129 lr: 0.02\n",
            "iteration: 22860 loss: 0.0252 lr: 0.02\n",
            "iteration: 22870 loss: 0.0387 lr: 0.02\n",
            "iteration: 22880 loss: 0.0213 lr: 0.02\n",
            "iteration: 22890 loss: 0.0173 lr: 0.02\n",
            "iteration: 22900 loss: 0.0136 lr: 0.02\n",
            "iteration: 22910 loss: 0.0172 lr: 0.02\n",
            "iteration: 22920 loss: 0.0158 lr: 0.02\n",
            "iteration: 22930 loss: 0.0266 lr: 0.02\n",
            "iteration: 22940 loss: 0.0443 lr: 0.02\n",
            "iteration: 22950 loss: 0.0206 lr: 0.02\n",
            "iteration: 22960 loss: 0.0213 lr: 0.02\n",
            "iteration: 22970 loss: 0.0273 lr: 0.02\n",
            "iteration: 22980 loss: 0.0211 lr: 0.02\n",
            "iteration: 22990 loss: 0.0337 lr: 0.02\n",
            "iteration: 23000 loss: 0.0139 lr: 0.02\n",
            "iteration: 23010 loss: 0.0093 lr: 0.02\n",
            "iteration: 23020 loss: 0.0216 lr: 0.02\n",
            "iteration: 23030 loss: 0.0195 lr: 0.02\n",
            "iteration: 23040 loss: 0.0195 lr: 0.02\n",
            "iteration: 23050 loss: 0.0469 lr: 0.02\n",
            "iteration: 23060 loss: 0.0143 lr: 0.02\n",
            "iteration: 23070 loss: 0.0142 lr: 0.02\n",
            "iteration: 23080 loss: 0.0263 lr: 0.02\n",
            "iteration: 23090 loss: 0.0173 lr: 0.02\n",
            "iteration: 23100 loss: 0.0222 lr: 0.02\n",
            "iteration: 23110 loss: 0.0409 lr: 0.02\n",
            "iteration: 23120 loss: 0.0362 lr: 0.02\n",
            "iteration: 23130 loss: 0.0215 lr: 0.02\n",
            "iteration: 23140 loss: 0.0059 lr: 0.02\n",
            "iteration: 23150 loss: 0.0127 lr: 0.02\n",
            "iteration: 23160 loss: 0.0259 lr: 0.02\n",
            "iteration: 23170 loss: 0.0178 lr: 0.02\n",
            "iteration: 23180 loss: 0.0203 lr: 0.02\n",
            "iteration: 23190 loss: 0.0207 lr: 0.02\n",
            "iteration: 23200 loss: 0.0186 lr: 0.02\n",
            "iteration: 23210 loss: 0.0278 lr: 0.02\n",
            "iteration: 23220 loss: 0.0383 lr: 0.02\n",
            "iteration: 23230 loss: 0.0177 lr: 0.02\n",
            "iteration: 23240 loss: 0.0265 lr: 0.02\n",
            "iteration: 23250 loss: 0.0157 lr: 0.02\n",
            "iteration: 23260 loss: 0.0326 lr: 0.02\n",
            "iteration: 23270 loss: 0.0201 lr: 0.02\n",
            "iteration: 23280 loss: 0.0114 lr: 0.02\n",
            "iteration: 23290 loss: 0.0200 lr: 0.02\n",
            "iteration: 23300 loss: 0.0206 lr: 0.02\n",
            "iteration: 23310 loss: 0.0175 lr: 0.02\n",
            "iteration: 23320 loss: 0.0128 lr: 0.02\n",
            "iteration: 23330 loss: 0.0162 lr: 0.02\n",
            "iteration: 23340 loss: 0.0197 lr: 0.02\n",
            "iteration: 23350 loss: 0.0058 lr: 0.02\n",
            "iteration: 23360 loss: 0.0196 lr: 0.02\n",
            "iteration: 23370 loss: 0.0198 lr: 0.02\n",
            "iteration: 23380 loss: 0.0279 lr: 0.02\n",
            "iteration: 23390 loss: 0.0108 lr: 0.02\n",
            "iteration: 23400 loss: 0.0146 lr: 0.02\n",
            "iteration: 23410 loss: 0.0289 lr: 0.02\n",
            "iteration: 23420 loss: 0.0212 lr: 0.02\n",
            "iteration: 23430 loss: 0.0193 lr: 0.02\n",
            "iteration: 23440 loss: 0.0183 lr: 0.02\n",
            "iteration: 23450 loss: 0.0208 lr: 0.02\n",
            "iteration: 23460 loss: 0.0081 lr: 0.02\n",
            "iteration: 23470 loss: 0.0196 lr: 0.02\n",
            "iteration: 23480 loss: 0.0162 lr: 0.02\n",
            "iteration: 23490 loss: 0.0197 lr: 0.02\n",
            "iteration: 23500 loss: 0.0424 lr: 0.02\n",
            "iteration: 23510 loss: 0.0210 lr: 0.02\n",
            "iteration: 23520 loss: 0.0152 lr: 0.02\n",
            "iteration: 23530 loss: 0.0178 lr: 0.02\n",
            "iteration: 23540 loss: 0.0319 lr: 0.02\n",
            "iteration: 23550 loss: 0.0222 lr: 0.02\n",
            "iteration: 23560 loss: 0.0359 lr: 0.02\n",
            "iteration: 23570 loss: 0.0092 lr: 0.02\n",
            "iteration: 23580 loss: 0.0280 lr: 0.02\n",
            "iteration: 23590 loss: 0.0283 lr: 0.02\n",
            "iteration: 23600 loss: 0.0208 lr: 0.02\n",
            "iteration: 23610 loss: 0.0256 lr: 0.02\n",
            "iteration: 23620 loss: 0.0246 lr: 0.02\n",
            "iteration: 23630 loss: 0.0176 lr: 0.02\n",
            "iteration: 23640 loss: 0.0197 lr: 0.02\n",
            "iteration: 23650 loss: 0.0200 lr: 0.02\n",
            "iteration: 23660 loss: 0.0305 lr: 0.02\n",
            "iteration: 23670 loss: 0.0132 lr: 0.02\n",
            "iteration: 23680 loss: 0.0085 lr: 0.02\n",
            "iteration: 23690 loss: 0.0304 lr: 0.02\n",
            "iteration: 23700 loss: 0.0130 lr: 0.02\n",
            "iteration: 23710 loss: 0.0145 lr: 0.02\n",
            "iteration: 23720 loss: 0.0230 lr: 0.02\n",
            "iteration: 23730 loss: 0.0428 lr: 0.02\n",
            "iteration: 23740 loss: 0.0383 lr: 0.02\n",
            "iteration: 23750 loss: 0.0142 lr: 0.02\n",
            "iteration: 23760 loss: 0.0144 lr: 0.02\n",
            "iteration: 23770 loss: 0.0179 lr: 0.02\n",
            "iteration: 23780 loss: 0.0292 lr: 0.02\n",
            "iteration: 23790 loss: 0.0259 lr: 0.02\n",
            "iteration: 23800 loss: 0.0284 lr: 0.02\n",
            "iteration: 23810 loss: 0.0151 lr: 0.02\n",
            "iteration: 23820 loss: 0.0273 lr: 0.02\n",
            "iteration: 23830 loss: 0.0138 lr: 0.02\n",
            "iteration: 23840 loss: 0.0127 lr: 0.02\n",
            "iteration: 23850 loss: 0.0103 lr: 0.02\n",
            "iteration: 23860 loss: 0.0178 lr: 0.02\n",
            "iteration: 23870 loss: 0.0435 lr: 0.02\n",
            "iteration: 23880 loss: 0.0149 lr: 0.02\n",
            "iteration: 23890 loss: 0.0287 lr: 0.02\n",
            "iteration: 23900 loss: 0.0357 lr: 0.02\n",
            "iteration: 23910 loss: 0.0314 lr: 0.02\n",
            "iteration: 23920 loss: 0.0193 lr: 0.02\n",
            "iteration: 23930 loss: 0.0327 lr: 0.02\n",
            "iteration: 23940 loss: 0.0352 lr: 0.02\n",
            "iteration: 23950 loss: 0.0133 lr: 0.02\n",
            "iteration: 23960 loss: 0.0257 lr: 0.02\n",
            "iteration: 23970 loss: 0.0249 lr: 0.02\n",
            "iteration: 23980 loss: 0.0223 lr: 0.02\n",
            "iteration: 23990 loss: 0.0240 lr: 0.02\n",
            "iteration: 24000 loss: 0.0262 lr: 0.02\n",
            "iteration: 24010 loss: 0.0247 lr: 0.02\n",
            "iteration: 24020 loss: 0.0213 lr: 0.02\n",
            "iteration: 24030 loss: 0.0120 lr: 0.02\n",
            "iteration: 24040 loss: 0.0189 lr: 0.02\n",
            "iteration: 24050 loss: 0.0125 lr: 0.02\n",
            "iteration: 24060 loss: 0.0329 lr: 0.02\n",
            "iteration: 24070 loss: 0.0325 lr: 0.02\n",
            "iteration: 24080 loss: 0.0232 lr: 0.02\n",
            "iteration: 24090 loss: 0.0197 lr: 0.02\n",
            "iteration: 24100 loss: 0.0270 lr: 0.02\n",
            "iteration: 24110 loss: 0.0210 lr: 0.02\n",
            "iteration: 24120 loss: 0.0134 lr: 0.02\n",
            "iteration: 24130 loss: 0.0246 lr: 0.02\n",
            "iteration: 24140 loss: 0.0354 lr: 0.02\n",
            "iteration: 24150 loss: 0.0187 lr: 0.02\n",
            "iteration: 24160 loss: 0.0244 lr: 0.02\n",
            "iteration: 24170 loss: 0.0202 lr: 0.02\n",
            "iteration: 24180 loss: 0.0149 lr: 0.02\n",
            "iteration: 24190 loss: 0.0457 lr: 0.02\n",
            "iteration: 24200 loss: 0.0223 lr: 0.02\n",
            "iteration: 24210 loss: 0.0238 lr: 0.02\n",
            "iteration: 24220 loss: 0.0112 lr: 0.02\n",
            "iteration: 24230 loss: 0.0388 lr: 0.02\n",
            "iteration: 24240 loss: 0.0128 lr: 0.02\n",
            "iteration: 24250 loss: 0.0119 lr: 0.02\n",
            "iteration: 24260 loss: 0.0184 lr: 0.02\n",
            "iteration: 24270 loss: 0.0181 lr: 0.02\n",
            "iteration: 24280 loss: 0.0257 lr: 0.02\n",
            "iteration: 24290 loss: 0.0237 lr: 0.02\n",
            "iteration: 24300 loss: 0.0313 lr: 0.02\n",
            "iteration: 24310 loss: 0.0233 lr: 0.02\n",
            "iteration: 24320 loss: 0.0254 lr: 0.02\n",
            "iteration: 24330 loss: 0.0192 lr: 0.02\n",
            "iteration: 24340 loss: 0.0188 lr: 0.02\n",
            "iteration: 24350 loss: 0.0194 lr: 0.02\n",
            "iteration: 24360 loss: 0.0234 lr: 0.02\n",
            "iteration: 24370 loss: 0.0148 lr: 0.02\n",
            "iteration: 24380 loss: 0.0233 lr: 0.02\n",
            "iteration: 24390 loss: 0.0234 lr: 0.02\n",
            "iteration: 24400 loss: 0.0306 lr: 0.02\n",
            "iteration: 24410 loss: 0.0210 lr: 0.02\n",
            "iteration: 24420 loss: 0.0314 lr: 0.02\n",
            "iteration: 24430 loss: 0.0255 lr: 0.02\n",
            "iteration: 24440 loss: 0.0226 lr: 0.02\n",
            "iteration: 24450 loss: 0.0140 lr: 0.02\n",
            "iteration: 24460 loss: 0.0223 lr: 0.02\n",
            "iteration: 24470 loss: 0.0234 lr: 0.02\n",
            "iteration: 24480 loss: 0.0229 lr: 0.02\n",
            "iteration: 24490 loss: 0.0111 lr: 0.02\n",
            "iteration: 24500 loss: 0.0208 lr: 0.02\n",
            "iteration: 24510 loss: 0.0262 lr: 0.02\n",
            "iteration: 24520 loss: 0.0144 lr: 0.02\n",
            "iteration: 24530 loss: 0.0206 lr: 0.02\n",
            "iteration: 24540 loss: 0.0244 lr: 0.02\n",
            "iteration: 24550 loss: 0.0165 lr: 0.02\n",
            "iteration: 24560 loss: 0.0216 lr: 0.02\n",
            "iteration: 24570 loss: 0.0232 lr: 0.02\n",
            "iteration: 24580 loss: 0.0227 lr: 0.02\n",
            "iteration: 24590 loss: 0.0424 lr: 0.02\n",
            "iteration: 24600 loss: 0.0307 lr: 0.02\n",
            "iteration: 24610 loss: 0.0275 lr: 0.02\n",
            "iteration: 24620 loss: 0.0225 lr: 0.02\n",
            "iteration: 24630 loss: 0.0133 lr: 0.02\n",
            "iteration: 24640 loss: 0.0200 lr: 0.02\n",
            "iteration: 24650 loss: 0.0368 lr: 0.02\n",
            "iteration: 24660 loss: 0.0236 lr: 0.02\n",
            "iteration: 24670 loss: 0.0323 lr: 0.02\n",
            "iteration: 24680 loss: 0.0189 lr: 0.02\n",
            "iteration: 24690 loss: 0.0095 lr: 0.02\n",
            "iteration: 24700 loss: 0.0263 lr: 0.02\n",
            "iteration: 24710 loss: 0.0285 lr: 0.02\n",
            "iteration: 24720 loss: 0.0224 lr: 0.02\n",
            "iteration: 24730 loss: 0.0270 lr: 0.02\n",
            "iteration: 24740 loss: 0.0273 lr: 0.02\n",
            "iteration: 24750 loss: 0.0147 lr: 0.02\n",
            "iteration: 24760 loss: 0.0334 lr: 0.02\n",
            "iteration: 24770 loss: 0.0280 lr: 0.02\n",
            "iteration: 24780 loss: 0.0098 lr: 0.02\n",
            "iteration: 24790 loss: 0.0189 lr: 0.02\n",
            "iteration: 24800 loss: 0.0350 lr: 0.02\n",
            "iteration: 24810 loss: 0.0251 lr: 0.02\n",
            "iteration: 24820 loss: 0.0170 lr: 0.02\n",
            "iteration: 24830 loss: 0.0265 lr: 0.02\n",
            "iteration: 24840 loss: 0.0200 lr: 0.02\n",
            "iteration: 24850 loss: 0.0177 lr: 0.02\n",
            "iteration: 24860 loss: 0.0125 lr: 0.02\n",
            "iteration: 24870 loss: 0.0320 lr: 0.02\n",
            "iteration: 24880 loss: 0.0256 lr: 0.02\n",
            "iteration: 24890 loss: 0.0206 lr: 0.02\n",
            "iteration: 24900 loss: 0.0474 lr: 0.02\n",
            "iteration: 24910 loss: 0.0162 lr: 0.02\n",
            "iteration: 24920 loss: 0.0102 lr: 0.02\n",
            "iteration: 24930 loss: 0.0144 lr: 0.02\n",
            "iteration: 24940 loss: 0.0296 lr: 0.02\n",
            "iteration: 24950 loss: 0.0298 lr: 0.02\n",
            "iteration: 24960 loss: 0.0330 lr: 0.02\n",
            "iteration: 24970 loss: 0.0311 lr: 0.02\n",
            "iteration: 24980 loss: 0.0165 lr: 0.02\n",
            "iteration: 24990 loss: 0.0377 lr: 0.02\n",
            "iteration: 25000 loss: 0.0464 lr: 0.02\n",
            "Exception in thread Thread-11 (load_and_enqueue):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\", line 86, in load_and_enqueue\n",
            "    sess.run(enqueue_op, feed_dict=food)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\", line 968, in run\n",
            "    result = self._run(None, fetches, feed_dict, options_ptr,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\", line 1116, in _run\n",
            "    raise RuntimeError('Attempted to use a closed Session.')\n",
            "RuntimeError: Attempted to use a closed Session.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network.\n"
          ]
        }
      ],
      "source": [
        "#let's also change the display and save_iters just in case Colab takes away the GPU...\n",
        "#if that happens, you can reload from a saved point. Typically, you want to train to 200,000 + iterations.\n",
        "#more info and there are more things you can set: https://github.com/DeepLabCut/DeepLabCut/wiki/DOCSTRINGS#train_network\n",
        "\n",
        "deeplabcut.train_network(path_config_file, shuffle=1, displayiters=10,maxiters=25000,saveiters=500)\n",
        "\n",
        "#this will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.03M iterations).\n",
        "#Whichever you chose, you will see what looks like an error message, but it's not an error - don't worry...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiDwIVf5-3H_"
      },
      "source": [
        "**When you hit \"STOP\" you will get a KeyInterrupt \"error\"! No worries! :)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZygsb2DoEJc"
      },
      "source": [
        "## Start evaluating:\n",
        "This function evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images)\n",
        "and stores the results as .csv file in a subdirectory under **evaluation-results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nv4zlbrnoEJg",
        "outputId": "b44bad7c-cc71-4a81-df7a-14e7a7ebb2f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1], [2], [3], [4], [5]],\n",
            " 'all_joints_names': ['left_wrist',\n",
            "                      'left_elbow',\n",
            "                      'left_shoulder',\n",
            "                      'right_wrist',\n",
            "                      'right_elbow',\n",
            "                      'right_shoulder'],\n",
            " 'batch_size': 1,\n",
            " 'crop_pad': 0,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_Benchpress_FillbodyNov12/Benchpress_Fillbody_YANGKAI95shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'deterministic': False,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.10/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 1.0,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'mirror': False,\n",
            " 'net_type': 'resnet_50',\n",
            " 'num_joints': 6,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': True,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'regularize': False,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/My '\n",
            "                    'Drive/Benchpress_Fullbody-YANGKAI-2024-11-12/dlc-models/iteration-0/Benchpress_FillbodyNov12-trainset95shuffle1/test/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n",
            "Config:\n",
            "{'all_joints': [[0], [1], [2], [3], [4], [5]],\n",
            " 'all_joints_names': ['left_wrist',\n",
            "                      'left_elbow',\n",
            "                      'left_shoulder',\n",
            "                      'right_wrist',\n",
            "                      'right_elbow',\n",
            "                      'right_shoulder'],\n",
            " 'alpha_r': 0.02,\n",
            " 'apply_prob': 0.5,\n",
            " 'batch_size': 1,\n",
            " 'contrast': {'clahe': True,\n",
            "              'claheratio': 0.1,\n",
            "              'histeq': True,\n",
            "              'histeqratio': 0.1},\n",
            " 'convolution': {'edge': False,\n",
            "                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},\n",
            "                 'embossratio': 0.1,\n",
            "                 'sharpen': False,\n",
            "                 'sharpenratio': 0.3},\n",
            " 'crop_pad': 0,\n",
            " 'cropratio': 0.4,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_Benchpress_FillbodyNov12/Benchpress_Fillbody_YANGKAI95shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'decay_steps': 30000,\n",
            " 'deterministic': False,\n",
            " 'display_iters': 1000,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.10/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 0.05,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'lr_init': 0.0005,\n",
            " 'max_input_size': 1500,\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_Benchpress_FillbodyNov12/Documentation_data-Benchpress_Fillbody_95shuffle1.pickle',\n",
            " 'min_input_size': 64,\n",
            " 'mirror': True,\n",
            " 'multi_stage': False,\n",
            " 'multi_step': [[0.005, 10000],\n",
            "                [0.02, 430000],\n",
            "                [0.002, 730000],\n",
            "                [0.001, 1030000]],\n",
            " 'net_type': 'resnet_50',\n",
            " 'num_joints': 6,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': False,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'pos_dist_thresh': 17,\n",
            " 'project_path': '/content/drive/My '\n",
            "                 'Drive/Benchpress_Fullbody-YANGKAI-2024-11-12',\n",
            " 'regularize': False,\n",
            " 'rotation': 25,\n",
            " 'rotratio': 0.4,\n",
            " 'save_iters': 50000,\n",
            " 'scale_jitter_lo': 0.5,\n",
            " 'scale_jitter_up': 1.25,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/My '\n",
            "                    'Drive/Benchpress_Fullbody-YANGKAI-2024-11-12/dlc-models/iteration-0/Benchpress_FillbodyNov12-trainset95shuffle1/train/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running  DLC_resnet50_Benchpress_FillbodyNov12shuffle1_25000  with # of training iterations: 25000\n",
            "This net has already been evaluated!\n",
            "Plots already exist for this snapshot... Skipping to the next one.\n"
          ]
        }
      ],
      "source": [
        "%matplotlib notebook\n",
        "deeplabcut.evaluate_network(path_config_file,plotting=True)\n",
        "\n",
        "# Here you want to see a low pixel error! Of course, it can only be as good as the labeler,\n",
        "#so be sure your labels are good! (And you have trained enough ;)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "videofile_path = ['/content/drive/My Drive/Benchpress_PlateTracking-YANGKAI-2024-12-07/TestVideos/testvideo3.mov']\n",
        "videotype = 'mov'"
      ],
      "metadata": {
        "id": "IUGeMVUonuWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaLBl3TQtrfB"
      },
      "source": [
        "## There is an optional refinement step you can do outside of Colab:\n",
        "- if your pixel errors are not low enough, please check out the protocol guide on how to refine your network!\n",
        "- You will need to adjust the labels **outside of Colab!** We recommend coming back to train and analyze videos...\n",
        "- Please see the repo and protocol instructions on how to refine your data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVFLSKKfoEJk"
      },
      "source": [
        "## Start Analyzing videos:\n",
        "This function analyzes the new video. The user can choose the best model from the evaluation results and specify the correct snapshot index for the variable **snapshotindex** in the **config.yaml** file. Otherwise, by default the most recent snapshot is used to analyse the video.\n",
        "\n",
        "The results are stored in hd5 file in the same directory where the video resides."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EN6_7v-CYWjA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_LZiS_0oEJl",
        "outputId": "8fcd73e7-c3b0-40d0-bccb-056176fd489f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using snapshot-50000 for model /content/drive/My Drive/Benchpress_PlateTracking-YANGKAI-2024-12-07/dlc-models/iteration-0/Benchpress_PlateTrackingDec7-trainset95shuffle1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to analyze %  /content/drive/My Drive/Benchpress_PlateTracking-YANGKAI-2024-12-07/TestVideos/testvideo3.mov\n",
            "Loading  /content/drive/My Drive/Benchpress_PlateTracking-YANGKAI-2024-12-07/TestVideos/testvideo3.mov\n",
            "Duration of video [s]:  7.94 , recorded with  39.66 fps!\n",
            "Overall # of frames:  315  found with (before cropping) frame dimensions:  1920 1342\n",
            "Starting to extract posture\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/315 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "deeplabcut.analyze_videos(path_config_file,videofile_path, videotype=VideoType)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GTiuJESoEKH"
      },
      "source": [
        "## Plot the trajectories of the analyzed videos:\n",
        "This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX21zZbXoEKJ",
        "outputId": "46cc3340-264f-4206-fec6-3e225dd4d458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading  /content/drive/My Drive/Benchpress_PlateTracking-YANGKAI-2024-12-07/TestVideos/testvideo2.mov and data.\n",
            "Plots created! Please check the directory \"plot-poses\" within the video directory\n"
          ]
        }
      ],
      "source": [
        "deeplabcut.plot_trajectories(path_config_file,videofile_path, videotype=VideoType)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqaCw15v8EmB"
      },
      "source": [
        "Now you can look at the plot-poses file and check the \"plot-likelihood.png\" might want to change the \"p-cutoff\" in the config.yaml file so that you have only high confidnece points plotted in the video. i.e. ~0.8 or 0.9. The current default is 0.4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCrUvQIvoEKD"
      },
      "source": [
        "## Create labeled video:\n",
        "This function is for visualiztion purpose and can be used to create a video in .mp4 format with labels predicted by the network. This video is saved in the same directory where the original video resides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aDF7Q7KoEKE",
        "outputId": "b5788e1d-c86a-4ad1-fbde-d31976ac18e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to process video: /content/drive/My Drive/Benchpress_PlateTracking-YANGKAI-2024-12-07/TestVideos/testvideo2.mov\n",
            "Loading /content/drive/My Drive/Benchpress_PlateTracking-YANGKAI-2024-12-07/TestVideos/testvideo2.mov and data.\n",
            "Duration of video [s]: 7.3, recorded with 60.0 fps!\n",
            "Overall # of frames: 438 with cropped frame dimensions: 1920 1072\n",
            "Generating frames and creating video.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 438/438 [00:07<00:00, 57.00it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[True]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "deeplabcut.create_labeled_video(path_config_file,videofile_path, videotype=VideoType)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "70cad038f2bddb56e8a0ba66c48b76ebce20579892bf83e71733a81977e3ceea"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}